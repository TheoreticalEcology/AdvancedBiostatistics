[["index.html", "Advanced Regression Models with R 1 Preface", " Advanced Regression Models with R Florian Hartig 2022-06-23 1 Preface If we have a response variable and want to understand how this response variable is influenced by one or several factors, we will typically use a regression model. The aim of this course is to enable you to run such a regresion model in the quality expected for a “real” scientific study. To do so, you will have to master a number of skills, in particular: Understanding the fundamental statistical indicators in regression analysis (p-value, estimator) and their quality (power, bias, error, coverage), Understanding what a causal effect means in a regression context, and what this means for experimental design and model selection, Knowing all building blocks of the “advanced GLMM framework” that helps us to correctly model our data, e.g. GLMs, random effects, GAMs, correlation structures, … Knowledge of standard non-parametric evaluation methods for regression models, such as parametric and non-parametric bootstrap, cross-validation, and the ability to use all of these methods in an applied data analysis. This is what we will mainly train in this course. Don’t worry if you think that this sounds too simple. We could spend an entire week on understanding the p-value alone, and still only scratch the surface. If you have an overview of what can be done with regression models, and are confident to run a realistic scientific analysis on your own, we have achieved a lot. This course assumes basic prior knowledge of statistical methods (tests, regressions, p-value, power, CIs, …) and the ability to apply those in R. In Regensburg, this knowledge would be taught in the Bachelors Biology Lecture “Statistik und Bioinformatik” (lecture notes in German here), and the block course “Introduction to statistics in R”. If you didn’t take those or comparable courses, you should at least try to get some basic understanding of R before proceeding with this book. This lecture series from MarinStatsLectures could be a good start. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License Note that some elements of this work (embedded videos, graphics) may be under a seperate licence and are thus not included in this licence. "],["reminder.html", "2 Reminder: R Basics 2.1 Your R System 2.2 Representing Data in R 2.3 Data Selection, Slicing and Subsetting 2.4 Applying Functions and Aggregates Across a Data Set 2.5 Plotting", " 2 Reminder: R Basics This chapter reminds you about basic R data types and how to operate on them. Also suitable for self-study prior to the course! 2.1 Your R System In this course, we work with the combination of R + RStudio. R is the calculation engine that performs the computations. RStudio is the editor that helps you sending inputs to R and collect outputs. Make sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is an introductory video. 2.1.1 Installing Libraries The R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is Bioconductor. To install a package from a library, use the command install.packages(LIBRARY) Exchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install build-essential gfortran libmagick++-dev r-base-dev cmake If you are new to installing packages on Debian / Ubuntu, etc., type the following: sudo apt update &amp;&amp; sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake In this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package, run: # install.packages(&quot;devtools&quot;) # If you don&#39;t have the devtools package installed, devtools::install_github(repo = &quot;TheoreticalEcology/EcoData&quot;, dependencies = T, build_vignettes = T) For your convenience, the EcoData installation also forces the installation of most of the packges needed in this book, so this may take a while. If you want to load only the EcoData package, or if you encounter problems during the install, set dependencies = F, build_vignettes = F. 2.2 Representing Data in R 2.2.1 Exploring Data Structures A fundamental requirement for working with data is representing it in a computer. In R, we can either read in data (e.g. with functions such as read.table()), or we can assign variables certain values. For example, if I type x &lt;- 1 the variable x now contains some data, namely the value 1, and I can use x in as a placeholder for the data it contains in further calculations. Alternatively to the &lt;- operator, you can also use = (in all circumstances that you are likely to encounter, it’s the same). x = 1 If you have worked with R previously, this should all be familiar to you, and you should also know that the commands class(x) dim(x) str(x) allow you to explore the structure of variables and the data they contain. Ask yourself, or discuss with your partner(s): Task What is the meaning of the three functions, and what is the structure / properties of the following data types in R: Atomic types (which atomic types exist), list, vector, data.frame, matrix, array. Solution Atomic types: e.g. numeric, factor, boolean …; List can have severa tyes, vector not! data.frame is list of vectors. matrix is 2-dim array, array can have any dim, only one type. Task What is the data type of the iris data set, which is built-in in R under the name iris Solution Iris, like most simple datasets, is of type data.frame 2.2.2 Dynamic Typing R is a dynamically typed language, which means that the type of variables is determined automatically depending on what values you supply. Try this: x = 1 class(x) x = &quot;dog&quot; class(x) This also works if a data set already exists, i.e. if you assign a different value, the type will automatically be changed. Look at what happens when we assign a character value to a previously numeric column in a data.frame: iris$Sepal.Length[2] = &quot;dog&quot; str(iris) Note that all numeric values are changed to characters as well. You can try to force back the values to numeric by: iris$Sepal.Length = as.numeric(iris$Sepal.Length) Have a look at what this does to the values in iris$Sepal.Length. Note: The actions above operate on a local copy of the iris data set. You don’t overwrite the base data and can use it again in a new R session or reset it with data(iris). 2.3 Data Selection, Slicing and Subsetting 2.3.1 Subsetting and Slicing for Single Data Types We often want to select only a subset of our data. You can generally subset from data structures using indices and TRUE/FALSE (or T/F). Here for a vector: vector = 1:6 vector[1] # First element. vector[1:3] # Elements 1, 2, 3. vector[c(1, 5, 6)] # Elements 1, 5, 6. vector[c(T, T, F, F, T)] # Elements 1, 2, 5. Careful, special behavior of R: If you specify fewer values than needed, the input vector will be repeated. This is called “recycling”. vector[c(T, F)] # Does NOT work! For a list, it’s basically the same, except the following points: Elements in lists usually have a name, so you can also access those via list$name. Lists accessed with [] return a list. If you want to select a single element, you have to access it via [[]], as in list[[2]]. myList = list(a = 1, b = &quot;dog&quot;, c = TRUE) myList[1] ## $a ## [1] 1 myList[[1]] ## [1] 1 myList$a ## [1] 1 For data.frames and other objects with dimension &gt; 2, the same is true, except that you have several indices. matrix = matrix(1:16, nrow = 4) matrix[1, 2] # Element in first row, second column. matrix[1:2,] # First two rows, all columns. matrix[, c(T, F ,T)] # All rows, 1st and 3rd column. The syntax matrix[1,] is also called slicing, for obvious reasons. Data.frames are the same as matrices, except that, like with lists of vectors, you can also access columns via names as in data.frame$column. This is because a data.frame ist a list of vectors. 2.3.2 Logic and Slicing Slicing is very powerful if you combine it with logical operators, such as “&amp;” (logical and), “|” (logical or), “==” (equal), “!=” (not equal), “&lt;=”, “&gt;”, etc. Here are a few examples: head(iris[iris$Species == &quot;virginica&quot;, ]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica Note that this is identical to using the subset command: head(subset(iris, Species == &quot;virginica&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica You can also combine several logical commands: iris[iris$Species == &quot;virginica&quot; &amp; iris$Sepal.Length &gt; 7, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 103 7.1 3.0 5.9 2.1 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica 2.4 Applying Functions and Aggregates Across a Data Set Apart from selecting data, you will often combine or calculate statistics on data. 2.4.1 Functions Maybe this is a good time to remind you about functions. The two basic options we use in R are: Variables / data structures. Functions. We have already used variables / data structures. Variables have a name and if you type this name in R, you get the values that are inside the respective data structure. Functions are algorithms that are called like: function(variable) For example, you can do: summary(iris) If you want to know what the summary function does, type ?summary, or put your mouse on the function and press “F1”. To be able to work properly with data, you have to know how to define your own functions. This works like the following: squareValue = function(x){ temp = x * x return(temp) } Tasks Try what happens if you type in squareValue(2). Write a function for multiplying 2 values. Hint: This should start with function(x1, x2). Change the first line of the squareValue function to function(x = 3) and try out the following commands: squareValue(2), squareValue(). What is the sense of this syntax? Solution 1 multiply = function(x1, x2){ return(x1 * x2) } 2 squareValue(2) ## [1] 4 3 squareValue = function(x = 3){ temp = x * x return(temp) } squareValue(2) ## [1] 4 squareValue() ## [1] 9 The given value (3 in the example above) is the default value. This value is used automatically, if no value is supplied for the respective variable. Default values can be specified for all variables, but you should put them to the end of the function definition. Hint: In R, it is always useful to name the parameters when using functions. Look at the following example: testFunction = function(a = 1, b, c = 3){ return(a * b + c) } testFunction() ## Error in testFunction(): argument &quot;b&quot; is missing, with no default testFunction(10) ## Error in testFunction(10): argument &quot;b&quot; is missing, with no default testFunction(10, 20) ## [1] 203 testFunction(10, 20, 30) ## [1] 230 testFunction(b = 10, c = 20, a = 30) ## [1] 320 2.4.2 The apply() Function Now that we know functions, we can introduce functions that use functions. One of the most important is the apply function. The apply function applies a function of a data structure, typically a matrix or data.frame. Try the following: apply(iris[,1:4], 2, mean) Tasks Check the help of apply to understand what this does. Why is the first result of apply(iris[,1:4], 2, mean) NA? Check the help of mean to understand this. Try apply(iris[,1:4], 1, mean). Think about what has changed here. What would happen if you use iris instead of iris[,1:4]? Solution 1 ?apply 2 Remember, what we have done above (if you run this part separately, execute the following lines again): iris$Sepal.Length[2] = &quot;Hund&quot; iris$Sepal.Length = as.numeric(iris$Sepal.Length) ## Warning: NAs introduced by coercion apply(iris[,1:4], 2, mean) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## NA 3.057333 3.758000 1.199333 Taking the mean of a character sequence is not possible, so the result is NA (Not Available, missing value(s)). But you can skip missing values with the option na.rm = TRUE of the mean function. To use it with the apply function, pass the argument(s) after. apply(iris[,1:4], 2, mean, na.rm = T) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.849664 3.057333 3.758000 1.199333 3 apply(iris[,1:4], 1, mean) ## [1] 2.550 NA 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500 ## [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650 ## [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400 ## [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350 ## [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300 ## [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550 ## [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850 ## [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525 ## [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575 ## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675 ## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025 ## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550 ## [145] 4.550 4.300 3.925 4.175 4.325 3.950 Arrays (and thus matrices, data.frame(s), etc.) have several dimensions. For a simple 2D array (or matrix), the first dimension is the rows and the second dimension is the columns. The second parameter of the “apply” function specifies the dimension of which the mean should be computed. If you use 1, you demand the row means (150), if you use 2, you request the column means (5, resp. 4). 4 apply(iris, 2, mean) ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: ## returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: ## returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: ## returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: ## returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: ## returning NA ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## NA NA NA NA NA The 5th column is “Species”. These values are not numeric. So the whole data.frame is taken as a data.frame full of characters. apply(iris[,1:4], 2, str) ## num [1:150] 5.1 NA 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## NULL apply(iris, 2, str) ## chr [1:150] &quot;5.1&quot; NA &quot;4.7&quot; &quot;4.6&quot; &quot;5.0&quot; &quot;5.4&quot; &quot;4.6&quot; &quot;5.0&quot; &quot;4.4&quot; &quot;4.9&quot; &quot;5.4&quot; ... ## chr [1:150] &quot;3.5&quot; &quot;3.0&quot; &quot;3.2&quot; &quot;3.1&quot; &quot;3.6&quot; &quot;3.9&quot; &quot;3.4&quot; &quot;3.4&quot; &quot;2.9&quot; &quot;3.1&quot; ... ## chr [1:150] &quot;1.4&quot; &quot;1.4&quot; &quot;1.3&quot; &quot;1.5&quot; &quot;1.4&quot; &quot;1.7&quot; &quot;1.4&quot; &quot;1.5&quot; &quot;1.4&quot; &quot;1.5&quot; ... ## chr [1:150] &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.4&quot; &quot;0.3&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.1&quot; ... ## chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... ## NULL Remark: The “NULL” statement is the return value of apply. str returns nothing (but prints something out), so the returned vector (or array, list, …) is empty, just like: c() ## NULL 2.4.3 The aggregate() Function aggregate() calculates a function per grouping variable. Try out this example: aggregate(. ~ Species, data = iris, FUN = max) ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 setosa 5.8 4.4 1.9 0.6 ## 2 versicolor 7.0 3.4 5.1 1.8 ## 3 virginica 7.9 3.8 6.9 2.5 Note that max` is the function to get the maximum value, and has nothing to do with your lecturer, who should be spelled Max. The dot is general R syntax and usually refers to “use all columns in the data set”. 2.4.4 For loops Apply and aggregate are convenience function for a far more general concept that exists in all programming language, which is the for loop. In R, a for loop look like this: for (i in 1:10){ #doSomething } and if it is executed, it will excecute 10 times the main block in the curly brackes, while counting the index variable i from 1:10. To demonstrate this, let’s execute a shorter for lool, going from 1:3, and printing i for (i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 For loops are very useful when you want to execute the same task many times. This can be for plotting, but also for data manipulation. For example, if I would like to re-programm the apply function with a for loop, it would look like that: apply(iris[,1:4], 2, mean, na.rm = T) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.849664 3.057333 3.758000 1.199333 out = rep(NA, 4) for (i in 1:4){ out[i] = mean(iris[,i]) } out ## [1] NA 3.057333 3.758000 1.199333 2.5 Plotting I assume that you have already made plots with R. Else here is a super-quick 5-min introduction video. In this course, we will not be using a lot graphics, but it will be useful for you to know the basic plot commands. Note in particular that the following two commands are identical: plot(iris$Sepal.Length, iris$Sepal.Width) plot(Sepal.Width ~ Sepal.Length, data = iris) The second option is preferable, because it allows you to subset data easier and can be directly copied to regression functions. plot(Sepal.Width ~ Sepal.Length, data = iris[iris$Species == &quot;versicolor&quot;, ]) The plot command will use the standard plot depending on the type of variable supplied. For example, if the x axis is a factor, a boxplot will be produced. plot(Sepal.Width ~ Species, data = iris) You can change color, size, shape etc. and this is often useful for visualization. plot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, cex = iris$Petal.Length) For more help on plotting, I recommend: Read “Fundamentals of Data Visualization” by Claus O. Wilke (explains all standard plots and why / when to use them) Data to Viz provides a decision tree for visualizations and links to the R graph gallery "],["understanding_linear_regression.html", "3 Understanding Linear Regression 3.1 Simple Linear Regression 3.2 Multiple Regression 3.3 Model Choice and Causal Inference 3.4 Case studies", " 3 Understanding Linear Regression This chapter is a reminder about the basic regression model functions in R. Here a warm-up exercise: Fit the regression: summary(lm(Ozone ~ Wind, data = airquality)) And answer / discuss with your partner the following questions: What is the effect of Wind on Ozone? How important is Wind to explain Ozone? Next, run the following regressions: summary(lm(Ozone ~ Wind + Temp, data = airquality)) summary(lm(Ozone ~ Wind * Temp, data = airquality)) Why does the effect of Wind on Ozone change so much as we change the formula? What is the “true” or correct estimate of the effect of Wind on Ozone? At the end of this chapter, you should be able to answer all these questions! 3.1 Simple Linear Regression OK, after our warm-up, let’s start with the basics. We will again used the data set airquality, which is built-in in R. If you don’t know the data set, have a look at the description via ?airquality and at the variables via str(airquality) To get started, let’s say we want to examine the relationship between Ozone and Wind. Let’s visualize this first: plot(Ozone ~ Wind, data = airquality) OK, I would say there is some dependency there. To quantify this numerically, you could also run cor(airquality$Ozone, airquality$Wind, use = &quot;complete.obs&quot;) to get the (Pearson) correlation, which is negative: -0.6015465. What we want to do now is fitting regression models through the data with the lm() function of R. The function name lm is short for “linear model”. However, remember from the basic course: This model is not called linear because we necessarily fit a linear function. It’s called linear because we express the response (in our case Wind) as a polynomial of the predictor(s). That means, the predictors have linear coefficients but they might themselves be for example quadratic or sinus terms. So \\(y = \\operatorname{f}(x) + \\mathcal{N}(0, \\sigma)\\), where \\(\\operatorname{f}\\) is a polynomial, e.g. \\({a}_{0} + {a}_{1} \\cdot x + {a}_{2} \\cdot {x}^{2}\\), and \\(\\mathcal{N}(0, \\sigma)\\) means that we assume the data scattering as a normal (Gaussian) distribution with unknown standard deviation \\(\\sigma\\) around \\(\\operatorname{f}(x)\\). The model is called linear because when estimating the unknown parameters (we call them “effects”) of the polynomial, we will see that they are all affecting the predictions linearly, and can thus be solved as a system of linear equations. 3.1.1 Fitting and Interpreting the Regression For fitting a line through this data, we have 3 options: Fit a horizontal line (intercept only). Fit only the slope, but assume the line goes through the origin (0, 0). Fit slope and intercept. Option 3 is the most common case, but we will discuss all 3 options here. Intercept Only Model The following code fits an intercept only model, meaning that we assume the line is perfectly flat, and we only adjust it’s height (the intercept). fit = lm(Ozone ~ 1, data = airquality) We can visualize the result via plot(Ozone ~ Wind, data = airquality) abline(fit) and get a summary of the fitted regression coefficients via summary(fit) ## ## Call: ## lm(formula = Ozone ~ 1, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.13 -24.13 -10.63 21.12 125.87 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.129 3.063 13.76 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.99 on 115 degrees of freedom ## (37 observations deleted due to missingness) We will talk more about this summary later, but for the moment, let’s look only at the coefficients. This tells us that We estimate the mean Ozone (our line) to be at \\(42.12 \\pm 3.1\\) units. The value is significantly different from zero (the t-test always tests \\({H}_{0}\\): “The estimate is zero”). By the way, the value for the intercept is identical to mean(airquality$Ozone, na.rm = T). This is no accident, as the mean is the maximum likelihood estimation for the mean of the normal distribution. Slope Only Model Although rarely sensible, you can also fit a model with just a slope. This only makes sense if you are sure that the line must go through the origin (0, 0) for physical or biological reasons. fit = lm(Ozone ~ Wind + 0, data = airquality) summary(fit) # Alternative for removing the linear term: fit = lm(Ozone ~ Wind - 1, data = airquality) summary(fit) In the results, you can see that we estimate a positive slope, in contradiction to our visual assessment that the data seems negatively correlated. This is because we are forcing the regression line to go through the origin (0, 0). ## ## Call: ## lm(formula = Ozone ~ Wind - 1, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -55.11 -19.34 -2.45 35.71 157.32 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Wind 3.1398 0.3742 8.391 1.4e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 42.25 on 115 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.3798, Adjusted R-squared: 0.3744 ## F-statistic: 70.41 on 1 and 115 DF, p-value: 1.404e-13 plot(Ozone ~ Wind, data = airquality) abline(fit) Slope and Intercept The most common case will be a model with slope and intercept which is probably corresponds most with our visual assessment. fit = lm(Ozone ~ Wind, data = airquality) plot(Ozone ~ Wind, data = airquality) abline(fit) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Wind, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.572 -18.854 -4.868 15.234 90.000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 96.8729 7.2387 13.38 &lt; 2e-16 *** ## Wind -5.5509 0.6904 -8.04 9.27e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.47 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.3619, Adjusted R-squared: 0.3563 ## F-statistic: 64.64 on 1 and 114 DF, p-value: 9.272e-13 This time, we want to look in full at the regression table. Recall that: “Call” repeats the regression formula. “Residuals” gives you an indication about how far the observed data scatters around the fitted regression line / function. The regression table (starting with “Coefficients”) provides the estimated parameters, one row for each fitted parameter. The first column is the estimate, the second (standard error) is the 0.63 confidence interval (for 0.95 confidence interval multiply with 1.96), and the fourth column is the p-value for a two-sided test with \\({H}_{0}\\): “Estimate is zero”. The t-value is used for calculation of the p-value and can usually be ignored. The last section of the summary provides information about the model fit. Residual error = Standard deviation of the residuals, 114 df = Degrees of freedom = Observed - fitted parameters. R-squared \\(\\left({R}^{2}\\right)\\) = How much of the signal, respective variance is explained by the model, calculated by \\(\\displaystyle 1 - \\frac{\\text{residual variance}}{\\text{total variance}}\\). Adjusted R-squared = Adjusted for model complexity. F-test = Test against intercept only model, i.e. is the fitted model significantly better than the intercept only model (most relevant for models with &gt; 1 predictor). Discussion What is the meaning of “An effect is not significant”? Solution You should NOT say that the effect is zero, or that the null hypothesis has been accepted. Official language is “there is no significant evidence for an effect(p = XXX)”. If we would like to assess what that means, some people do a post-hoc power analysis (which effect size could have been estimated), but better is typically just to discuss the confidence interval, i.e. look at the confidence interval and say: if there is an effect, we are relatively certain that it is smaller than X, given the confidence interval of XYZ. Discussion Is an effect with three *** more significant / certain than an effect with one *? Solution Many people view it that way, and some even write “highly significant” for *** . It is probably true that we should have a slightly higher confidence in a very small p-value, but strictly speaking, however, there is only significant, or not significant. Interpreting the p-value as a measure of certainty is a slight misinterpretation. Again, if we want to say how certain we are about the effect, it is better to look again at the confidence interval, i.e. the standard error and use this to discuss the precision of the estimate (small confidence interval / standard error = high precision / certainty). Task Fit simple (univariate) linear regression models for the other two numeric variables (Temp and Solar.R) and interpret the results with your partner. Solution fit = lm(Ozone ~ Temp, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.9955 18.2872 -8.038 9.37e-13 *** ## Temp 2.4287 0.2331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 plot(Ozone ~ Temp, data = airquality) abline(fit) Temperature seems to have a positive effect of Ozone and this effect is significant. The intercept (value for Ozone at Temp = 0) is negative and also significant. This model explains nearly 50% of the variance of the given data. This holds even for the complexity adjusted \\({R}^{2}\\) measure. 37 observations have missing data and are omitted. Compared to the model with only an intercept, this model is significantly different. fit = lm(Ozone ~ Solar.R, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Solar.R, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.292 -21.361 -8.864 16.373 119.136 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.59873 6.74790 2.756 0.006856 ** ## Solar.R 0.12717 0.03278 3.880 0.000179 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31.33 on 109 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.1213, Adjusted R-squared: 0.1133 ## F-statistic: 15.05 on 1 and 109 DF, p-value: 0.0001793 plot(Ozone ~ Solar.R, data = airquality) abline(fit) Solar.R seems to have a positive effect of Ozone and this effect is significant. The intercept (value for Ozone at Solar.R = 0) is positive and also significant. This model explains slightly more than 10% of the variance of the given data. This holds even for the complexity adjusted \\({R}^{2}\\) measure. 42 observations have missing data and are omitted. Thus this model has not the power of the previous one. Compared to the model with only an intercept, this model is significantly different. 3.1.2 Centering and Scaling of Predictors In the last model fit = lm(Ozone ~ Wind, data = airquality) summary(fit) we saw an intercept of 96 for the Wind parameter. Per definition, the intercept is the predicted value for \\(y\\) (Ozone) at \\(x\\) (Wind) = 0. It’s fine to report this, as long as we are interested in this value. However, there are certain situations where the value at predictor = 0 is not particularly interesting. Let’s look at the regression for Temp, for example: fit = lm(Ozone ~ Temp, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.9955 18.2872 -8.038 9.37e-13 *** ## Temp 2.4287 0.2331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 Here, the intercept is -146, which doesn’t make much sense for an ozone concentration, which should be positive. We can see the reason when we plot the results: plot(Ozone ~ Temp, data = airquality, xlim = c(-10, 110), ylim = c(-200, 170)) abline(fit) abline(h = 0, lty = 2) abline(v = 0, lty = 2) That shows us that the value 0 is far outside of the set of our observed values for Temp, which is measured in Fahrenheit. Thus, we are extrapolating the Ozone far beyond the observed data. What we can do to avoid this is to simply re-define the x-Axis, by subtracting the mean, which is called centering: airquality$cTemp = airquality$Temp - mean(airquality$Temp) Alternatively, you can center with the build-in R command scale airquality$cTemp = scale(airquality$Temp, center = T, scale = F) Fitting the model with the centered variable fit = lm(Ozone ~ cTemp, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ cTemp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.1576 2.2018 19.15 &lt;2e-16 *** ## cTemp 2.4287 0.2331 10.42 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 produces a more interpretable value for the intercept. We can see this also visual if we plot the results, i.e. the Ozone concentration at the mean observed temperature. plot(Ozone ~ cTemp, data = airquality) abline(fit) abline(v = 0, lty = 2) When we center, the intercept of the centered variable can be interpreted as the Ozone concentrate at the mean temperature. This value will also typically be very similar to the grand mean mean(airquality$Ozone). Another very common transformation is to divide the x axis by its standard deviation. This is called scaling. airquality$sTemp = airquality$Temp / sd(airquality$Temp) Fitting the model with the scaled variable mainly changes the estimate of the regression slope fit = lm(Ozone ~ sTemp, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ sTemp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.995 18.287 -8.038 9.37e-13 *** ## sTemp 22.988 2.207 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 which is now around 23 (before it was 2.4). The difference in interpretation is the following: for the unscaled variable, we estimate the effect of 1 unit change of temperature on Ozone. For the scaled variable, we estimate the effect of a temperature change of 1 sd of the temperature values, so we can interpret this as an Ozone effect scaled to typical temperature differences in the data. Task Have a look at the results below, where we apply linear transformations on a variable (linear = either subtract / add something to the variable, or multiply / divide the variable by a certain value). How does the transformation change the regression’s estimates? Solution Original model fit = lm(Ozone ~ Temp, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.9955 18.2872 -8.038 9.37e-13 *** ## Temp 2.4287 0.2331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 plot(Ozone ~ Temp, data = airquality, main = &quot;Standard&quot;) abline(fit) Additive transformation change the intercept value, all p-values, CIs stay the same (except for the intercept, as the test changes) airquality$TempAdd = airquality$Temp + 10 fit = lm(Ozone ~ TempAdd, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ TempAdd, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -171.2825 20.6034 -8.313 2.22e-13 *** ## TempAdd 2.4287 0.2331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 plot(Ozone ~ TempAdd, data = airquality, main = &quot;Addition + 10&quot;) abline(fit) Multiplicative transformations change the slope value, p-values and relative CIs for intercept and slope stay the same. airquality$TempMult = airquality$Temp * 10 fit = lm(Ozone ~ TempMult, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ TempMult, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.99549 18.28717 -8.038 9.37e-13 *** ## TempMult 0.24287 0.02331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 plot(Ozone ~ TempMult, data = airquality, main = &quot;Multiplication * 10&quot;) abline(fit) Combinations of both have both effects together airquality$TempMix = airquality$Temp * 0.1 - 10 fit = lm(Ozone ~ TempMix, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ TempMix, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 95.875 5.609 17.09 &lt;2e-16 *** ## TempMix 24.287 2.331 10.42 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 plot(Ozone ~ TempMix, data = airquality, main = &quot;Mixed&quot;) abline(fit) Pro Task Look at the centered and uncentered regression models fit1 = lm(Ozone ~ Temp, data = airquality) summary(fit1) ## ## Call: ## lm(formula = Ozone ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -146.9955 18.2872 -8.038 9.37e-13 *** ## Temp 2.4287 0.2331 10.418 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 fit2 = lm(Ozone ~ cTemp, data = airquality) summary(fit2) ## ## Call: ## lm(formula = Ozone ~ cTemp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.729 -17.409 -0.587 11.306 118.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.1576 2.2018 19.15 &lt;2e-16 *** ## cTemp 2.4287 0.2331 10.42 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.71 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 ## F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 Why do the confidence intervals (Std. Error) on the intercept in the two models (centered and uncentered) differ? To get an idea, look at the effect plots (library effects) for the model. You can also run compare vcov(fit) (calculates variance-covariance matrix) for both models. Solution library(effects) preList = list(Temp = seq(-10, 110, 1)) plot(effect(&quot;Temp&quot;, fit1, xlevels = preList), main = &quot;Standard&quot;) preList = list(cTemp = seq(-10, 110, 1)) plot(effect(&quot;cTemp&quot;, fit2, xlevels = preList), main = &quot;Centered&quot;) vcov(fit1) ## (Intercept) Temp ## (Intercept) 334.420718 -4.23230774 ## Temp -4.232308 0.05435046 vcov(fit2) ## (Intercept) cTemp ## (Intercept) 4.848002921 0.000633905 ## cTemp 0.000633905 0.054350459 Solution: both centered and uncentered inherently fit the same model, but uncertainty of the intercept for the uncentered model is higher, because this is wide outside the data area, thus we are extrapolating. Task 3.1.3 Residual Checks So far, we fitted a regression model, but we didn’t check if the model assumptions fit to the data. Actually, in quite a few examples above we actually saw quite bad fits. For example, let’s take the slope only model lm(Ozone ~ Wind - 1, data = airquality), where we assumed that the regression line should go through (0, 0). Maybe we have good reasons to think that this should be the case biologically, but our data seem to suggest a different behavior. Wht about the slope and intercept model? Also here, if we plot the predicitons, it seems the model systematically underpredicts Ozone for low Wind, and overpredicts for high Wind. fit = lm(Ozone ~ cTemp, data = airquality) plot(Ozone ~ cTemp, data = airquality) abline(fit) We can see this a bit better if we use the effects.{R} package, which we will use from now on for doing result plots for regression models. library(effects) plot(allEffects(fit, partial.residuals = T)) ## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the ## predictor cTemp is a one-column matrix that was converted to a vector Here, the blue line is the fitted model (with confidence interval in light blue), purple circles are the data, and the purple line is a nonparametric fit to the data. What we see highlighted here is that the data seems to follow a completely different curve than the fitted model. The conclusion here would be: The model we are fitting does not fit to the data, we should not interpret its outputs, but rather say that we reject it, it’s the wrong model, we have to search for a more appropriate description of the data. Let’s look at the same plot for the following model: fit = lm(Ozone ~ Wind + Temp, data = airquality) plot(allEffects(fit, partial.residuals = T)) This looks already better, but there seems to be still a bit of a pattern regarding the scattering of the observed data around the regression line. We can get the difference between model and observations via residuals(fit), and we could plot them against the model predictions (which can be obtained via the predict function) via plot(residuals(fit) ~ predict(fit)) abline(h = 0) Remember: The model assumes that the data scatters with a homogenous normal distribution around the regression predictions (which is the 0 line here). What seems to happen, however, is that the scatter increases towards higher predictions, and there also seems to be a tendency towards underprediction at the high and low end. To better analyse these residuals (and potential problems), R offers a function for residual plots. It produces 4 plots. I think it’s most convenient plotting them all into one figure, via par(mfrow = c(2, 2)) which produces a figure with 2 x 2 = 4 panels. par(mfrow = c(2, 2)) plot(fit) Interpretation: Residuals vs Fitted: Shows misfits and wrong functional form. Scattering should be uniformly distributed. Normal Q-Q: Checks if residuals follow an overall normal distribution. Bullets should lie on the line in the middle of the plot and may scatter a little bit at the ends. Scale - Location: Checks for heteroskedasticity. Does the variance change with predictions/changing values? Scattering should be uniformly distributed. Residuals vs Leverage: How much impact do outliers have on the regression? Data points with high leverage should not have high residuals and vice versa. Bad points lie in the upper right or in the lower right corner. This is measured via the Cook’s distance. Distances higher than 0.5 indicate candidates for relevant outliers or strange effects. Important: Residuals are always getting better for more complex models. They should therefore NOT solely be used for model selection. Select your model structure in a different way, residual checks are just for doing a final check to see if the fitted model makes sense. Generally: If you want to do model selection, control for model complexity. The more complex the model, the higher the cost related to the increase of accuracy. Task Modify the formula to get (as far as possible) an acceptable fit to the data. Consider the following options: fit = lm(Ozone ~ Wind, data = airquality) # Intercept + slope. fit = lm(Ozone ~ 1, data = airquality) # Only intercept. fit = lm(Ozone ~ Wind - 1 , data = airquality) # Only slope. fit = lm(Ozone ~ log(Wind), data = airquality) # Predictor variables can be transformed. fit = lm(Ozone^0.5 ~ Wind, data = airquality) # Output variables can also be transformed. fit = lm(Ozone ~ Wind + I(Wind^2), data = airquality) # Mathematical functions with I() command. library(MASS) fit = lm(Ozone ~ Wind, data = airquality) # Calculates optimal transformation for Ozone^lambda to achieve residuals as normally distributed as possible. boxcox(fit) Annotation: In the picture above, you can see, that the 95% confidence interval of the best \\(\\lambda\\) lies approximately in \\([0.15, 0.5]\\). Solution Possible solution, adding a quadratic predictor and chosing a power of 0.35 transformation based on the boxcox function: fit1 = lm(Ozone^0.35 ~ Wind + I(Wind^2), data = airquality) plot(allEffects(fit1, partial.residuals = T), selection = 1) You could get even better fit by adding more and more predictors, as we will discuss on the section on model selection, this model probably overfits: fit2 = lm(Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) + I(Wind^6) + I(Wind^7) + I(Wind^8), data = airquality) plot(allEffects(fit2, partial.residuals = T), selection = 1) We can see this by looking at common model selection indicators (again, more in the section on model selection). AIC comparison (lower = better) AIC(fit1) ## [1] 270.2059 AIC(fit2) ## [1] 274.7512 Likelihood ratio test (is there evidence for the more complex model?) anova(fit1, fit2) ## Analysis of Variance Table ## ## Model 1: Ozone^0.35 ~ Wind + I(Wind^2) ## Model 2: Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) + ## I(Wind^6) + I(Wind^7) + I(Wind^8) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 113 65.112 ## 2 107 61.059 6 4.0528 1.1837 0.3205 3.1.4 Categorical Predictors The lm() function can handle both numerical and categorical variables. To understand what happens if the predictor is categorical, we’ll use another data set here, PlantGrowth (type ?PlantGrowth or F1 help if you want details). We visualize the data via: boxplot(weight ~ group, data = PlantGrowth) A basic lm() Let’s fit an lm() now with the categorical explanatory variable group. They syntax is the same as before: fit = lm(weight ~ group, data = PlantGrowth) summary(fit) ## ## Call: ## lm(formula = weight ~ group, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.0320 0.1971 25.527 &lt;2e-16 *** ## grouptrt1 -0.3710 0.2788 -1.331 0.1944 ## grouptrt2 0.4940 0.2788 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 But the interpretation of the results often leads to confusion. Let’s look at the results of summary(fit). Where did the group ctrl go? The answer is there is a short, and longer answer to this. Let’s first give the short one: ctrl is the intercept, and the other predictors depict the difference between ctrl and the respective levels. So, we could say that ctrl is a kind of “reference”, encoded by the intercept, and we test for a difference of the other levels against this reference. Re-ordering the levels If you want to change which factor level is the reference, you can use: PlantGrowth$group2 = relevel(PlantGrowth$group, &quot;trt1&quot;) Now, we plot boxplot(weight ~ group2, data = PlantGrowth) We see that trt1 is the first level (you can also see this if checking levels() or str() for the factor). Let’s fit the model: fit = lm(weight ~ group2, data = PlantGrowth) summary(fit) ## ## Call: ## lm(formula = weight ~ group2, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.6610 0.1971 23.644 &lt; 2e-16 *** ## group2ctrl 0.3710 0.2788 1.331 0.19439 ## group2trt2 0.8650 0.2788 3.103 0.00446 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 Weird, now suddenly we have a significant difference between the groups. Wasn’t the group difference not significant before? What’s the difference? The answer is that we are still fitting the identical regression model, and if you would do a plot(allEffects(fit)) for the first and second model, it would look the same. However, as the p-values in the regression table always compare against the reference, we now do a comparison (ctr1 vs ctr2) that we didn’t do before, and this comparison is significant. So, if the ordering influences what levels are compared (technically, we call this contrasts, see below), how can we deal with the problem that the order influences which factors are compared. There are three answers for this: First, in many cases, the scientific question / experimental design determines which factor level should be first. In this case, the original reference was ctrl. This clearly stands for control. So, we have a special treatment here (control), and we are probably interested in the contrast between control and the treatments, but not between the different treatments. In this case, we are probably fine. ANOVA (Analysis of Variance) Second, there is a another test that is commonly performed in this case, the ANOVA. We can run this via anov = aov(fit) summary(anov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group2 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And the result is ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group2 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To interpret this, recall that in a nutshell, the ANOVA starts with a base model (in this case intercept only) and adds the variable group. It then measures: How much the model improves in terms of \\({R}^{2}\\) (this is in the column Sum Sq). If this increase of model fit is significant. In this case, we can conclude that the variable group (3 levels) significantly improves model fit, i.e. the group seems to have an overall effect, even though the individual contrasts in the original model where not significant. Post-Hoc Tests Third, if there is no clear reference level, and the ANOVA confirms that the factor has an effect, we may want to compute p-values for all possible combinations of factor levels. This is done via the so-called post-hoc tests: TukeyHSD(anov) The result is: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = fit) ## ## $group2 ## diff lwr upr p adj ## ctrl-trt1 0.371 -0.3202161 1.062216 0.3908711 ## trt2-trt1 0.865 0.1737839 1.556216 0.0120064 ## trt2-ctrl 0.494 -0.1972161 1.185216 0.1979960 This highlights, as before, a significant difference between trt1 and trt2. It is common to visualize the results of the post-hoc tests with the so-called Compact Letter Display (cld). This doesn’t work with the base TukeyHSD function, so we will use the multcomp.{R} pacakge: library(multcomp) fit = lm(weight ~ group, data = PlantGrowth) tuk = glht(fit, linfct = mcp(group = &quot;Tukey&quot;)) summary(tuk) # Standard display. ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = weight ~ group, data = PlantGrowth) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## trt1 - ctrl == 0 -0.3710 0.2788 -1.331 0.391 ## trt2 - ctrl == 0 0.4940 0.2788 1.772 0.198 ## trt2 - trt1 == 0 0.8650 0.2788 3.103 0.012 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) tuk.cld = cld(tuk) # Letter-based display. plot(tuk.cld) The cld gives a new letter for each group of factor levels that are statistically undistinguishable. You can see the output via tuk.cld, here I only show the plot: Task: Categorical analysis for the airquality data set The airquality data set contains a categorical predictor “month”, which, however, is wrongly coded as a numeric value. We can correct this by doing airquality$fMonth = factor(airquality$Month) Execute this code and fit a regression for fMonth! Solution Advanced topic: Changing the contrasts Before, I said that there is a long and short answer to the interpretation of the regression coefficients. Now here is the long answer: If you have a categorical predictor with &gt; 2 levels, there are several ways to set up the model to fit those levels. Maybe the easiest idea would be to fit a mean per level. You can actually tell R to do this via fit = lm(weight ~ 0 + group, data = PlantGrowth) If we look at the output, we see that now we simply get the mean of each group (level): ## ## Call: ## lm(formula = weight ~ 0 + group, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## groupctrl 5.0320 0.1971 25.53 &lt;2e-16 *** ## grouptrt1 4.6610 0.1971 23.64 &lt;2e-16 *** ## grouptrt2 5.5260 0.1971 28.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.9867, Adjusted R-squared: 0.9852 ## F-statistic: 665.5 on 3 and 27 DF, p-value: &lt; 2.2e-16 Why does R not do that by default? Because now, we see the comparison of each group against zero in the p-values. In some cases, this can be interesting, but in most cases where we have a control and treatment and are interested in the difference between treatment and control, this is not informative. Therefore, R uses the so-called treatment contrasts, which is what we had before. There are actually a number of further options for specifying contrasts. You can tell R by hand how the levels should be compared or use some of the pre-defined contrasts. Here is an example: PlantGrowth$group3 = PlantGrowth$group contrasts(PlantGrowth$group3) = contr.helmert fit = lm(weight ~ group3, data = PlantGrowth) summary(fit) ## ## Call: ## lm(formula = weight ~ group3, data = PlantGrowth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.07300 0.11381 44.573 &lt; 2e-16 *** ## group31 -0.18550 0.13939 -1.331 0.19439 ## group32 0.22650 0.08048 2.814 0.00901 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 What we are using here is Helmert contrasts, which contrast the second level with the first, the third with the average of the first two, and so on. Which contrasts make most sense depends on the question. For more details, see here: https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x. 3.1.5 Exercise: Global Plant Trait Analysis Look at the plantHeight dataset in Ecodata. Let’s assume we want to analyze whether height of plant species from around the world depends on temperature at the location of occurrence. Note that “loght” = log(height). library(EcoData) model = lm(loght ~ temp, data = plantHeight) summary(model) ## ## Call: ## lm(formula = loght ~ temp, data = plantHeight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.97903 -0.42804 -0.00918 0.43200 1.79893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.225665 0.103776 -2.175 0.031 * ## temp 0.042414 0.005593 7.583 1.87e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6848 on 176 degrees of freedom ## Multiple R-squared: 0.2463, Adjusted R-squared: 0.242 ## F-statistic: 57.5 on 1 and 176 DF, p-value: 1.868e-12 The model suggests a significant global trend of plant height increasing with temperature. Tasks Perform residual checks and modify the model if you think it is necessary. Does the effect still hold? A concern regarding this analysis is that species are not fully independent. E.g., the plant family of Ericaceae, comprising many tiny dwarf shrubs, could have evolved in colder regions by chance. Is the signal still there if we look at families, rather than species? For that, try fitting the regression for the mean per family. Hint: you could use the aggregate() function to get means per family. The data set also includes a categorical variable “growthform”. Test if growthform has an effect on the plant height. Solution 1. par(mfrow = c(2, 2)) plot(model) Looks OK! 2. aggDat = aggregate(. ~ Family, data = plantHeight[, c(4, 7, 14)], FUN = mean) model2 = lm(loght ~ temp, data = aggDat) summary(model2) ## ## Call: ## lm(formula = loght ~ temp, data = aggDat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.16556 -0.38220 0.02092 0.26734 1.38896 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.27817 0.14910 -1.866 0.0665 . ## temp 0.04884 0.00781 6.254 3.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5195 on 66 degrees of freedom ## Multiple R-squared: 0.3721, Adjusted R-squared: 0.3626 ## F-statistic: 39.12 on 1 and 66 DF, p-value: 3.349e-08 summary(aov(model2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## temp 1 10.56 10.56 39.12 3.35e-08 *** ## Residuals 66 17.81 0.27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes, there is still an effect 3. model3 = lm(loght + temp ~ growthform, data = plantHeight) summary(model3) ## ## Call: ## lm(formula = loght + temp ~ growthform, data = plantHeight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.973 -4.362 1.440 5.811 16.561 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.75527 8.62265 1.595 0.113 ## growthformHerb -2.71491 8.72008 -0.311 0.756 ## growthformHerb/Shrub 3.92082 12.19427 0.322 0.748 ## growthformShrub 0.02093 8.71019 0.002 0.998 ## growthformShrub/Tree 11.46166 8.97474 1.277 0.203 ## growthformTree 6.88269 8.69304 0.792 0.430 ## ## Residual standard error: 8.623 on 162 degrees of freedom ## (10 observations deleted due to missingness) ## Multiple R-squared: 0.232, Adjusted R-squared: 0.2083 ## F-statistic: 9.787 on 5 and 162 DF, p-value: 3.451e-08 summary(aov(model3)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## growthform 5 3638 727.7 9.787 3.45e-08 *** ## Residuals 162 12045 74.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 10 observations deleted due to missingness There is also an effect of growth form. Note that the comparisons are against the growth form fern (intercept), which has only one observation, so it may make sense to re-order the factor in the regression so that you compare, e.g., against herbs (will yield more significant comparisons). 3.2 Multiple Regression Multiple (linear) regression means that we consider more than 1 predictor in the same model. The syntax is very easy: Just add your predictors (numerical or categorical) to your regression formula, as in the following example for the airquality dataset. To be able to also add a factor, I created a new variable fMonth to have month as a factor (categorical): airquality$fMonth = factor(airquality$Month) fit = lm(Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality) The resulting regression table looks already a bit intimidating, but in principle everything is interpreted as before: ## ## Call: ## lm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.344 -13.495 -3.165 10.399 92.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -74.23481 26.10184 -2.844 0.00537 ** ## Temp 1.87511 0.34073 5.503 2.74e-07 *** ## Wind -3.10872 0.66009 -4.710 7.78e-06 *** ## Solar.R 0.05222 0.02367 2.206 0.02957 * ## fMonth6 -14.75895 9.12269 -1.618 0.10876 ## fMonth7 -8.74861 7.82906 -1.117 0.26640 ## fMonth8 -4.19654 8.14693 -0.515 0.60758 ## fMonth9 -15.96728 6.65561 -2.399 0.01823 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.72 on 103 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.6122 ## F-statistic: 25.81 on 7 and 103 DF, p-value: &lt; 2.2e-16 Luckily, we also have the effect plots to make sense of this: plot(allEffects(fit, partial.residuals = T) ) Multiple regression != A lot of univariate regressions A common misunderstanding is that the above regression simply amounts to 4 independent univariate regressions. Let’s look at the model fit = lm(Ozone ~ Wind , data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Wind, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.572 -18.854 -4.868 15.234 90.000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 96.8729 7.2387 13.38 &lt; 2e-16 *** ## Wind -5.5509 0.6904 -8.04 9.27e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.47 on 114 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.3619, Adjusted R-squared: 0.3563 ## F-statistic: 64.64 on 1 and 114 DF, p-value: 9.272e-13 The estimated effect is - 5.55, while in the multiple regression, we had -3.1. What’s going on? The reason is that Wind and Temp are correlated (the technical term is collinear). You can see this by running plot(Wind ~ Temp, data = airquality) This means that if we take Temp out of the model, Wind will absorb a part of the effect of Temp, or, to put it differently: If we include Temp in the model, the model will fit the effect of Wind after removing the effect that can be explained by Temp, and vice versa. Task Try out different combinations of predictors and observe how the estimates change. Try to find the predictor combination for which the effect of Wind on Temp is maximal. Solution So, which effect is the correct one, the univariate or the multivariate model? We will speak about the rules when to put variables in and out of the regression later, in the chapter on model choice. For the moment, however, note that if two variables correlate, including or removing one will change the estimate for the other. Remember: If there is collinearity, including one variable changes the effect size for other variables! 3.2.1 Understanding the Effect of Collinearity We can understand the problem of one variable influencing the effect of the other in more detail if we simulate some data. Let’s create 2 positively collinear predictors: x1 = runif(100, -5, 5) x2 = x1 + 0.2*runif(100, -5, 5) We can check whether this has worked, through visual inspection as well as by calculating the correlation coefficient: plot(x1, x2) cor(x1, x2) ## [1] 0.9823957 The first case I want to look at, is when effect1 and effect2 have equal sign. Let’s create such a situation, by simulating a normal response \\(y\\), where the intercept is 0, and both predictors have effect = 1: y = 0 + 1*x1 + 1*x2 + rnorm(100) In this case, univariate models have too high effect sizes, because in conjunction, 1) positive correlation between predictors and 2) equal effect direction can lead to predictors absorbing each other’s effect if one is taken out: coef(lm(y ~ x1)) ## (Intercept) x1 ## 0.1223649 2.0490812 coef(lm(y ~ x2)) ## (Intercept) x2 ## -0.1093031 1.9838618 You can also see this visually: par(mfrow = c(1, 2)) plot(x1, y, main = &quot;x1 effect&quot;, ylim = c(-12, 12)) abline(lm(y ~ x1)) # Draw a line with intercept 0 and slope 1, # just like we simulated the true dependency of y on x1: abline(0, 1, col = &quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;fitted&quot;, &quot;true&quot;), lwd = 1, col = c(&quot;black&quot;, &quot;red&quot;)) plot(x2, y, main = &quot;x2 effect&quot;, ylim = c(-12, 12)) abline(lm(y ~ x2)) abline(0, 1, col = &quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;fitted&quot;, &quot;true&quot;), lwd = 1, col = c(&quot;black&quot;, &quot;red&quot;)) The multivariate model, on the other hand, gets the right estimates (with a bit of error): coef(lm(y~x1 + x2)) ## (Intercept) x1 x2 ## 0.04269288 1.33989604 0.70264737 Task Check what happens if the 2 effects have opposite sign. Solution x1 = runif(100, -5, 5) x2 = -x1 + 0.2*runif(100, -5, 5) y = 0 + 1*x1 + 1*x2 + rnorm(100) cor(x1, x2) ## [1] -0.9797563 coef(lm(y ~ x1)) ## (Intercept) x1 ## -0.21750989 0.03526607 coef(lm(y ~ x2)) ## (Intercept) x2 ## -0.2111353836 0.0008128962 par(mfrow = c(1, 2)) plot(x1, y, main = &quot;x1 effect&quot;, ylim = c(-12, 12)) abline(lm(y ~ x1)) abline(0, 1, col = &quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;fitted&quot;, &quot;true&quot;), lwd = 1, col = c(&quot;black&quot;, &quot;red&quot;)) plot(x2, y, main = &quot;x2 effect&quot;, ylim = c(-12, 12)) abline(lm(y ~ x2)) abline(0, 1, col = &quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;fitted&quot;, &quot;true&quot;), lwd = 1, col = c(&quot;black&quot;, &quot;red&quot;)) coef(lm(y~x1 + x2)) ## (Intercept) x1 x2 ## -0.2164637 0.8995268 0.8951650 Both effects cancel out. 3.2.2 Scaling Variables in the Multiple Regression Before, we had already computed the regression table for a regression with 4 predictors: airquality$fMonth = factor(airquality$Month) fit = lm(Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.344 -13.495 -3.165 10.399 92.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -74.23481 26.10184 -2.844 0.00537 ** ## Temp 1.87511 0.34073 5.503 2.74e-07 *** ## Wind -3.10872 0.66009 -4.710 7.78e-06 *** ## Solar.R 0.05222 0.02367 2.206 0.02957 * ## fMonth6 -14.75895 9.12269 -1.618 0.10876 ## fMonth7 -8.74861 7.82906 -1.117 0.26640 ## fMonth8 -4.19654 8.14693 -0.515 0.60758 ## fMonth9 -15.96728 6.65561 -2.399 0.01823 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.72 on 103 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.6122 ## F-statistic: 25.81 on 7 and 103 DF, p-value: &lt; 2.2e-16 So, which of the predictors is the strongest (= most effect on the response)? Superficially, it looks as if Month has the highest values. But that does mean that Month is the most important? No, and the reason is that we have to remember the effect on the response \\(y = \\text{regression estimate} * \\text{predictor}\\), i.e if we have a predictor with a large range (difference between min/max values), it may have a strong effect even though the estimate is small. So, we cannot compare the effect sizes directly. A small trick that is therefore often applied is to divide all numeric predictors by their standard deviation to bring them all on the same range, which will then be roughly between -2, 2. You can do this by hand, or use the scale() function in R: airquality$sTemp = scale(airquality$Temp) # also performs centering airquality$sTemp = airquality$Temp / sd(airquality$Temp) # only scaling. We do the same for the other numeric variables and run the regression: airquality$sWind = scale(airquality$Wind) airquality$sSolar.R = scale(airquality$Solar.R) fit = lm(Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.344 -13.495 -3.165 10.399 92.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -95.481 23.505 -4.062 9.51e-05 *** ## sTemp 17.748 3.225 5.503 2.74e-07 *** ## sWind -10.952 2.325 -4.710 7.78e-06 *** ## sSolar.R 4.703 2.131 2.206 0.0296 * ## fMonth6 -14.759 9.123 -1.618 0.1088 ## fMonth7 -8.749 7.829 -1.117 0.2664 ## fMonth8 -4.197 8.147 -0.515 0.6076 ## fMonth9 -15.967 6.656 -2.399 0.0182 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.72 on 103 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.6122 ## F-statistic: 25.81 on 7 and 103 DF, p-value: &lt; 2.2e-16 We can compare the effect sizes directly, which suggests that Temp is actually the most important predictor. Note: In the code above, I used scale(...). By default, the scale function will scale and center. As discussed before, centering is nearly always useful as it improves the interpretability of the intercept, so I would suggest to use this as a default when scaling. Tasks Discuss: Under which circumstances should you center / scale, and how should you discuss the estimated coefficients in a paper? Solution Scaling = estimate of relative imporatance. Original units: interpretable as effect per unit change. 3.2.3 ANOVA for Multiple Regression Another option to see which variable is more important is variance partitioning, aka ANOVA. In an ANOVA, we add variable by variable to the model, and see how much the fit to the data (expressed by residual sum of squares) improves. We can do this via fit = lm(Ozone ~ Wind + Temp, data = airquality) summary(aov(fit)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Wind 1 45284 45284 94.81 &lt; 2e-16 *** ## Temp 1 25886 25886 54.20 3.15e-11 *** ## Residuals 113 53973 478 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 37 observations deleted due to missingness So, why has Wind the larger effect, again? Didn’t we just say that Temp has a larger effect? Is there something wrong with our ANOVA? The problem with the aov function is that it performs a so-called type I ANOVA. The type I ANOVA adds variables in the order in which they are in the model formula. If I specify another formula, the result is different: fit = lm(Ozone ~ Temp + Wind, data = airquality) summary(aov(fit)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temp 1 61033 61033 127.78 &lt; 2e-16 *** ## Wind 1 10137 10137 21.22 1.08e-05 *** ## Residuals 113 53973 478 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 37 observations deleted due to missingness The difference is due to the collinearity of the variables. Because Temp and Wind are collinear, the variable that is added first to the model will absorb variation from the other, and thus seems to explain more of the response. There are other types of ANOVA that avoid this problem. The so-called type II ANOVA shows for each variable only the part that is uniquely attributable to the respective variable car::Anova(fit, type = &quot;II&quot;) ## Anova Table (Type II tests) ## ## Response: Ozone ## Sum Sq Df F value Pr(&gt;F) ## Temp 25886 1 54.196 3.149e-11 *** ## Wind 10137 1 21.223 1.080e-05 *** ## Residuals 53973 113 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is also type III, which is as type II, but avoids a similar problem for interactions (see next subchapter). This is probably the most conservative setting: car::Anova(fit, type = &quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: Ozone ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 4335 1 9.0763 0.003196 ** ## Temp 25886 1 54.1960 3.149e-11 *** ## Wind 10137 1 21.2230 1.080e-05 *** ## Residuals 53973 113 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here is an overview of the situation for 2 predictors A and B and their interaction. The upper left figure corresponds to the case where we have no collinearity between either of those variables. The figure on the top right (and similarly types I - III) are the three possible types of ANOVA for variables with collinearity. The “overlap” between the circles depicts the shared part, i.e. the variability that can be expressed by either variable (due to collinearity). Note that the shares in Type II, III do not add up to 1, as there is a kind of “dark variation” that we cannot securely add to either variable. Task Try out the difference between type I, II, III ANOVA for the airquality data set, either for the simple Wind + Temp model, or for more complicated models. If you want to see the effects of Type III Anova, you need to add an interaction (see next section). Solution 3.2.4 Interactions When we have multiple variables, we can have the situation that the value of one variable influences the effect of the other(s). Technically, this is called in interaction. In situations where the causal direction is known, this is also called a moderator. An example: Imagine we observe that the effect of aspirin differs depending on the weight of the subject. Technically, we have an interaction between aspirin and weight. Physiologically, we know the causal direction is “weight -&gt; effect of aspirin”, so we can say weight is a moderator for the effect of aspirin. fit = lm(Ozone ~ Temp * Wind, data = airquality) plot(allEffects(fit)) We will have a look at the summary later, but for the moment, let’s just look at the output visually. In the effect plots, we see the effect of Temperature on Ozone for different values of Wind. We also see that the slope changes. For low Wind, we have a strong effect of Temperature. For high Wind, the effect is basically gone. Let’s look at the interaction syntax in more detail. The “*” operator in an lm().{R} is a shorthand for main effects + interactions. You can write equivalently: fit = lm(Ozone ~ Wind + Temp + Wind:Temp, data = airquality) What is fit here is literally a third predictor that is specified as Wind * Temp (normal multiplication). The above syntax would allow you to also have interactions without main effects, e.g.: fit = lm(Ozone ~ Wind + Wind:Temp, data = airquality) Although this is generally never advisable, as the main effect influences the interaction, unless you are sure that the main effect must be zero. There is another important syntax in R: fit = lm(Ozone ~ (Wind + Temp + Solar.R)^2 , data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ (Wind + Temp + Solar.R)^2, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.685 -11.727 -2.169 7.360 91.244 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.408e+02 6.419e+01 -2.193 0.03056 * ## Wind 1.055e+01 4.290e+00 2.460 0.01555 * ## Temp 2.322e+00 8.330e-01 2.788 0.00631 ** ## Solar.R -2.260e-01 2.107e-01 -1.073 0.28591 ## Wind:Temp -1.613e-01 5.896e-02 -2.735 0.00733 ** ## Wind:Solar.R -7.231e-03 6.688e-03 -1.081 0.28212 ## Temp:Solar.R 5.061e-03 2.445e-03 2.070 0.04089 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.17 on 104 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.6863, Adjusted R-squared: 0.6682 ## F-statistic: 37.93 on 6 and 104 DF, p-value: &lt; 2.2e-16 plot(allEffects(fit), selection = 1) plot(allEffects(fit), selection = 2) plot(allEffects(fit), selection = 3) This creates all main effect and second order (aka two-way) interactions between variables. You can also use ^3 to create all possible 2-way and 3-way interactions between the variables in the parentheses. By the way: The ()^2 syntax for interactions is the reason why we have to write I(x^2) if we want to write a quadratic effect in an lm. Categorical variables When you include an interaction with a categorical variable, that means a separate effect will be fit for each level of the categorical variable, as in fit = lm(Ozone ~ Wind * fMonth, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Wind * fMonth, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.528 -12.562 -2.246 10.691 77.750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.748 15.748 3.223 0.00169 ** ## Wind -2.368 1.316 -1.799 0.07484 . ## fMonth6 -41.793 31.148 -1.342 0.18253 ## fMonth7 68.296 20.995 3.253 0.00153 ** ## fMonth8 82.211 20.314 4.047 9.88e-05 *** ## fMonth9 23.439 20.663 1.134 0.25919 ## Wind:fMonth6 4.051 2.490 1.627 0.10680 ## Wind:fMonth7 -4.663 2.026 -2.302 0.02329 * ## Wind:fMonth8 -6.154 1.923 -3.201 0.00181 ** ## Wind:fMonth9 -1.874 1.820 -1.029 0.30569 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.12 on 106 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.5473, Adjusted R-squared: 0.5089 ## F-statistic: 14.24 on 9 and 106 DF, p-value: 7.879e-15 The interpretation is like for a single categorical predictor, i.e. we see the effect of Wind as the effect for the first Month 5, and the Wind:fMonth6 effect, for example, tests for a difference in the Wind effect between month 5 (reference) and month 6. As before, you could change this behavior by changing contrasts. Interactions and centering A super important topic when working with numeric interactions is centering. Task Compare the estimates for Wind / Temp for the following models Ozone ~ Wind Ozone ~ Temp Ozone ~ Wind + Temp Ozone ~ Wind * Temp How do you explain the differences in the estimates for the main effects of Wind and Temp? What do you think corresponds most closely to the “true” effect of Wind and Temp? Maybe you know the answer already. If not, consider the following simulation, where we create data with known effect sizes: # Create predictor variables. x1 = runif(100, -1, 1) x2 = runif(100, -1, 1) # Create response for lm, all effects are 1. y = x1 + x2 + x1*x2 + rnorm(100, sd = 0.3) # Fit model, but shift the mean of the predictor. fit = lm(y ~ x1 * I(x2 + 5)) summary(fit) ## ## Call: ## lm(formula = y ~ x1 * I(x2 + 5)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82652 -0.20877 0.00984 0.20251 0.87495 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.01327 0.28118 -17.829 &lt; 2e-16 *** ## x1 -4.20433 0.49065 -8.569 1.75e-13 *** ## I(x2 + 5) 1.00200 0.05555 18.037 &lt; 2e-16 *** ## x1:I(x2 + 5) 1.03698 0.09623 10.776 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2894 on 96 degrees of freedom ## Multiple R-squared: 0.9179, Adjusted R-squared: 0.9154 ## F-statistic: 357.9 on 3 and 96 DF, p-value: &lt; 2.2e-16 plot(allEffects(fit)) Play around with the shift in x2, and observe how the effects change. Try how the estimates change when centering the variables via the scale() command. If you understand what’s going on, you will realize that you should always center your variables, whenever you use any interactions. Excellent explanations of the issues also in the attached paper https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x. Solution 3.2.5 Exercise: Global Plant Trait Analysis #2 Task Revisit exercise 3.1.5, and test If temp or NPP (net primary productivity) is a more important predictor. If growth forms (variable growthform) differ in their temperature effects. If the effect of temp remains significant if we include latitude and an interaction of latitude with temp. If not, why? Plot temp ~ lat. Ask me to comment on case 3! Solution 3.3 Model Choice and Causal Inference What we saw so far is that there is a large number of models we could fit. But how do we decide which is the “right” one? A basic requirement is that the residuals should more or less fit. It is seldom sensible to use a model that does not fit to the data. Beyond that, however, there is a range of options which is sensible, depending on the purpose of the model. In stats, we distinguish at least 2 basic purposes: Prediction: If our purpose is to build a predictive model, we are searching for the model that makes the smallest possible error on a new data sample. (Causal) inference: When we are speaking about inference, that means we are interested in the estimated effects and we would like them to be identical to the “true” causal effects. There is a further subdivision with regards to prior knowledge: In an exploratory analysis, we have only a vague idea what we are looking for. We might just be scanning the data set for possible (causal) relationships. In a confirmatory analysis, we have a clear target for the analysis, and ideally a plan for which model we want to fit, prior to seeing the data. Depending on the analysis goal, different methods are appropriate, and we will talk about those in this chapter. The most common goal for scientific papers is a confirmatory causal analysis (even though the actual practice does not always follow this). Even within each of these objectives, there are a number of additional criteria that may influence which method and model one will choose for the analysis. For example, Either for predictions or for estimators, do I care more about a small error, or about bias? (Error = typical (mean) difference between estimator and truth; Bias = systematic difference between estimator and truth) Do I want confidence intervals to be correct (coverage), and calibrated p-values? Do we have experimental data, where all predictors are known, measured, and randomized / orthogonal, or do we have observational data, where we do not have controlled predictors, and collinearity / confounding is the norm. All of these play into the choice of model and model selection method. Some methods, for example, produce smaller errors on the estimators, but a larger bias. In this chapter, I will provide you with a rough overview about the methods. We will talk about them in more detail in the next days. Discussion Discuss with your partners: How do you typically choose which regression formula to fit? 3.3.1 The Bias-Variance Trade-off One fundamental idea about modelling choice is the bias-variance trade-off, which applies regardless of whether we are interested in causal effects (next section) or predictions. The idea is the following: The more variables / complexity we include in the model, the better it can (in principle) adjust to the true relationship, thus reducing model error from bias. The more variables / complexity we include in the model, the larger our error (variance) on the fitted coefficients, thus increasing model error from variance. This means, the model adopts to the given data but no longer to the underlying relationship. If we sum both terms up, we see that at the total error of a model that is too simple will be dominated by bias (underfitting), and the total error of a model that is too complex will be dominated by variance (overfitting): We will do some practical simulations on this on Wednesday, for the moment let’s just accept this idea as a fact. 3.3.2 Causal Inference Apart from the bias-variance trade-off, a crucial consideration is if we are just interested in predictions, or in causal effects. If we are after causal effects, the correct selection of variables is crucial, while it isn’t if we just want to predict. This is reviewed in the excellent paper by Lederer et al., which is available here. The basic idea is the following: Let’s first define what we mean by “causality”: Assume we look at the effect of a target variable (something that could be manipulated = predictor) on another variable (the outcome = response) in the presence of other (non-target) variables. The goal of a causal analysis is to control for these other variables, in such a way that we estimate the same effect size we would obtain if only the target predictor was manipulated (as in a randomized controlled trial). You probably have learned in your intro stats class that, to do so, we have to control for confounders. I am less sure, however, if everyone is clear about what a confounder is. In particular, confounding is more specific than having a variable that correlates with predictor and response. The direction is crucial to identify true confounders. For example, C) in the figure below shows a collider, i.e. a variable that is influenced by predictor and response. Although it correlates with predictor and response, correcting for it (or including it) in a multiple regression will create a collider bias on the causal link we are interested in (Corollary: Including all variables is not always a good thing). The bottom line of this discussions (and the essence of Pearl 2000, 2009) is that to establish causality for a specific link, we have to close the so-called back-door paths for this link. So, the strategy for fitting a causal effect is: Start by writing down the hypothesis / structure that you want to estimate causally (for example, in A, B “Plant diversity” -&gt; Ecosystem productivity). Then, include / exclude other variables with the goal of: Controlling for confounders (back-doors, blue paths in the figure). Not controlling for colliders, (something similar, called “M-Bias”,) and other similar relationships (red paths). It depends on the question whether we should control for mediators (yellow paths). Note: These other variables (if included) are just there to correct our estimates (-&gt; called nuisance parameters), and we should later not discuss them, as they were not themselves checked for confounding (Table 2 fallacy). Case study 1 Take the example of the past exercise (airquality) and assume, the goal is to understand the causal effect of Temperature on Ozone (primary hypothesis). Draw a causal diagram to decide which variables to take into the regression (i.e. noting which are confounders, mediators or colliders), and fit the model. Solution Solar.R could affect both Temp, Ozone -&gt; Coufounder, include Wind could affect Temp, Ozone -&gt; Coufounder, include. Alternatively, one could assume that Temp is also affecting Wind, then it’s a mediator I would not include Month, as the Month itself should not affect Ozone, it’s the Temp, Solar.R of the month that must have the effect. It’s more like a placeholder, but if you include it it will nearly act as a collider, because it can snitch away some of the effects of the other variables. Case study 2 Perform a causal, a predictive and an exploratory analysis of the Swiss fertility data set called “swiss”, available in the standard R data sets. Target for the causal analysis is to estimate the causal (separate direct and indirect effects) of education on fertility, i.e. lm(Fertility ~ Education, data = swiss). Solution Agriculture, Catholic could be seen as confounders or mediators, depending on whether you think Education affects the number of people being in Agriculture or Catholic, or vice versa Infant mortality could be a mediator or a collider, depeding on whether you think fertility -&gt; infant mortality or infant mortality -&gt; fertility. I would tend to see it as a mediator. For all mediators: remember that if you want to get the total (indirect + direct) effect of education on fertility, you should not include mediators. If you want to get the direct effect only, they should be included. 3.3.3 Model Selection Methods Regardless of whether we do a causal, exploratory or a predictive analysis, we sometimes may still want to get some aid in deciding on the model structure. Specifically: For a predictive analysis, even if we know the true causal structure, it may be better to fit a simpler model to reduce the bias-variance trade-off. For a causal analysis, we may not be sure about certain relationships, and we may want to test if a particular hypothesis is better supported by the data than another, or we may be data-limited as well, which means we have to reduce complexity. In these situations, model selection methods may help. The key for using them is to understand that neither of them can do magic. If you have a limited data set and a massive number of predictors, they will not magically produce the correct model. However, they can be useful in certain situations. Let’s introduce them first. I discuss possible problems in the next chapter. Likelihood-ratio tests A likelihood-ratio test (LRT) is a hypothesis test that can be used to compare 2 nested models. Nested means that the simpler of the 2 models is included in the more complex model. The more complex model will always fit the data better, i.e. have a higher likelihood. This is the reason why you shouldn’t use fit or residual patterns for model selection. The likelihood-ratio test tests whether this improvement in likelihood is significantly larger than one would expect if the simpler model is the correct model. Likelihood-ratio tests are used to get the p-values in an R ANOVA, and thus you can also use the anova function to perform an likelihood-ratio test between 2 models (Note: For simple models, this will run an F-test, which is technically not exactly a likelihood-ratio test, but the principle is the same): # Model 1 m1 = lm(Ozone ~ Wind , data = airquality) # Model 2 m2 = lm(Ozone ~ Wind + Temp, data = airquality) # LRT anova(m1, m2) ## Analysis of Variance Table ## ## Model 1: Ozone ~ Wind ## Model 2: Ozone ~ Wind + Temp ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 114 79859 ## 2 113 53973 1 25886 54.196 3.149e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 AIC model selection Another method for model selection, and probably the most widely used, also because it does not require that models are nested, is the AIC = Akaike Information Criterion. The AIC is defined as \\(2 \\ln(\\text{likelihood}) + 2k\\), where \\(k\\) = number of parameters. Essentially, this means AIC = Fit - Penalty for complexity. Lower AIC is better! m1 = lm(Ozone ~ Temp, data = airquality) m2 = lm(Ozone ~ Temp + Wind, data = airquality) AIC(m1) ## [1] 1067.706 AIC(m2) ## [1] 1049.741 Note 1: It can be shown that AIC is asymptotically identical to leave-one-out cross-validation, so what AIC is optimizing is essentially the predictive error of the model on new data. Note 2: There are other information criteria, such as BIC, DIC, WAIC etc., as well as sample-size corrected versions of either of them (e.g. AICc). The difference between the methods is beyond the scope of this course. For the most common one (BIC), just the note that this penalizes more strongly for large data sets, and thus corrects a tendency of AIC to overfit for large data sets. Task Compare results of AIC with likelihood-ratio tests. Discuss: When to use one or the other? Solution Shrinkage estimation A third option option for model selection are shrinkage estimators. These include the LASSO and ridge. The basic idea behind these estimators is not to reduce the number of parameters, but to reduce the flexibility of the model by introducing a penalty on the regression coefficients that code a preference for smaller or zero coefficient values. Effectively, this can either amount to model selection (because some coefficients are shrunk directly to zero), or it can mean that we can fit very large models while still being able to do good predictions, or avoid overfitting. To put a ridge penalty on the standard lm, we can use lm.ridge(Ozone ~ Wind + Temp + Solar.R, data = airquality, lambda = 2) ## Wind Temp Solar.R ## -62.73376169 -3.30622990 1.62842247 0.05961015 We can see how the regression estimates vary for different penalties via plot( lm.ridge( Ozone ~ Wind + Temp + Solar.R, data = airquality, lambda = seq(0, 200, 0.1) ) ) 3.3.4 P-hacking The most dubious model selection strategy, actually considered scientific misconduct, is p-hacking. The purpose of this exercises is to show you how not to do model selection, i.e, that by playing around with the variables, you can make any outcome significant. That is why your hypothesis needs to be fixed before looking at the data, ideally through pre-registration, based on an experimental plan or a causal analysis. Here is the example: Measurements of plant performance. Target was to find out if Gen1 has an effect on Performance. Various other variables are measured set.seed(1) dat = data.frame(matrix(rnorm(300), ncol = 10)) colnames(dat) = c(&quot;Performance&quot;, &quot;Gen1&quot;, &quot;Gen2&quot;, &quot;soilC&quot;, &quot;soilP&quot;, &quot;Temp&quot;, &quot;Humidity&quot;, &quot;xPos&quot;, &quot;yPos&quot;, &quot;Water&quot;) summary(dat) ## Performance Gen1 Gen2 soilC ## Min. :-2.21470 Min. :-1.37706 Min. :-1.8050 Min. :-1.2766 ## 1st Qu.:-0.43496 1st Qu.:-0.38752 1st Qu.:-0.5373 1st Qu.:-0.5656 ## Median : 0.25658 Median :-0.05656 Median : 0.1138 Median :-0.1924 ## Mean : 0.08246 Mean : 0.13277 Mean : 0.1103 Mean : 0.1133 ## 3rd Qu.: 0.70870 3rd Qu.: 0.66515 3rd Qu.: 0.5643 3rd Qu.: 0.7126 ## Max. : 1.59528 Max. : 1.98040 Max. : 2.4016 Max. : 1.7673 ## soilP Temp Humidity xPos ## Min. :-1.914359 Min. :-1.48746 Min. :-2.28524 Min. :-2.8889 ## 1st Qu.:-0.733529 1st Qu.:-0.33002 1st Qu.:-0.75750 1st Qu.:-0.8995 ## Median :-0.312623 Median : 0.04362 Median : 0.10326 Median :-0.1313 ## Mean :-0.330028 Mean : 0.23700 Mean : 0.06683 Mean :-0.2380 ## 3rd Qu.: 0.003638 3rd Qu.: 0.97163 3rd Qu.: 0.63563 3rd Qu.: 0.3813 ## Max. : 2.087166 Max. : 2.30798 Max. : 2.49766 Max. : 1.8031 ## yPos Water ## Min. :-2.40310 Min. :-2.2891 ## 1st Qu.:-0.41395 1st Qu.:-0.5373 ## Median : 0.03328 Median : 0.2001 ## Mean : 0.02441 Mean : 0.1368 ## 3rd Qu.: 0.70437 3rd Qu.: 0.8813 ## Max. : 1.71963 Max. : 2.6492 # As you see, no effect of Gen1. summary(lm(Performance ~ ., data = dat)) ## ## Call: ## lm(formula = Performance ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1014 -0.2262 0.1023 0.5836 1.0351 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01744 0.19941 0.087 0.931 ## Gen1 -0.02324 0.29154 -0.080 0.937 ## Gen2 -0.02607 0.23874 -0.109 0.914 ## soilC 0.04102 0.25354 0.162 0.873 ## soilP -0.07209 0.24970 -0.289 0.776 ## Temp -0.23499 0.19354 -1.214 0.239 ## Humidity -0.04075 0.21180 -0.192 0.849 ## xPos -0.33340 0.20491 -1.627 0.119 ## yPos 0.15390 0.21238 0.725 0.477 ## Water 0.13047 0.24852 0.525 0.605 ## ## Residual standard error: 0.9503 on 20 degrees of freedom ## Multiple R-squared: 0.2707, Adjusted R-squared: -0.05751 ## F-statistic: 0.8248 on 9 and 20 DF, p-value: 0.6012 Task Task for you: P-hack the analysis, i.e. make an effect appear, by trying around (systematically, e.g. with selecting with data, model selection, or by hand to find a model combination that has an effect). The group who finds the model with the highest significance for Gen1 wins! Example summary(lm(Performance ~ Gen1 * Humidity, data = dat[20:30,])) ## ## Call: ## lm(formula = Performance ~ Gen1 * Humidity, data = dat[20:30, ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71665 -0.39627 -0.05915 0.28044 0.91257 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.5248 0.2277 -2.304 0.05465 . ## Gen1 0.8657 0.2276 3.804 0.00668 ** ## Humidity 0.6738 0.2544 2.649 0.03298 * ## Gen1:Humidity -0.5480 0.1756 -3.122 0.01680 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6102 on 7 degrees of freedom ## Multiple R-squared: 0.7004, Adjusted R-squared: 0.572 ## F-statistic: 5.454 on 3 and 7 DF, p-value: 0.03 Here some inspiration: Hack Your Way To Scientific Glory: https://projects.fivethirtyeight.com/p-hacking/ False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant: https://journals.sagepub.com/doi/full/10.1177/0956797611417632 Sixty seconds on … P-hacking: https://sci-hub.tw/https://www.bmj.com/content/362/bmj.k4039 John Oliver about p-hacking: 3.3.5 Problems of Stepwise Model Selection LRT or AIC model selections are often used stepwise or global, i.e. we run either a chain of model selections (AIC or LRT), adding or removing complexity, or we run immediately all possible models and compare their AIC. Options in R for automatic model selection using AIC are the StepAIC function MuMIn.{R} package Here is an example for either of those: library(MASS) library(MuMIn) fit = lm(Ozone ~ . , data = airquality) stepAIC(fit) ## Start: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + TempMix + mTemp10 + mTemp01 + fMonth + sTemp + ## sWind + sSolar.R ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + TempMix + mTemp10 + mTemp01 + fMonth + sTemp + ## sWind ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + TempMix + mTemp10 + mTemp01 + fMonth + sTemp ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + TempMix + mTemp10 + mTemp01 + fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + TempMix + mTemp10 + fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + TempMix + fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## TempMult + fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + ## fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Month + Day + fMonth ## ## ## Step: AIC=681.55 ## Ozone ~ Solar.R + Wind + Temp + Day + fMonth ## ## Df Sum of Sq RSS AIC ## - Day 1 429.5 44231 680.63 ## &lt;none&gt; 43801 681.55 ## - fMonth 4 3636.8 47438 682.40 ## - Solar.R 1 2101.6 45903 684.75 ## - Wind 1 9552.6 53354 701.44 ## - Temp 1 13410.1 57212 709.19 ## ## Step: AIC=680.63 ## Ozone ~ Solar.R + Wind + Temp + fMonth ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 44231 680.63 ## - fMonth 4 3771.8 48003 681.71 ## - Solar.R 1 2090.7 46322 683.76 ## - Wind 1 9524.7 53756 700.28 ## - Temp 1 13005.6 57237 707.24 ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp + fMonth, data = airquality) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp fMonth6 fMonth7 ## -74.23481 0.05222 -3.10872 1.87511 -14.75895 -8.74861 ## fMonth8 fMonth9 ## -4.19654 -15.96728 # Default na.action for regressions in R is that NA lines are removed. # MuMIn requires that there are no NA in the data in the first place. # We have to change the default and remove the NA in the data. options(na.action = &quot;na.fail&quot;) dat = airquality[complete.cases(airquality),] fit = lm(Ozone ~ . , data = dat) out = dredge(fit) ## Fixed term is &quot;(Intercept)&quot; # Set back to default NA action. options(na.action = &quot;na.omit&quot;) # Plot only first 6 and last 6 elements of the (realy) long list: head(out) ## Global model call: lm(formula = Ozone ~ ., data = dat) ## --- ## Model selection table ## (Int) cTm Mnt mT0 mT1 Slr.R sWn df logLik AICc delta ## 586 54.63 1.871 -2.992 0.0496 -11.68 6 -492.356 997.5 0 ## 601 -91.08 -2.992 18.71 0.0496 -11.68 6 -492.356 997.5 0 ## 602 54.63 1.871 -2.992 0.0496 -11.68 6 -492.356 997.5 0 ## 617 -91.08 -2.992 0.1871 0.0496 -11.68 6 -492.356 997.5 0 ## 618 54.63 1.871 -2.992 0.0496 -11.68 6 -492.356 997.5 0 ## 633 -91.08 -2.992 18.71 0.0496 -11.68 6 -492.356 997.5 0 ## weight ## 586 0.167 ## 601 0.167 ## 602 0.167 ## 617 0.167 ## 618 0.167 ## 633 0.167 ## Models ranked by AICc(x) tail(out) ## Global model call: lm(formula = Ozone ~ ., data = dat) ## --- ## Model selection table ## (Int) Day Mnt Slr.R sSl.R df logLik AICc delta weight ## 131 41.33 0.05724 11.48 4 -538.843 1086.1 88.54 0.496 ## 195 17.63 0.05724 0.1275 4 -538.843 1086.1 88.54 0.496 ## 9 18.81 3.227 3 -544.892 1096.0 98.49 0.003 ## 1 42.10 2 -546.037 1096.2 98.66 0.003 ## 11 19.06 -0.01492 3.226 4 -544.891 1098.2 100.64 0.001 ## 3 42.42 -0.01983 3 -546.035 1098.3 100.77 0.001 ## Models ranked by AICc(x) Now, let’s have a look at what happens if we perform a model selection on this model library(MASS) set.seed(1) dat = data.frame(matrix(runif(20000), ncol = 100)) dat$y = rnorm(200) fullModel = lm(y ~ . , data = dat) # Number of predictors + intercept: length(fullModel$coefficients) ## [1] 101 # Number of significant predictors: length(summary(fullModel)[[4]][,4][summary(fullModel)[[4]][,4] &lt;= 0.05]) ## [1] 2 2 predictors out of 100are significant (on average, we expect 5 of 100 to be significant). selection = stepAIC(fullModel) summary(selection) ## ## Call: ## lm(formula = y ~ X1 + X2 + X3 + X5 + X7 + X13 + X20 + X23 + X30 + ## X37 + X42 + X45 + X46 + X47 + X48 + X64 + X65 + X66 + X71 + ## X75 + X80 + X81 + X87 + X88 + X89 + X90 + X94 + X100, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.04660 -0.50885 0.05722 0.49612 1.53704 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0314 0.5045 2.044 0.04244 * ## X1 0.4728 0.2185 2.164 0.03187 * ## X2 -0.3809 0.2012 -1.893 0.06008 . ## X3 0.3954 0.1973 2.004 0.04668 * ## X5 -0.2742 0.1861 -1.473 0.14251 ## X7 -0.4442 0.1945 -2.284 0.02359 * ## X13 0.4396 0.1980 2.220 0.02775 * ## X20 0.3984 0.1918 2.078 0.03924 * ## X23 -0.4137 0.2081 -1.988 0.04836 * ## X30 -0.3750 0.1991 -1.884 0.06125 . ## X37 0.4006 0.1989 2.015 0.04550 * ## X42 -0.3934 0.2021 -1.946 0.05325 . ## X45 -0.3197 0.2063 -1.550 0.12296 ## X46 0.3673 0.1992 1.844 0.06690 . ## X47 -0.4240 0.2029 -2.090 0.03811 * ## X48 0.5130 0.1937 2.649 0.00884 ** ## X64 -0.3676 0.2094 -1.755 0.08102 . ## X65 -0.2887 0.1975 -1.462 0.14561 ## X66 0.2769 0.2107 1.315 0.19039 ## X71 -0.5301 0.2003 -2.646 0.00891 ** ## X75 0.5020 0.1969 2.550 0.01165 * ## X80 0.3722 0.2058 1.809 0.07224 . ## X81 -0.3731 0.2176 -1.715 0.08820 . ## X87 -0.2684 0.1958 -1.371 0.17225 ## X88 -0.4524 0.2069 -2.187 0.03011 * ## X89 -0.4123 0.2060 -2.002 0.04691 * ## X90 -0.3528 0.2067 -1.707 0.08971 . ## X94 0.3813 0.2049 1.861 0.06440 . ## X100 -0.4058 0.2024 -2.005 0.04653 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.76 on 171 degrees of freedom ## Multiple R-squared: 0.3177, Adjusted R-squared: 0.2059 ## F-statistic: 2.843 on 28 and 171 DF, p-value: 1.799e-05 # Number of predictors + intercept: length(selection$coefficients) ## [1] 29 # Number of significant predictors: length(summary(selection)[[4]][,4][summary(selection)[[4]][,4] &lt;= 0.05]) ## [1] 15 Voila, 15 out of 28 (before 100) predictors significant. Looks like we could have good fun to discuss / publish these results! Conclusion: Stepwise selection + regression table is hidden multiple testing and has inflated Type I error rates! This is well-known in the stats literature. You CAN do hypothesis tests after model selection, but those require corrections and are not particularly popular, because they are even less significant than the full regression. That being said, those methods work excellent to generate predictive models! 3.4 Case studies 3.4.1 Exercise: Global Plant Trait Analysis #3 Task Revisit exercises 3.1.5 / 3.2.5 (using the dataset plantHeight), and discuss / analyze: Which would be the appropriate model, if we want to get a predictive model for plant height, based on all the variables in the data set? Note: some text-based variables may need to be included, so probably it’s the easiest if you start with a large model that you specify by hand. You can also include interactions. The syntax: fit &lt;- lm((x1 + x2 + x3)^2) includes all possible 2nd-order interactions between the variables in your model. You can extend this to x^3, x^4 but I would not recommend it, your model will get too large. Solution Possible solution: library(EcoData) plantHeight ## sort_number site Genus_species Family growthform ## 1 1402 193 Acer_macryophyllum Sapindaceae Tree ## 2 25246 103 Quararibea_cordata Malvaceae Tree ## 3 11648 54 Eragrostis_dielsii Poaceae Herb ## 4 8168 144 Cistus_salvifolius Cistaceae Shrub ## 5 22422 178 Phlox_bifida Polemoniaceae Herb ## 6 15925 59 Homalium_betulifolium Salicaceae Shrub ## 7 25151 27 Pultenaea_microphylla Fabaceae - P Shrub ## 8 26007 118 Rhizophora_mucronata Rhizophoraceae Tree ## 9 6597 154 Carya_ovata Juglandaceae Tree ## 10 16908 106 Ischaemum_nativitatis Poaceae Herb ## 11 4610 201 Betula_nana Betulaceae Shrub ## 12 1593 86 Acmena_graveolens Myrtaceae Tree ## 13 22359 69 Phaleria_ixoroides Thymelaeaceae Tree ## 14 24493 123 Premna_serratifolia Lamiaceae Shrub/Tree ## 15 25129 72 Pullea_perryana Cunoniaceae Tree ## 16 25921 161 Retama_sphaerocarpa Fabaceae - P Shrub ## 17 30396 46 Themeda_triandra Poaceae Herb ## 18 19298 63 Maesa_tongensis Maesaceae Shrub/Tree ## 19 11305 73 Eleusine_aegyptica Poaceae Herb ## 20 17982 164 Lepechinia_calycina Lamiaceae Shrub ## 21 19569 117 Maranthes_glabra Chrysobalanaceae Tree ## 22 24893 189 Pseudotsuga_menziesii Pinaceae Tree ## 23 3156 113 Aporusa_globifera Phyllanthaceae &lt;NA&gt; ## 24 23821 40 Polygonum_lapathifolium Polygonaceae Herb ## 25 10692 71 Diospyros_gillespiei Ebenaceae Tree ## 26 15714 107 Hibiscus_tiliaceus Malvaceae Tree ## 27 20585 78 Myristica_macrantha Myristicaceae Tree ## 28 32237 81 Weinmannia_richii Cunoniaceae Shrub/Tree ## 29 16107 122 Hybanthus_prunifolius Violaceae Shrub ## 30 19395 140 Mallotus_japonicus Euphorbiaceae Tree ## 31 19849 64 Melochia_longepetiolata Malvaceae Tree ## 32 11723 53 Eremophila_forrestii Scrophulariaceae Shrub ## 33 18175 26 Leucadendron_sp Proteaceae Shrub ## 34 14954 18 Hakea_rostrata Proteaceae Shrub ## 35 32697 38 Adenanthos_cygnorum Proteaceae Tree ## 36 6851 222 Cassiope_tetragona Ericaceae Shrub ## 37 30345 156 Thalictrum_thalictroides Ranunculaceae Herb ## 38 26673 209 Rumex_acetosa Polygonaceae Herb ## 39 6787 137 Casimiroa_greggii Rutaceae Shrub ## 40 4175 10 Baccharis_divaricata Asteraceae Shrub ## 41 15864 121 Hirtella_triandra Chrysobalanaceae Tree ## 42 20367 139 Morus_boninensis Moraceae Tree ## 43 3175 173 Aquilegia_caerulea Ranunculaceae Herb ## 44 21747 116 Parashorea_malaanonan Dipterocarpaceae Tree ## 45 5003 82 Brackenridgea_nitida Ochnaceae Shrub/Tree ## 46 26205 77 Richella_monosperma Annonaceae Tree ## 47 32313 37 Xanthorrhoea_preissii Xanthorrhoeaceae Shrub ## 48 21013 126 Ocotea_meziana Lauraceae Tree ## 49 18104 21 Leptospermum_continentale Myrtaceae Shrub ## 50 2720 129 Ampelocera_hottlei Ulmaceae Tree ## 51 12031 99 Eucalyptus_miniata Myrtaceae Tree ## 52 15651 61 Heteropogon_triticeus Poaceae Herb ## 53 8736 13 Coprosma_robusta Rubiaceae Shrub ## 54 21930 220 Pedicularis_lapponica Orobanchaceae Herb ## 55 25274 151 Quercus_calliprinos Fagaceae Shrub ## 56 29888 30 Syncarpia_glomulifera Myrtaceae Tree ## 57 89 158 Abies_veitchii Pinaceae Tree ## 58 12090 23 Eucalyptus_sp4 Myrtaceae Tree ## 59 3646 79 Ascarina_swamyana Chloranthaceae Tree ## 60 25068 34 Pteronia_pallens Asteraceae &lt;NA&gt; ## 61 29156 94 Sorocea_pileata Moraceae Tree ## 62 9641 65 Cyathocalyx_insularis Annonaceae Tree ## 63 18176 31 Leucadendron_corymbosum Proteaceae Shrub ## 64 18392 165 Linanthus_sp Polemoniaceae Herb ## 65 32694 134 Piranhea_sp Picrodendraceae Tree ## 66 9244 28 Crassula_rupestris Crassulaceae Shrub ## 67 24129 215 Potentilla_nivea Rosaceae Herb ## 68 7054 132 Cecropia_obtusifolia Urticaceae Tree ## 69 4583 111 Berlinia_grandiflora Fabaceae - C Tree ## 70 10189 85 Decaspermum_cryptanthum Myrtaceae Shrub ## 71 8927 218 Cornus_sp Cornaceae Herb ## 72 31945 3 Viola_magellanica Violaceae Herb ## 73 3876 142 Astragalus_cruciatus Fabaceae - P Herb ## 74 13286 87 Gardenia_actinocarpa Rubiaceae Shrub/Tree ## 75 3943 80 Astronidium_parviflorum Melastomataceae Shrub/Tree ## 76 26391 212 Rosa_acicularis Rosaceae Shrub ## 77 1762 163 Aesculus_californica Sapindaceae Tree ## 78 21743 62 Paraserianthes_toona Fabaceae - M Tree ## 79 32699 49 Corymbia_maculata Myrtaceae Tree ## 80 30075 105 Tachigali_sp Fabaceae - C Tree ## 81 10684 115 Diospyros_borneensis Ebenaceae Tree ## 82 9078 203 Corylus_avellana Betulaceae Tree ## 83 32688 20 Allocasuarina_sp Casuarinaceae Shrub ## 84 11477 50 Englerophytum_natalense Sapotaceae Tree ## 85 2173 95 Alchornea_castaneaefolia Euphorbiaceae Shrub ## 86 11376 216 Empetrum_nigrum Ericaceae Shrub ## 87 26025 192 Rhododendron_macrophyllum Ericaceae Shrub ## 88 18073 83 Lepironia_articulata Cyperaceae Herb ## 89 17755 180 Larix_olgensis Pinaceae Tree ## 90 12094 8 Eucalyptus_oblongifolia Myrtaceae Tree ## 91 23390 104 Planchonia_careya Lecythidaceae &lt;NA&gt; ## 92 8122 200 Cirsium_vulgare Asteraceae Herb ## 93 12539 152 Fagus_crenata Fagaceae Tree ## 94 6781 84 Casearia_stenophylla Salicaceae Shrub/Tree ## 95 5478 214 Calamagrostis_stricta Poaceae Herb ## 96 22889 196 Pinus_ponderosa Pinaceae Tree ## 97 22657 56 Phyllostylon_rhamnoides Ulmaceae Tree ## 98 11224 75 Elatostema_nemorosum Urticaceae Herb ## 99 24987 51 Psychotria_carthagenensis Rubiaceae Shrub ## 100 22103 29 Persoonia_lanceolata Proteaceae Shrub ## 101 2356 100 Alloteropsis_semialata Poaceae Herb ## 102 11775 194 Erigeron_glaucus Asteraceae Herb ## 103 24239 97 Pourouma_minor Urticaceae Tree ## 104 31432 213 Vaccinium_vitis-idaea Ericaceae Shrub ## 105 11884 183 Erucastrum_gallicum Brassicaceae Herb ## 106 18226 70 Leucopogon_septentrionalis Ericaceae Shrub/Tree ## 107 21605 93 Panicum_sp Poaceae Herb ## 108 4619 217 Betula_nana Betulaceae Shrub ## 109 4101 14 Austrocedrus_chilensis Cupressaceae Tree ## 110 30008 66 Syzygium_brackenridgei Myrtaceae Tree ## 111 25017 15 Pteridium_esculentum Dennstaedtiaceae Fern ## 112 2661 159 Amelanchier_arborea Rosaceae Tree ## 113 29608 19 Stipa_sp Poaceae Herb ## 114 29609 9 Stipa_speciosa Poaceae Herb ## 115 3971 11 Atherosperma_moschatum Atherospermataceae Tree ## 116 10796 60 Dombeya_ciliata Malvaceae &lt;NA&gt; ## 117 19797 74 Melicytus_fasciger Violaceae Shrub/Tree ## 118 7039 148 Ceanothus_greggii Rhamnaceae Shrub ## 119 10986 114 Duguetia_surinamensis Annonaceae Tree ## 120 21931 221 Pedicularis_hirsuta Orobanchaceae Herb ## 121 4330 44 Banksia_hookeriana Proteaceae Shrub ## 122 8341 130 Clidemia_sericea Melastomataceae &lt;NA&gt; ## 123 32296 167 Xanthium_occidentale Asteraceae Herb ## 124 27405 141 Sarcopoterium_spinosum Rosaceae Shrub ## 125 4297 96 Bambusa_weberbaueri Poaceae Shrub ## 126 30971 207 Triglochin_palustre Juncaginaceae Herb ## 127 4614 202 Betula_pendula Betulaceae Shrub ## 128 12621 4 Festuca_gracillima Poaceae Herb ## 129 12043 55 Eucalyptus_gillenii Myrtaceae Shrub ## 130 32675 102 _8324 &lt;NA&gt; &lt;NA&gt; ## 131 5226 25 Brunia_albifora Bruniaceae &lt;NA&gt; ## 132 12046 43 Eucalyptus_sp2 Myrtaceae Tree ## 133 227 138 Acacia_berlandieri Fabaceae - M Shrub/Tree ## 134 18174 22 Leucadendron_meridianum Proteaceae Shrub ## 135 5464 195 Cakile_edentula Brassicaceae Herb ## 136 12089 24 Eucalyptus_socialis Myrtaceae Tree ## 137 27860 88 Sclerolobium_paniculatum Fabaceae - C &lt;NA&gt; ## 138 6529 143 Carnegiea_gigantea Cactaceae Shrub ## 139 5079 92 Bridelia_micrantha Phyllanthaceae Tree ## 140 28046 5 Senecio_filaginoides Asteraceae Herb ## 141 32120 171 Vulpia_microstachys Poaceae Herb ## 142 29331 48 Spirostachys_africanus Euphorbiaceae Shrub ## 143 28244 67 Sesbania_grandiflora Fabaceae - P Tree ## 144 12604 145 Ferocactus_cylindraceus Cactaceae Shrub ## 145 16616 172 Ipomopsis_aggregata Polemoniaceae &lt;NA&gt; ## 146 22732 211 Picea_mariana Pinaceae Tree ## 147 8880 198 Corema_conradi Ericaceae Shrub ## 148 21232 149 Opuntia_acanthocarpa Cactaceae Shrub ## 149 18833 147 Ludwigia_leptocarpa Onagraceae Herb/Shrub ## 150 4331 42 Banksia_hookeriana Proteaceae Shrub ## 151 2834 109 Andropogon_greenwayi Poaceae Herb ## 152 10460 12 Dicksonia_antarctica Dicksoniaceae Shrub ## 153 9565 101 Cupaniopsis_anacardioides Sapindaceae Tree ## 154 4332 41 Banksia_tricuspis Proteaceae Shrub ## 155 12097 16 Eucalyptus_sp5 Myrtaceae Tree ## 156 4954 176 Bouteloua_gracilis Poaceae Herb ## 157 25823 174 Ratibida_columnifera Asteraceae Herb ## 158 11205 112 Elateriospermum_tapos Euphorbiaceae Tree ## 159 26532 205 Rubus_chamaemorus Rosaceae Shrub ## 160 21730 58 Paraneurachne_muelleri Poaceae Herb ## 161 150 57 Acacia_aneura Fabaceae - M Shrub/Tree ## 162 16438 179 Impatiens_capensis Balsaminaceae Herb ## 163 26983 208 Salix_lapponum Salicaceae Shrub ## 164 12619 7 Festuca_novae-zealandiae Poaceae Herb ## 165 3358 135 Ardisia_tenera Myrsinaceae Tree ## 166 12541 199 Fagus_sylvatica Fagaceae Tree ## 167 17783 150 Larrea_tridentata Zygophyllaceae Shrub ## 168 13427 204 Gentiana_campestris Gentianaceae Herb ## 169 7867 6 Chionochloa_pallens Poaceae Herb ## 170 12303 177 Euphorbia_characias Euphorbiaceae &lt;NA&gt; ## 171 15377 110 Heliconia_acuminata Heliconiaceae Herb ## 172 17329 170 Juniperus_virginiana Cupressaceae Shrub ## 173 11188 76 Elaeocarpus_pyriformis Elaeocarpaceae Tree ## 174 2357 91 Alloteropsis_semialata Poaceae Herb ## 175 30141 68 Tapeinosperma_grande Myrsinaceae Shrub/Tree ## 176 21230 32 Opuntia_aurantiaca Cactaceae Shrub ## 177 29075 210 Sorbus_aucuparia Rosaceae Tree ## 178 13426 197 Gentiana_cruciata Gentianaceae Herb ## height loght Country Site ## 1 28.000000 1.44715803 USA Oregon - McDun ## 2 26.600000 1.42488164 Peru Manu ## 3 0.300000 -0.52287874 Australia Central Australia ## 4 1.600000 0.20411998 Israel Hanadiv ## 5 0.200000 -0.69897000 USA Indiana Dunes ## 6 1.700000 0.23044892 New Caledonia &lt;NA&gt; ## 7 0.500000 -0.30103000 Australia Kuringai Chase, Sydney ## 8 10.000000 1.00000000 &lt;NA&gt; Marshall Islands ## 9 40.000000 1.60205999 USA Colorado ## 10 0.500000 -0.30103000 Australia Christmas Island ## 11 0.550000 -0.25963731 Estonia &lt;NA&gt; ## 12 32.000000 1.50514998 Australia Cairns - Daintree canopy crane ## 13 5.000000 0.69897000 Fiji Viti Levu ## 14 7.000000 0.84509804 Micronesia Yap ## 15 12.000000 1.07918125 Fiji ao ## 16 1.680000 0.22530928 Spain &lt;NA&gt; ## 17 0.700000 -0.15490196 South Africa Zululand - ledube ## 18 4.000000 0.60205999 Fiji fulanga ## 19 0.600000 -0.22184875 Fiji ab ## 20 1.600000 0.20411998 USA Jasper Ridge - Chaparral ## 21 32.000000 1.50514998 Liberia &lt;NA&gt; ## 22 61.000000 1.78532983 USA Oregon ## 23 14.800000 1.17026172 Malaysia &lt;NA&gt; ## 24 1.000000 0.00000000 Australia &lt;NA&gt; ## 25 15.000000 1.17609126 Fiji abko ## 26 7.000000 0.84509804 Papua New Guinea Motupore Island ## 27 20.000000 1.30103000 Fiji abo ## 28 7.000000 0.84509804 Fiji abt ## 29 2.900000 0.46239800 Panama BCI ## 30 9.670000 0.98542647 Japan &lt;NA&gt; ## 31 8.000000 0.90308999 Fiji k ## 32 2.000000 0.30103000 Australia WA ## 33 0.600000 -0.22184875 South Africa Stellenbosch - fynbos ## 34 1.700000 0.23044892 Australia &lt;NA&gt; ## 35 7.000000 0.84509804 Australia Perth - Melaleuca Park ## 36 0.080000 -1.09691001 Greenland Zackenberg - hill ## 37 0.200000 -0.69897000 USA Duke Forest, Durham, NC ## 38 0.707000 -0.15058059 Finland &lt;NA&gt; ## 39 7.000000 0.84509804 Mexico Linares - Puenta Viejo ## 40 0.500000 -0.30103000 Argentina Puerto Madryn - dune ## 41 23.500000 1.37106786 Panama Panama - BCI ## 42 16.000000 1.20411998 Japan &lt;NA&gt; ## 43 0.233000 -0.63264408 USA Rockies ## 44 34.000000 1.53147892 &lt;NA&gt; &lt;NA&gt; ## 45 15.000000 1.17609126 Fiji abrambi ## 46 15.000000 1.17609126 Fiji aot ## 47 1.500000 0.17609126 Australia Perth - Darling Scarp ## 48 16.000000 1.20411998 Costa Rica &lt;NA&gt; ## 49 2.800000 0.44715803 Australia Adelaide - ferries ## 50 25.000000 1.39794001 Mexico Chajul ## 51 20.000000 1.30103000 Australia Howard Springs, Darwin ## 52 0.400000 -0.39794001 Australia Townsville savanna ## 53 6.000000 0.77815125 New Zealand Nelson ## 54 0.200000 -0.69897000 Greenland disko island ## 55 3.500000 0.54406804 Israel Adulam ## 56 25.000000 1.39794001 Australia Kuringai - Diatreme ## 57 18.000000 1.25527250 Japan &lt;NA&gt; ## 58 10.000000 1.00000000 Australia Adelaide - Brookfield Chenopod ## 59 10.000000 1.00000000 Fiji at ## 60 0.500000 -0.30103000 South Africa &lt;NA&gt; ## 61 19.000000 1.27875360 Peru Los Amigos floodplain ## 62 20.000000 1.30103000 Fiji ak ## 63 3.000000 0.47712126 South Africa Stellenbosch - renosterveld ## 64 0.110000 -0.95860731 USA Jasper Ridge - Serpentine ## 65 12.500000 1.09691001 Mexico Chamela ## 66 0.600000 -0.22184875 South Africa Stellenbosch - Karoo ## 67 0.050000 -1.30103000 Greenland Kangerlussuaq - dry ## 68 30.000000 1.47712125 &lt;NA&gt; &lt;NA&gt; ## 69 35.000000 1.54406804 Republic of Congo Congo - bai ## 70 3.000000 0.47712126 Fiji b ## 71 0.200000 -0.69897000 Norway Norway ## 72 0.050000 -1.30103000 Argentina Rio Turbio - Nothofagus ## 73 0.032200 -1.49214413 &lt;NA&gt; &lt;NA&gt; ## 74 3.500000 0.54406804 Australia Queensland ## 75 12.000000 1.07918125 Fiji abot ## 76 0.800000 -0.09691001 USA Alaska campus ## 77 16.000000 1.20411998 USA Jasper Ridge - Oak forest ## 78 16.000000 1.20411998 Australia Townsville Vine Thicket ## 79 30.000000 1.47712125 Australia Toowoomba ## 80 30.000000 1.47712125 &lt;NA&gt; &lt;NA&gt; ## 81 18.100000 1.25767857 Brunei &lt;NA&gt; ## 82 10.000000 1.00000000 Sweden Stockholm ## 83 3.000000 0.47712126 Australia Adelaide - Cox&#39;s scrub ## 84 7.000000 0.84509804 South Africa Zululand - forest ## 85 4.000000 0.60205999 Peru Los Amigos - successional ## 86 0.080000 -1.09691001 Sweden Abisko - Paddus ## 87 5.000000 0.69897000 Western Oregon &lt;NA&gt; ## 88 2.500000 0.39794001 Fiji t ## 89 32.000000 1.50514998 China &lt;NA&gt; ## 90 30.000000 1.47712125 Australia Huon Rd, Tasmania ## 91 10.000000 1.00000000 &lt;NA&gt; Melville Island ## 92 2.000000 0.30103000 Netherlands &lt;NA&gt; ## 93 29.300000 1.46686762 Japan &lt;NA&gt; ## 94 4.000000 0.60205999 Fiji bt ## 95 0.220000 -0.65757732 Greenland Kangerlussuaq - wet ## 96 41.000000 1.61278386 &lt;NA&gt; &lt;NA&gt; ## 97 24.000000 1.38021124 Argentina Tucuman - Yungas North ## 98 2.000000 0.30103000 Fiji abkt ## 99 4.500000 0.65321251 Argentina Tucuman - Yungas South ## 100 2.400000 0.38021124 Australia Kuringai - Challenger ## 101 2.000000 0.30103000 Australia Howard Springs, Darwin ## 102 0.040000 -1.39794001 USA Oregon - Yaquina Head ## 103 28.000000 1.44715803 Peru Los Amigos -terrace ## 104 0.070000 -1.15490196 USA Alaska - 12 Mile ## 105 0.280000 -0.55284197 USA &lt;NA&gt; ## 106 5.000000 0.69897000 Fiji abk ## 107 0.500000 -0.30103000 Zambia Zambia - miombo ## 108 0.800000 -0.09691001 Sweden Abisko - forest ## 109 35.000000 1.54406804 Argentina Bariloche ## 110 20.000000 1.30103000 Fiji ako ## 111 1.800000 0.25527250 Australia Green&#39;s Bush - Melbourne ## 112 19.000000 1.27875360 USA Twin springs, Virginia ## 113 0.350000 -0.45593196 Argentina Mendoza - Payunia ## 114 0.250000 -0.60205999 Argentina Puerto Madryn - steppe ## 115 30.000000 1.47712125 Australia Tasmania ## 116 15.000000 1.17609126 France? Reunion Island ## 117 10.000000 1.00000000 Fiji atngau ## 118 3.000000 0.47712126 USA California ## 119 30.000000 1.47712125 &lt;NA&gt; &lt;NA&gt; ## 120 0.080000 -1.09691001 Greenland Zackenberg - salix ## 121 2.020000 0.30535137 Australia &lt;NA&gt; ## 122 0.800000 -0.09691001 &lt;NA&gt; &lt;NA&gt; ## 123 1.150000 0.06069784 Japan &lt;NA&gt; ## 124 0.450000 -0.34678749 Israel Lehavim ## 125 6.000000 0.77815125 Peru Los Amigos - Bamboo ## 126 0.150000 -0.82390874 USA Alaska, Yukon delta ## 127 1.584893 0.20000000 &lt;NA&gt; &lt;NA&gt; ## 128 0.140000 -0.85387196 Argentina Rio Turbio - heath ## 129 5.000000 0.69897000 Australia Alice - the gap ## 130 2.500000 0.39794001 Peru Manu ## 131 3.000000 0.47712126 South Africa &lt;NA&gt; ## 132 20.000000 1.30103000 Australia Armidale - Goonoowigal ## 133 6.000000 0.77815125 Mexico Linares - thornscrub ## 134 1.700000 0.23044892 South Africa &lt;NA&gt; ## 135 0.200000 -0.69897000 Canada Nova Scotia ## 136 6.000000 0.77815125 Australia Adelaide - Brookfield Mallee ## 137 3.800000 0.57978360 Brazil &lt;NA&gt; ## 138 8.000000 0.90308999 USA Tucson - Sonoran desert ## 139 9.000000 0.95424251 Zambia Zambia - Mateshi ## 140 0.600000 -0.22184875 &lt;NA&gt; Patagonia ## 141 0.239000 -0.62160210 &lt;NA&gt; &lt;NA&gt; ## 142 4.500000 0.65321251 South Africa Zululand - Mbuzane ## 143 12.000000 1.07918125 Fiji a-ngau ## 144 1.700000 0.23044892 USA Tucson ## 145 0.810000 -0.09151498 USA Colorado ## 146 13.500000 1.13033377 USA Alaska - Bonanza ## 147 0.500000 -0.30103000 Canada Quebec ## 148 0.720000 -0.14266750 USA California ## 149 1.500000 0.17609126 USA South Carolina ## 150 1.710000 0.23299611 Australia Western Australia ## 151 0.300000 -0.52287874 Tanzania Serengeti, Naabi hill ## 152 3.000000 0.47712126 Australia Mt Field, Tasmania ## 153 8.000000 0.90308999 Australia Darwin - East point ## 154 2.900000 0.46239800 Australia &lt;NA&gt; ## 155 13.000000 1.11394335 Australia Bunyip - Melbourne ## 156 0.200000 -0.69897000 USA Colorado ## 157 1.000000 0.00000000 USA Kansas ## 158 39.600000 1.59769519 Malaysia &lt;NA&gt; ## 159 0.158000 -0.80134291 Finland &lt;NA&gt; ## 160 0.500000 -0.30103000 Australia Kunoth Paddock - Alice Springs ## 161 9.000000 0.95424251 Australia Kunoth Paddock - Alice Springs ## 162 3.000000 0.47712126 Rhode Island USA ## 163 1.050000 0.02118930 Norway &lt;NA&gt; ## 164 0.500000 -0.30103000 New Zealand &lt;NA&gt; ## 165 11.000000 1.04139268 China &lt;NA&gt; ## 166 39.000000 1.59106461 Germany Barvaria ## 167 1.940000 0.28780173 &lt;NA&gt; &lt;NA&gt; ## 168 12.400000 1.09342169 Sweden Sodermanland ## 169 1.500000 0.17609126 New Zealand Canterbury ## 170 1.000000 0.00000000 Spain &lt;NA&gt; ## 171 0.750000 -0.12493874 Brazil &lt;NA&gt; ## 172 4.000000 0.60205999 USA &lt;NA&gt; ## 173 15.000000 1.17609126 Fiji abngau ## 174 0.550000 -0.25963731 Australia Cape York ## 175 6.000000 0.77815125 Fiji abkngau ## 176 0.500000 -0.30103000 &lt;NA&gt; &lt;NA&gt; ## 177 15.000000 1.17609126 Sweeden Umea ## 178 0.246000 -0.60906489 Switzerland &lt;NA&gt; ## lat long entered.by alt temp diurn.temp isotherm temp.seas ## 1 44.600 -123.334 Angela 179 10.8 11.8 4.4 5.2 ## 2 12.183 -70.550 Angela 386 24.5 10.8 7.4 0.9 ## 3 23.800 133.833 Michelle 553 20.9 16.3 4.8 6.0 ## 4 32.555 34.938 Angela 115 19.9 9.7 4.4 4.9 ## 5 41.617 -86.950 Michelle 200 9.7 10.7 2.8 9.7 ## 6 21.500 165.500 Laura 95 22.6 7.4 5.4 2.2 ## 7 33.650 151.200 Michelle 157 16.8 10.0 4.8 3.9 ## 8 9.000 168.000 Laura 2 27.7 4.8 8.8 0.2 ## 9 35.800 -89.900 Angela 71 15.5 11.4 3.2 8.6 ## 10 10.417 105.667 Laura 2 26.4 5.0 7.4 0.6 ## 11 58.500 25.000 Angela 28 5.4 6.6 2.1 8.3 ## 12 16.103 145.446 Angela 263 25.2 8.3 5.8 2.1 ## 13 17.800 178.000 Laura 1108 19.3 6.3 5.8 1.5 ## 14 9.500 138.167 Laura 15 27.2 6.8 9.1 0.2 ## 15 17.742 178.392 Laura 47 24.8 6.3 6.3 1.3 ## 16 37.133 -2.367 Angela 648 15.3 10.1 3.8 5.7 ## 17 28.234 32.017 Angela 289 20.5 10.2 5.8 2.4 ## 18 19.133 -178.567 Laura 0 24.9 6.2 6.0 1.4 ## 19 17.667 178.167 Laura 312 23.5 6.3 6.2 1.3 ## 20 37.400 -122.233 Angela 150 13.8 11.3 5.5 3.3 ## 21 5.500 -7.500 Nate 152 26.0 8.8 7.5 0.9 ## 22 44.000 -122.000 Angela 1446 5.0 12.8 4.1 6.1 ## 23 3.000 102.333 Angela 228 25.8 9.5 8.5 0.5 ## 24 30.517 145.133 Michelle 106 19.9 13.8 4.5 5.9 ## 25 17.758 178.577 Laura 13 24.9 6.3 6.3 1.3 ## 26 9.500 147.267 Laura 30 26.8 8.3 7.6 0.8 ## 27 17.356 178.675 Laura 3 25.0 6.3 6.4 1.2 ## 28 17.081 179.069 Laura 2 25.4 6.2 6.5 1.1 ## 29 9.167 -79.850 Angela 94 26.3 6.7 7.3 0.7 ## 30 30.333 130.400 Angela 530 18.2 6.3 2.8 5.5 ## 31 18.967 178.283 Laura 72 24.1 6.4 5.9 1.5 ## 32 24.650 113.700 Angela 7 22.3 10.7 4.6 4.1 ## 33 33.992 18.975 Angela 387 15.7 11.2 5.3 3.5 ## 34 36.917 142.417 Angela 366 13.0 12.5 4.9 4.4 ## 35 31.689 115.886 Angela 60 18.3 11.5 4.9 4.1 ## 36 74.476 -20.629 Angela 83 -11.1 7.0 2.1 9.3 ## 37 35.967 -79.000 Angela 99 14.9 13.4 3.9 7.6 ## 38 62.000 27.000 Angela 114 3.4 7.3 2.1 9.0 ## 39 24.749 -99.799 Angela 736 20.7 14.3 5.5 4.0 ## 40 42.769 -64.101 Angela 66 13.0 10.3 4.7 4.2 ## 41 9.150 -79.849 Angela 165 26.5 6.8 7.3 0.7 ## 42 26.650 142.133 Angela 230 23.4 4.6 3.1 3.6 ## 43 38.966 -106.987 Angela 2966 0.2 16.9 4.0 8.6 ## 44 4.967 117.800 Angela 214 25.9 7.7 8.9 0.3 ## 45 16.956 179.069 Laura 5 25.5 6.2 6.6 1.1 ## 46 17.447 178.917 Laura 97 24.6 6.2 6.3 1.2 ## 47 32.020 116.044 Angela 209 17.0 11.2 4.6 4.5 ## 48 10.433 -83.983 Angela 50 26.0 8.8 7.6 0.8 ## 49 35.235 139.132 Angela 52 15.6 11.7 5.3 3.7 ## 50 16.106 -90.987 Angela 41 25.9 11.3 6.9 1.5 ## 51 12.494 131.108 Angela 26 27.3 10.5 6.0 1.7 ## 52 19.337 146.755 Angela 79 23.9 9.9 5.2 3.1 ## 53 42.000 173.000 Angela 1650 4.2 8.5 4.4 3.7 ## 54 69.250 -53.600 Angela 150 -5.7 5.5 1.8 8.0 ## 55 34.935 34.935 Angela 358 4.8 7.1 2.2 8.7 ## 56 33.578 151.292 Angela 188 17.0 9.7 4.8 3.8 ## 57 36.083 138.350 Angela 2090 2.4 8.7 2.6 8.6 ## 58 34.347 139.517 Angela 99 16.2 13.4 5.1 4.5 ## 59 17.329 178.983 Laura 2 25.2 6.2 6.4 1.2 ## 60 33.167 22.283 Angela 732 15.7 15.0 5.6 4.0 ## 61 12.567 -70.093 Angela 217 24.9 10.5 7.4 1.0 ## 62 18.383 178.142 Laura 30 24.6 6.3 6.1 1.4 ## 63 33.448 19.048 Angela 89 17.9 13.5 5.3 4.3 ## 64 37.400 -122.224 Angela 179 13.7 11.4 5.5 3.3 ## 65 19.500 -105.043 Angela 357 26.2 12.8 7.2 1.6 ## 66 33.609 19.457 Angela 394 16.5 13.8 5.4 4.0 ## 67 66.973 -50.568 Angela 75 -5.1 8.2 2.3 10.0 ## 68 18.583 -95.117 Angela 550 22.7 8.7 5.7 2.0 ## 69 2.160 16.153 Angela 403 24.8 10.5 8.0 0.6 ## 70 16.583 179.242 Laura 611 22.5 6.1 6.4 1.1 ## 71 68.606 17.606 Angela 44 1.2 6.3 2.6 6.2 ## 72 51.578 -72.315 Angela 702 3.7 9.7 4.9 3.6 ## 73 31.500 35.100 Angela 860 16.8 11.6 4.5 5.3 ## 74 16.100 145.367 Angela 840 21.2 9.4 5.9 2.3 ## 75 17.231 178.998 Laura 2 25.4 6.2 6.5 1.1 ## 76 64.860 -147.862 Angela 688 -2.9 10.9 2.2 14.1 ## 77 37.400 -122.236 Angela 84 13.8 11.2 5.5 3.3 ## 78 19.332 146.773 Angela 99 23.9 9.9 5.2 3.1 ## 79 28.085 151.729 Angela 643 17.0 13.7 5.1 4.8 ## 80 10.983 -65.717 Angela 165 26.5 11.1 7.0 0.7 ## 81 4.500 115.167 Nate 280 26.0 6.7 9.0 0.2 ## 82 58.953 17.610 Angela 10 7.0 6.5 2.4 7.1 ## 83 35.341 138.740 Angela 177 14.9 10.0 5.1 3.5 ## 84 28.072 32.039 Angela 267 19.4 10.2 6.0 2.3 ## 85 12.566 -70.105 Angela 217 24.8 10.5 7.3 1.0 ## 86 68.324 18.843 Angela 554 -1.0 7.0 2.4 7.5 ## 87 44.217 -122.233 Angela 1446 5.0 12.8 4.1 6.1 ## 88 16.858 179.967 Laura 265 24.3 6.1 6.5 1.1 ## 89 42.333 135.500 Angela 3 23.0 4.6 3.1 3.8 ## 90 42.921 147.275 Angela 346 9.9 8.8 5.0 3.0 ## 91 11.500 130.967 Angela 44 27.1 9.4 6.3 1.4 ## 92 52.800 4.333 Angela 17 9.0 5.4 2.7 5.3 ## 93 35.350 133.550 Angela 867 9.4 8.3 2.6 8.3 ## 94 16.721 179.604 Laura 74 25.3 6.2 6.7 1.0 ## 95 66.973 -50.569 Angela 65 -5.1 8.2 2.3 10.0 ## 96 45.817 -121.950 Angela 355 9.5 10.6 3.9 5.7 ## 97 23.747 -64.854 Angela 737 19.8 12.9 5.2 3.9 ## 98 17.552 178.873 Laura 6 25.0 6.2 6.3 1.2 ## 99 26.763 -65.333 Angela 935 18.6 11.5 4.8 4.2 ## 100 33.595 151.276 Angela 151 16.9 9.8 4.8 3.8 ## 101 12.494 131.108 Angela 26 27.3 10.5 6.0 1.7 ## 102 44.667 -124.072 Angela 324 10.6 7.7 5.0 2.8 ## 103 12.552 -70.111 Angela 237 24.8 10.5 7.3 1.0 ## 104 65.391 -145.854 Angela 966 -6.4 11.2 2.1 15.1 ## 105 43.000 -76.150 Angela 134 8.5 10.8 2.9 9.3 ## 106 17.783 178.508 Laura 30 24.8 6.3 6.3 1.3 ## 107 13.249 30.280 Angela 1506 18.6 12.4 5.6 2.4 ## 108 68.329 18.836 Angela 521 -0.7 7.0 2.4 7.4 ## 109 41.242 -71.425 Angela 1039 7.0 11.7 5.1 4.2 ## 110 18.150 178.356 Laura 13 24.8 6.3 6.1 1.4 ## 111 38.430 144.922 Angela 165 13.5 7.5 4.5 3.2 ## 112 36.817 -82.464 Angela 501 12.7 13.0 3.8 7.8 ## 113 36.250 -68.824 Angela 1502 10.9 16.3 5.2 5.5 ## 114 42.790 -64.092 Angela 76 13.0 10.2 4.6 4.3 ## 115 42.683 146.350 Angela 740 7.5 8.4 4.8 3.0 ## 116 21.000 55.650 Angela 191 22.7 6.3 5.2 2.1 ## 117 17.564 179.089 Laura 1 25.1 6.3 6.4 1.2 ## 118 33.383 -116.000 Angela -71 22.9 17.4 4.6 7.1 ## 119 4.083 -52.667 Angela 174 24.9 8.8 7.4 0.4 ## 120 74.474 -20.536 Angela 37 -10.5 7.0 2.1 9.3 ## 121 29.567 115.233 Angela 94 20.2 13.5 5.0 4.7 ## 122 17.000 -89.000 Angela 371 23.9 8.5 5.9 1.8 ## 123 38.217 140.833 Angela 38 12.4 8.0 2.5 8.1 ## 124 31.356 34.835 Angela 379 18.9 12.0 4.7 5.0 ## 125 12.566 -70.099 Angela 256 24.8 10.5 7.3 1.0 ## 126 61.250 -165.500 Angela 1 -1.2 6.8 2.3 8.3 ## 127 58.867 25.000 Angela 75 5.1 6.6 2.1 8.2 ## 128 51.575 -72.312 Angela 747 3.7 9.7 4.9 3.6 ## 129 23.795 133.863 Angela 701 20.9 16.3 4.8 6.0 ## 130 12.183 -70.917 Angela 350 24.8 10.8 7.6 0.8 ## 131 34.317 19.917 Angela 227 16.4 10.9 5.3 3.4 ## 132 29.815 151.121 Angela 686 15.3 14.5 4.9 5.3 ## 133 24.786 -99.515 Angela 354 22.6 13.9 4.8 5.1 ## 134 34.583 19.917 Angela 71 16.7 8.5 5.1 2.8 ## 135 44.683 -63.117 Angela 4 6.4 9.1 2.8 8.0 ## 136 34.320 139.503 Angela 95 16.2 13.3 5.1 4.6 ## 137 15.933 -47.883 Angela 1100 20.8 11.6 7.2 1.1 ## 138 32.310 -110.739 Angela 971 18.9 15.7 4.6 6.9 ## 139 13.253 30.047 Angela 1388 19.1 12.7 5.5 2.6 ## 140 45.417 -70.000 Angela 500 9.6 11.2 4.6 4.8 ## 141 38.867 -122.417 Angela 646 13.5 15.3 4.7 6.0 ## 142 28.221 31.794 Angela 161 21.0 11.0 5.9 2.5 ## 143 17.917 178.650 Laura 12 24.8 6.3 6.3 1.3 ## 144 32.600 -111.233 Angela 690 20.5 17.2 4.6 7.4 ## 145 38.867 -106.967 Angela 2698 1.3 17.9 4.0 9.0 ## 146 64.769 -148.283 Angela 382 -4.3 10.7 2.2 13.9 ## 147 47.533 -61.700 Angela 1 4.9 6.2 2.0 8.2 ## 148 33.633 -116.400 Angela 900 16.7 16.6 4.7 6.6 ## 149 33.217 -81.750 Angela 54 17.5 14.0 4.2 6.9 ## 150 29.867 115.250 Angela 79 20.1 13.3 5.1 4.6 ## 151 3.217 35.483 Angela 1550 13.7 8.8 6.5 1.3 ## 152 42.679 146.669 Angela 704 7.9 8.8 4.8 3.2 ## 153 12.406 130.820 Angela 11 27.5 9.6 6.2 1.5 ## 154 30.167 115.233 Angela 274 18.1 12.5 5.1 4.2 ## 155 38.010 145.620 Angela 164 13.6 10.5 4.8 3.7 ## 156 40.817 -107.783 Angela 2177 4.3 16.9 3.9 9.0 ## 157 39.083 -96.583 Angela 407 12.0 12.7 3.0 10.0 ## 158 2.983 102.300 Angela 116 26.5 9.5 8.5 0.5 ## 159 60.000 27.000 Angela 75 4.5 7.1 2.2 8.7 ## 160 23.533 133.748 Angela 713 20.4 16.0 4.8 6.1 ## 161 23.533 133.748 Angela 713 20.4 16.0 4.8 6.1 ## 162 41.667 -71.250 Angela 44 10.1 10.0 2.9 8.4 ## 163 61.600 7.500 Angela 1520 -2.1 6.2 2.8 5.4 ## 164 43.033 171.750 Angela 587 9.1 9.0 4.6 3.6 ## 165 21.960 101.200 Angela 749 21.0 11.9 5.3 3.3 ## 166 49.867 10.450 Angela 415 8.0 8.9 3.2 6.7 ## 167 34.550 -116.883 Angela 920 16.6 16.6 4.5 7.1 ## 168 59.333 16.850 Angela 33 6.2 7.3 2.5 7.3 ## 169 43.533 171.550 Angela 1070 7.8 9.4 4.5 3.9 ## 170 41.417 2.100 Angela 314 15.3 7.0 3.1 5.3 ## 171 2.500 -60.000 Angela 111 27.0 8.6 8.3 0.4 ## 172 38.750 -96.583 Angela 407 12.1 12.8 3.0 9.9 ## 173 17.472 178.847 Laura 41 25.0 6.3 6.4 1.2 ## 174 14.967 143.583 Angela 85 26.0 12.4 6.2 2.3 ## 175 17.846 178.706 Laura 8 24.9 6.3 6.4 1.3 ## 176 33.200 26.367 Angela 614 16.7 13.6 5.9 3.1 ## 177 63.817 20.267 Angela 21 2.7 8.5 2.4 8.9 ## 178 46.500 7.000 Angela 1608 3.5 7.5 3.0 6.0 ## temp.max.warm temp.min.cold temp.ann.range temp.mean.wetqr temp.mean.dryqr ## 1 27.0 0.3 26.7 4.9 17.4 ## 2 31.2 16.7 14.5 25.1 23.2 ## 3 37.0 3.6 33.4 28.1 14.8 ## 4 30.7 8.7 22.0 13.6 25.3 ## 5 28.6 -9.5 38.1 21.6 -3.3 ## 6 29.0 15.5 13.5 25.4 20.4 ## 7 26.1 5.5 20.6 21.2 12.3 ## 8 30.6 25.2 5.4 27.9 27.5 ## 9 32.9 -2.6 35.5 15.6 21.5 ## 10 29.9 23.2 6.7 26.8 25.7 ## 11 21.2 -9.0 30.2 6.5 -1.6 ## 12 31.9 17.8 14.1 27.2 22.8 ## 13 25.3 14.6 10.7 21.1 17.4 ## 14 31.1 23.7 7.4 27.2 27.2 ## 15 30.0 20.1 9.9 26.1 23.0 ## 16 30.1 3.6 26.5 12.3 22.8 ## 17 28.5 11.0 17.5 23.4 17.2 ## 18 30.3 20.1 10.2 26.5 23.2 ## 19 28.9 18.8 10.1 25.1 21.7 ## 20 24.2 4.0 20.2 9.6 17.7 ## 21 32.5 20.8 11.7 25.9 26.4 ## 22 23.5 -7.3 30.8 -1.5 13.1 ## 23 31.5 20.4 11.1 25.4 25.9 ## 24 35.2 5.0 30.2 26.7 13.6 ## 25 30.1 20.1 10.0 26.5 23.3 ## 26 32.5 21.6 10.9 27.5 25.7 ## 27 30.2 20.4 9.8 26.5 23.4 ## 28 30.4 20.9 9.5 26.8 23.9 ## 29 31.2 22.1 9.1 25.7 26.4 ## 30 29.1 7.3 21.8 22.5 11.2 ## 31 29.8 19.1 10.7 25.7 22.3 ## 32 33.8 10.8 23.0 18.2 23.1 ## 33 27.3 6.3 21.0 12.2 20.2 ## 34 27.7 2.6 25.1 7.5 18.6 ## 35 31.7 8.5 23.2 13.5 23.5 ## 36 6.0 -26.5 32.5 -4.3 -7.4 ## 37 31.8 -2.4 34.2 24.6 10.2 ## 38 21.2 -12.2 33.4 13.5 -3.6 ## 39 32.9 7.2 25.7 23.5 15.2 ## 40 24.5 2.7 21.8 13.7 17.0 ## 41 31.4 22.2 9.2 25.8 26.6 ## 42 30.1 15.7 14.4 27.2 18.5 ## 43 21.3 -20.8 42.1 10.5 4.0 ## 44 30.5 21.9 8.6 25.6 25.7 ## 45 30.4 21.1 9.3 26.8 24.0 ## 46 29.8 20.1 9.7 26.1 23.0 ## 47 31.2 7.2 24.0 11.7 22.7 ## 48 32.2 20.7 11.5 26.2 26.8 ## 49 27.8 5.8 22.0 10.9 20.2 ## 50 34.0 17.8 16.2 26.6 25.8 ## 51 34.6 17.2 17.4 28.1 24.7 ## 52 31.7 12.9 18.8 27.1 20.3 ## 53 14.8 -4.4 19.2 -0.1 8.9 ## 54 8.3 -20.7 29.0 1.2 -16.2 ## 55 21.8 -10.2 32.0 10.6 -5.7 ## 56 26.1 5.9 20.2 21.4 12.6 ## 57 19.0 -14.2 33.2 13.3 -8.6 ## 58 30.7 4.8 25.9 11.1 21.8 ## 59 30.2 20.6 9.6 26.6 23.6 ## 60 29.5 2.9 26.6 18.8 10.6 ## 61 31.5 17.4 14.1 25.6 23.3 ## 62 30.0 19.7 10.3 26.0 22.9 ## 63 31.0 5.7 25.3 13.2 23.2 ## 64 24.4 4.0 20.4 9.6 17.7 ## 65 33.6 16.0 17.6 27.7 25.2 ## 66 29.6 4.4 25.2 11.3 21.4 ## 67 14.1 -21.2 35.3 6.3 -13.7 ## 68 30.6 15.4 15.2 23.5 23.9 ## 69 31.6 18.5 13.1 24.4 24.8 ## 70 27.7 18.3 9.4 23.8 21.0 ## 71 14.5 -9.5 24.0 6.0 -0.6 ## 72 14.0 -5.6 19.6 3.5 4.2 ## 73 30.3 4.9 25.4 9.8 22.9 ## 74 28.9 13.0 15.9 23.4 20.0 ## 75 30.4 20.9 9.5 26.8 23.9 ## 76 22.4 -26.7 49.1 14.7 -10.5 ## 77 24.1 4.0 20.1 9.6 17.7 ## 78 31.6 12.8 18.8 27.0 20.2 ## 79 29.5 2.8 26.7 22.6 10.5 ## 80 34.4 18.7 15.7 26.5 25.7 ## 81 29.9 22.5 7.4 25.9 26.0 ## 82 21.8 -4.8 26.6 15.3 1.0 ## 83 25.9 6.4 19.5 10.5 19.3 ## 84 27.2 10.3 16.9 22.1 16.5 ## 85 31.4 17.2 14.2 25.5 23.2 ## 86 14.6 -14.1 28.7 8.1 -6.7 ## 87 23.5 -7.3 30.8 -1.5 13.1 ## 88 29.3 20.0 9.3 25.6 22.8 ## 89 30.0 15.2 14.8 27.1 18.0 ## 90 19.6 2.2 17.4 6.5 13.8 ## 91 33.2 18.5 14.7 27.7 24.8 ## 92 19.9 0.1 19.8 10.6 9.9 ## 93 25.8 -5.7 31.5 19.9 1.4 ## 94 30.2 21.0 9.2 26.5 23.8 ## 95 14.1 -21.2 35.3 6.3 -13.7 ## 96 25.4 -1.6 27.0 2.9 16.8 ## 97 30.7 6.2 24.5 23.4 14.4 ## 98 30.2 20.4 9.8 26.5 23.3 ## 99 29.8 6.0 23.8 23.6 14.6 ## 100 26.0 5.7 20.3 21.3 12.4 ## 101 34.6 17.2 17.4 28.1 24.7 ## 102 18.9 3.5 15.4 7.6 14.0 ## 103 31.4 17.2 14.2 25.4 23.2 ## 104 20.6 -31.4 52.0 12.7 -15.0 ## 105 27.6 -9.5 37.1 10.3 -3.0 ## 106 30.1 20.1 10.0 26.2 23.2 ## 107 29.3 7.5 21.8 20.2 15.0 ## 108 14.8 -13.7 28.5 8.3 -6.4 ## 109 20.3 -2.6 22.9 1.7 12.4 ## 110 30.2 19.9 10.3 26.1 23.1 ## 111 22.8 6.2 16.6 10.2 17.6 ## 112 29.6 -4.5 34.1 20.6 13.4 ## 113 27.7 -3.6 31.3 17.3 6.9 ## 114 24.4 2.6 21.8 13.7 17.0 ## 115 17.5 0.1 17.4 4.0 11.4 ## 116 28.7 16.8 11.9 25.3 20.4 ## 117 30.2 20.5 9.7 26.6 23.5 ## 118 41.7 4.5 37.2 15.5 25.7 ## 119 31.4 19.6 11.8 24.8 25.4 ## 120 6.5 -25.8 32.3 -3.7 -6.8 ## 121 35.2 8.7 26.5 15.7 24.0 ## 122 30.7 16.4 14.3 23.9 23.3 ## 123 28.4 -2.4 30.8 22.9 2.3 ## 124 31.8 6.4 25.4 12.1 24.7 ## 125 31.4 17.2 14.2 25.6 23.3 ## 126 13.8 -15.3 29.1 9.1 -8.7 ## 127 20.9 -9.2 30.1 6.1 -1.8 ## 128 14.0 -5.6 19.6 3.5 4.2 ## 129 37.0 3.6 33.4 28.1 14.8 ## 130 31.4 17.3 14.1 25.2 23.6 ## 131 26.7 6.2 20.5 12.0 20.5 ## 132 29.3 0.1 29.2 21.8 8.2 ## 133 36.2 7.6 28.6 26.0 15.6 ## 134 25.0 8.5 16.5 13.2 20.1 ## 135 22.5 -9.5 32.0 -0.8 13.4 ## 136 30.7 4.8 25.9 13.0 21.9 ## 137 27.9 11.8 16.1 21.3 19.3 ## 138 36.3 2.5 33.8 27.2 22.2 ## 139 30.1 7.3 22.8 20.9 15.1 ## 140 22.6 -1.3 23.9 3.9 15.6 ## 141 32.8 0.5 32.3 6.5 21.4 ## 142 29.3 10.7 18.6 23.9 17.4 ## 143 30.1 20.1 10.0 26.2 23.2 ## 144 39.2 2.6 36.6 29.5 23.8 ## 145 23.6 -20.9 44.5 -10.6 5.6 ## 146 21.0 -27.1 48.1 13.2 -12.1 ## 147 20.4 -10.2 30.6 -1.8 11.8 ## 148 36.3 1.5 34.8 9.9 22.6 ## 149 33.4 0.6 32.8 26.1 13.0 ## 150 35.0 9.0 26.0 14.7 26.0 ## 151 20.4 6.9 13.5 14.8 12.2 ## 152 18.2 0.0 18.2 4.2 12.0 ## 153 33.9 18.5 15.4 28.2 25.2 ## 154 31.9 7.7 24.2 13.1 21.5 ## 155 25.7 4.2 21.5 11.1 18.4 ## 156 27.7 -15.5 43.2 3.4 -7.3 ## 157 32.3 -9.1 41.4 22.1 -1.4 ## 158 32.2 21.1 11.1 26.1 26.6 ## 159 21.5 -10.5 32.0 10.2 -6.0 ## 160 36.3 3.1 33.2 26.6 14.2 ## 161 36.3 3.1 33.2 26.6 14.2 ## 162 27.4 -6.5 33.9 1.8 18.3 ## 163 10.0 -11.7 21.7 -4.9 0.1 ## 164 19.1 -0.3 19.4 8.8 12.2 ## 165 30.5 8.4 22.1 23.9 17.5 ## 166 22.9 -4.1 27.0 16.5 0.5 ## 167 36.9 0.4 36.5 9.3 23.3 ## 168 21.7 -6.6 28.3 14.7 0.2 ## 169 18.5 -2.0 20.5 5.5 12.5 ## 170 27.2 5.2 22.0 16.3 22.2 ## 171 32.5 22.2 10.3 26.6 27.4 ## 172 32.4 -9.0 41.4 22.1 -1.3 ## 173 30.1 20.4 9.7 26.4 23.3 ## 174 35.1 15.1 20.0 27.8 23.4 ## 175 30.1 20.3 9.8 26.5 23.3 ## 176 27.4 4.6 22.8 20.6 13.1 ## 177 20.7 -13.8 34.5 13.1 -4.2 ## 178 16.5 -7.9 24.4 -4.1 4.6 ## temp.mean.warmqr temp.mean.coldqr rain rain.wetm rain.drym rain.seas ## 1 17.6 4.5 1208 217 13 69 ## 2 25.3 23.1 3015 416 99 45 ## 3 28.1 12.8 278 37 9 42 ## 4 25.7 13.6 598 159 0 115 ## 5 21.6 -3.3 976 104 44 23 ## 6 25.4 19.7 1387 216 59 46 ## 7 21.4 11.5 1283 157 63 29 ## 8 27.9 27.5 2585 300 82 34 ## 9 26.1 3.8 1262 129 66 18 ## 10 27.1 25.5 1704 309 16 66 ## 11 16.1 -5.0 664 77 31 28 ## 12 27.5 22.3 2087 459 26 93 ## 13 21.1 17.3 3191 412 160 31 ## 14 27.5 26.9 3031 368 144 31 ## 15 26.3 23.0 2770 381 118 37 ## 16 23.1 8.6 355 43 5 44 ## 17 23.4 17.2 926 129 32 43 ## 18 26.5 23.0 1831 278 76 41 ## 19 25.1 21.7 2814 379 126 35 ## 20 18.1 9.6 598 120 2 87 ## 21 27.2 24.8 2110 287 49 45 ## 22 13.1 -2.1 1427 246 23 65 ## 23 26.4 25.1 2012 255 102 30 ## 24 27.3 12.1 338 44 16 28 ## 25 26.5 23.2 2767 390 113 38 ## 26 27.7 25.6 1184 201 33 60 ## 27 26.5 23.4 2664 384 103 40 ## 28 26.8 23.9 2494 362 88 44 ## 29 27.3 25.5 2607 390 29 56 ## 30 25.4 11.2 3042 521 137 39 ## 31 26.0 22.2 2314 316 119 34 ## 32 27.6 17.0 216 50 0 83 ## 33 20.3 11.5 1052 160 29 59 ## 34 18.6 7.5 723 92 32 36 ## 35 23.9 13.5 762 160 8 84 ## 36 1.7 -21.4 252 27 11 22 ## 37 24.6 4.8 1150 113 81 12 ## 38 15.2 -8.1 637 84 33 29 ## 39 25.4 15.2 703 149 14 71 ## 40 18.3 7.4 214 28 9 35 ## 41 27.5 25.6 2542 383 27 56 ## 42 27.7 18.5 1315 151 52 27 ## 43 11.0 -10.8 526 53 30 16 ## 44 26.3 25.4 2315 253 147 14 ## 45 26.8 24.0 2462 356 83 45 ## 46 26.1 23.0 2660 382 103 41 ## 47 23.1 11.7 1003 213 13 85 ## 48 27.1 25.1 3991 481 162 30 ## 49 20.3 10.9 384 45 18 29 ## 50 27.5 23.7 3048 515 57 68 ## 51 29.2 24.7 1505 371 1 104 ## 52 27.3 19.4 1027 251 9 101 ## 53 8.9 -0.7 2043 204 105 16 ## 54 4.3 -16.2 305 38 16 30 ## 55 16.1 -6.3 599 77 26 34 ## 56 21.5 11.9 1307 159 64 29 ## 57 13.3 -8.6 2142 303 63 51 ## 58 22.0 10.4 276 31 15 23 ## 59 26.6 23.6 2576 375 96 42 ## 60 20.7 10.6 281 36 17 22 ## 61 25.7 23.3 3273 471 113 46 ## 62 26.3 22.7 2674 345 132 32 ## 63 23.2 12.4 583 94 11 63 ## 64 18.0 9.6 630 127 2 86 ## 65 28.0 23.9 790 215 1 110 ## 66 21.4 11.3 546 87 14 60 ## 67 8.1 -16.6 257 31 11 33 ## 68 24.9 20.0 2726 506 56 66 ## 69 25.6 24.1 1649 224 43 40 ## 70 23.8 21.0 2865 383 111 40 ## 71 9.6 -6.2 788 95 41 25 ## 72 8.0 -1.2 500 57 34 15 ## 73 22.9 9.8 420 103 0 106 ## 74 23.7 17.9 1741 365 27 86 ## 75 26.8 23.9 2494 362 88 44 ## 76 14.7 -21.0 301 54 8 57 ## 77 18.0 9.6 603 121 2 86 ## 78 27.2 19.3 1036 253 9 101 ## 79 22.6 10.5 706 92 32 35 ## 80 27.4 25.7 1661 261 14 66 ## 81 26.3 25.7 3612 356 238 14 ## 82 16.4 -1.7 508 61 24 28 ## 83 19.3 10.5 682 96 22 47 ## 84 22.1 16.2 964 139 28 49 ## 85 25.6 23.2 3283 472 113 45 ## 86 9.1 -10.1 436 66 19 38 ## 87 13.1 -2.1 1427 246 23 65 ## 88 25.6 22.8 2567 351 95 42 ## 89 27.6 18.0 1327 151 53 26 ## 90 13.8 5.9 1037 115 63 17 ## 91 28.5 24.8 1664 355 2 100 ## 92 15.7 2.9 781 88 42 25 ## 93 20.1 -1.2 1975 288 102 37 ## 94 26.5 23.8 2444 343 84 44 ## 95 8.1 -16.6 257 31 11 33 ## 96 16.9 2.4 2561 458 21 71 ## 97 24.2 14.4 793 162 4 89 ## 98 26.5 23.3 2662 386 103 40 ## 99 23.6 12.9 882 184 10 82 ## 100 21.4 11.7 1313 160 65 29 ## 101 29.2 24.7 1505 371 1 104 ## 102 14.4 7.2 1936 315 27 62 ## 103 25.6 23.2 3283 471 113 45 ## 104 12.7 -25.5 244 45 7 59 ## 105 20.2 -3.9 996 96 62 13 ## 106 26.4 23.0 2803 390 117 37 ## 107 20.9 15.0 1099 270 0 118 ## 108 9.3 -9.7 422 64 18 39 ## 109 12.4 1.7 972 175 28 56 ## 110 26.4 22.9 2993 372 150 30 ## 111 17.6 9.5 915 101 45 25 ## 112 22.4 2.3 1121 117 71 12 ## 113 18.0 3.8 208 23 13 17 ## 114 18.3 7.4 214 28 9 35 ## 115 11.4 3.6 1720 182 72 23 ## 116 25.3 20.0 1397 235 37 57 ## 117 26.6 23.5 2598 378 100 41 ## 118 31.9 13.9 73 12 0 51 ## 119 25.4 24.3 3329 503 78 47 ## 120 2.3 -20.7 236 26 10 24 ## 121 26.5 14.5 475 108 7 84 ## 122 25.9 21.4 1545 214 54 46 ## 123 22.9 2.3 1263 181 50 43 ## 124 24.7 12.1 293 69 0 106 ## 125 25.7 23.3 3269 470 112 45 ## 126 9.6 -11.1 657 133 24 61 ## 127 15.7 -5.1 691 78 34 28 ## 128 8.0 -1.2 500 57 34 15 ## 129 28.1 12.7 278 37 9 42 ## 130 25.6 23.6 2920 397 99 43 ## 131 20.5 12.0 501 63 24 29 ## 132 21.8 8.2 780 105 37 33 ## 133 28.5 15.6 803 154 20 66 ## 134 20.2 13.2 484 59 20 34 ## 135 16.5 -3.8 1379 149 91 16 ## 136 22.0 10.5 272 30 15 23 ## 137 21.7 19.0 1698 301 7 78 ## 138 27.9 10.4 380 71 6 61 ## 139 21.7 15.1 1085 265 0 118 ## 140 15.6 3.3 174 23 7 35 ## 141 21.4 6.5 867 182 1 88 ## 142 23.9 17.4 834 116 25 47 ## 143 26.4 23.0 2835 401 116 37 ## 144 30.0 11.4 354 57 8 50 ## 145 12.6 -10.6 539 60 28 20 ## 146 13.2 -21.9 296 57 7 68 ## 147 15.6 -5.4 977 110 64 18 ## 148 25.4 8.9 290 46 2 57 ## 149 26.1 8.2 1165 123 63 19 ## 150 26.4 14.7 520 117 5 83 ## 151 14.8 11.7 1019 192 9 73 ## 152 12.0 3.7 1156 117 54 20 ## 153 29.1 25.2 1663 428 1 106 ## 154 23.8 13.1 597 132 6 83 ## 155 18.4 8.8 1016 107 52 20 ## 156 15.9 -7.3 374 37 23 13 ## 157 24.5 -1.4 872 137 20 51 ## 158 27.1 25.8 1974 254 103 30 ## 159 15.8 -6.6 597 78 26 34 ## 160 27.5 12.1 310 41 11 42 ## 161 27.5 12.1 310 41 11 42 ## 162 20.9 -0.8 1176 117 81 11 ## 163 5.2 -8.5 1418 159 71 23 ## 164 13.6 4.3 2421 247 131 15 ## 165 24.3 16.1 1476 275 20 74 ## 166 16.5 -0.7 692 80 43 18 ## 167 25.9 8.1 212 34 2 60 ## 168 15.9 -2.8 564 70 27 29 ## 169 12.6 2.6 1211 120 71 12 ## 170 22.4 8.9 656 85 30 26 ## 171 27.6 26.5 2319 299 108 31 ## 172 24.5 -1.3 859 133 20 49 ## 173 26.4 23.3 2616 379 100 41 ## 174 28.4 22.7 1117 290 2 113 ## 175 26.5 23.2 2731 394 109 39 ## 176 20.6 12.5 630 73 28 27 ## 177 14.5 -8.6 572 70 32 26 ## 178 11.1 -4.1 1555 155 112 10 ## rain.wetqr rain.dryqr rain.warmqr rain.coldqr LAI NPP hemisphere ## 1 601 68 75 560 2.51 572 1 ## 2 1177 340 928 359 4.26 1405 -1 ## 3 109 35 109 42 1.32 756 -1 ## 4 408 0 2 408 1.01 359 1 ## 5 299 165 299 165 3.26 1131 1 ## 6 600 186 600 212 6.99 1552 -1 ## 7 450 208 385 279 4.14 1563 -1 ## 8 870 305 855 405 NA NA 1 ## 9 382 249 268 325 3.14 1266 1 ## 10 806 92 659 135 4.51 2296 -1 ## 11 220 106 191 137 3.07 536 1 ## 12 1294 92 1031 108 4.04 908 -1 ## 13 1136 521 1136 523 4.26 1795 -1 ## 14 1055 436 689 489 NA NA 1 ## 15 1006 391 1005 391 4.51 1864 -1 ## 16 121 27 33 114 2.79 991 1 ## 17 347 104 344 104 3.35 525 -1 ## 18 724 257 724 270 4.50 1800 -1 ## 19 1027 413 1027 413 4.26 1795 -1 ## 20 311 9 13 311 1.51 223 1 ## 21 735 217 458 542 4.51 1857 1 ## 22 692 106 106 642 2.07 478 1 ## 23 719 327 486 652 4.51 2270 1 ## 24 116 67 100 68 2.07 490 -1 ## 25 1012 379 1012 381 4.51 1864 -1 ## 26 556 117 486 123 3.26 907 -1 ## 27 1011 349 1011 349 4.51 1864 -1 ## 28 1003 298 1003 298 4.26 1886 -1 ## 29 1026 138 407 705 4.51 2146 1 ## 30 1117 495 837 495 4.26 1335 1 ## 31 836 372 834 384 4.51 1864 -1 ## 32 127 5 54 108 1.51 464 -1 ## 33 468 95 98 459 2.01 517 -1 ## 34 259 102 102 259 2.26 698 -1 ## 35 426 33 39 426 1.51 575 -1 ## 36 76 40 60 68 2.60 4 1 ## 37 325 252 325 277 3.26 1274 1 ## 38 221 104 209 129 2.76 477 1 ## 39 325 54 268 54 2.26 844 1 ## 40 77 36 37 55 1.01 404 -1 ## 41 1004 132 397 975 4.51 2146 1 ## 42 414 195 394 195 NA NA 1 ## 43 148 105 134 144 2.24 339 1 ## 44 692 512 515 670 4.51 2246 1 ## 45 1007 284 1007 284 4.26 1886 -1 ## 46 1023 345 1023 345 4.50 1800 -1 ## 47 566 50 54 566 1.93 708 -1 ## 48 1281 581 741 1206 4.51 2132 1 ## 49 127 59 60 127 2.17 661 -1 ## 50 1422 195 680 304 4.51 1809 1 ## 51 970 7 408 7 4.07 956 -1 ## 52 668 33 626 42 4.14 1012 -1 ## 53 582 388 388 580 4.26 1102 -1 ## 54 106 52 87 52 1.01 32 1 ## 55 217 90 185 105 1.01 386 1 ## 56 455 212 391 280 4.51 1435 -1 ## 57 885 204 885 204 3.26 751 1 ## 58 84 49 51 80 2.14 773 -1 ## 59 1003 325 1003 325 4.50 1800 -1 ## 60 92 55 71 55 1.26 255 -1 ## 61 1310 381 1001 381 4.26 1475 -1 ## 62 943 414 925 431 4.26 1795 -1 ## 63 271 48 48 266 1.51 633 -1 ## 64 328 9 13 328 1.51 223 1 ## 65 546 12 403 29 4.07 855 1 ## 66 253 48 48 253 1.51 557 -1 ## 67 92 40 84 45 1.29 65 1 ## 68 1248 198 463 371 4.14 991 1 ## 69 603 177 352 504 4.26 1648 1 ## 70 1110 374 1110 374 4.50 1800 -1 ## 71 260 132 213 193 1.51 121 1 ## 72 152 106 120 122 1.26 30 -1 ## 73 265 0 0 265 1.26 305 1 ## 74 1031 92 868 116 4.04 908 -1 ## 75 1003 298 1003 298 4.26 1886 -1 ## 76 144 31 144 47 2.51 317 1 ## 77 313 9 13 313 1.51 223 1 ## 78 674 33 631 43 4.14 1012 -1 ## 79 258 111 258 111 2.51 1119 -1 ## 80 759 64 375 120 4.07 1225 -1 ## 81 1050 761 881 838 4.51 2337 1 ## 82 172 81 162 112 2.79 533 1 ## 83 272 75 75 272 2.14 695 -1 ## 84 372 97 371 97 3.35 525 -1 ## 85 1313 383 1004 383 4.26 1475 -1 ## 86 172 65 162 89 1.24 61 1 ## 87 692 106 106 642 2.07 478 1 ## 88 1009 324 1009 324 4.50 1800 -1 ## 89 409 196 382 196 NA NA 1 ## 90 311 205 205 296 2.82 743 -1 ## 91 1032 12 453 12 4.04 969 -1 ## 92 256 139 200 184 3.46 733 1 ## 93 721 331 706 349 4.14 1106 1 ## 94 989 288 989 288 4.50 1800 -1 ## 95 92 40 84 45 1.29 65 1 ## 96 1278 125 142 1214 1.76 579 1 ## 97 455 20 439 20 2.42 476 -1 ## 98 1008 350 1008 350 4.50 1800 -1 ## 99 472 33 472 34 1.51 596 -1 ## 100 458 213 392 283 4.51 1435 -1 ## 101 970 7 408 7 4.07 956 -1 ## 102 900 133 134 848 2.51 572 1 ## 103 1312 383 1004 383 4.26 1475 -1 ## 104 122 28 122 38 2.07 192 1 ## 105 275 200 271 213 3.14 972 1 ## 106 1017 389 1013 393 4.51 1864 -1 ## 107 790 0 383 0 2.51 684 -1 ## 108 168 62 158 85 1.24 61 1 ## 109 443 106 106 443 4.14 691 -1 ## 110 1025 468 969 483 4.51 1864 -1 ## 111 289 150 150 287 4.04 1023 -1 ## 112 322 240 310 265 4.07 1325 1 ## 113 65 44 61 47 1.89 522 -1 ## 114 77 36 37 55 1.01 404 -1 ## 115 530 280 280 495 2.96 643 -1 ## 116 650 137 650 172 NA NA -1 ## 117 994 339 994 339 4.50 1800 -1 ## 118 31 4 10 30 1.51 415 1 ## 119 1245 304 328 1054 4.51 1896 1 ## 120 73 36 57 63 2.60 4 1 ## 121 267 27 32 267 1.99 411 -1 ## 122 616 163 319 256 4.51 1761 1 ## 123 506 162 506 162 3.57 932 1 ## 124 186 0 0 186 1.26 305 1 ## 125 1308 380 1001 397 4.26 1475 -1 ## 126 313 85 256 90 2.15 204 1 ## 127 225 112 202 144 2.76 474 1 ## 128 152 106 120 122 1.26 30 -1 ## 129 109 35 109 42 1.32 756 -1 ## 130 1125 336 891 350 4.26 1461 -1 ## 131 171 81 89 171 2.14 825 -1 ## 132 287 133 287 133 3.26 1272 -1 ## 133 338 68 285 68 2.26 1044 1 ## 134 176 67 77 176 2.14 825 -1 ## 135 432 294 296 400 3.26 711 1 ## 136 83 49 50 79 2.14 773 -1 ## 137 853 33 472 49 4.26 1180 -1 ## 138 176 24 144 98 1.01 477 1 ## 139 778 0 383 0 4.03 652 -1 ## 140 65 25 25 57 2.42 505 -1 ## 141 469 11 11 469 2.14 595 1 ## 142 321 84 309 84 3.35 525 -1 ## 143 1026 385 1019 398 4.51 1864 -1 ## 144 143 29 117 97 1.26 349 1 ## 145 166 100 123 166 2.24 339 1 ## 146 157 28 157 41 2.51 363 1 ## 147 308 201 222 239 NA NA 1 ## 148 129 22 50 126 1.01 492 1 ## 149 358 220 358 297 3.35 1415 1 ## 150 292 29 30 292 1.67 75 -1 ## 151 451 33 451 44 3.48 1209 -1 ## 152 347 200 200 322 2.96 643 -1 ## 153 1091 10 428 10 4.07 956 -1 ## 154 333 34 38 333 1.67 75 -1 ## 155 313 176 176 275 2.71 793 -1 ## 156 106 77 93 77 0.51 261 1 ## 157 355 71 326 71 4.14 1414 1 ## 158 710 324 524 633 4.51 2270 1 ## 159 216 90 184 106 2.82 502 1 ## 160 121 40 121 46 1.01 605 -1 ## 161 121 40 121 46 1.01 605 -1 ## 162 332 256 262 311 3.39 1044 1 ## 163 446 244 337 399 1.01 45 1 ## 164 704 502 537 580 1.46 41 -1 ## 165 753 67 616 73 4.04 1435 1 ## 166 218 143 218 155 3.57 827 1 ## 167 97 16 19 95 1.26 279 1 ## 168 197 90 188 116 3.07 615 1 ## 169 339 259 266 326 1.26 30 -1 ## 170 221 132 160 134 2.26 826 1 ## 171 803 352 415 718 4.26 1692 -1 ## 172 338 73 310 73 2.76 1127 1 ## 173 1012 335 1012 335 4.50 1800 -1 ## 174 767 9 501 13 4.03 811 -1 ## 175 1011 365 1011 371 4.51 1864 -1 ## 176 197 98 197 104 1.76 933 -1 ## 177 189 99 168 128 2.79 414 1 ## 178 438 359 385 438 3.26 690 1 fullModel &lt;- lm(loght ~ (growthform + Family + lat + long + alt + temp + NPP )^2, data = plantHeight) selectedModel = stepAIC(fullModel) ## Start: AIC=-786.71 ## loght ~ (growthform + Family + lat + long + alt + temp + NPP)^2 ## ## ## Step: AIC=-786.71 ## loght ~ growthform + Family + lat + long + alt + temp + NPP + ## growthform:Family + growthform:lat + growthform:long + growthform:alt + ## growthform:temp + growthform:NPP + Family:lat + Family:long + ## Family:alt + Family:temp + lat:long + lat:alt + lat:temp + ## lat:NPP + long:alt + long:temp + long:NPP + alt:temp + alt:NPP + ## temp:NPP ## ## ## Step: AIC=-786.71 ## loght ~ growthform + Family + lat + long + alt + temp + NPP + ## growthform:Family + growthform:lat + growthform:long + growthform:alt + ## growthform:NPP + Family:lat + Family:long + Family:alt + ## Family:temp + lat:long + lat:alt + lat:temp + lat:NPP + long:alt + ## long:temp + long:NPP + alt:temp + alt:NPP + temp:NPP ## ## ## Step: AIC=-786.71 ## loght ~ growthform + Family + lat + long + alt + temp + NPP + ## growthform:Family + growthform:lat + growthform:long + growthform:NPP + ## Family:lat + Family:long + Family:alt + Family:temp + lat:long + ## lat:alt + lat:temp + lat:NPP + long:alt + long:temp + long:NPP + ## alt:temp + alt:NPP + temp:NPP ## ## Df Sum of Sq RSS AIC ## - lat:alt 1 0.000009 0.19505 -788.71 ## - alt:NPP 1 0.000838 0.19588 -788.01 ## - long:alt 1 0.000979 0.19602 -787.90 ## &lt;none&gt; 0.19504 -786.71 ## - lat:NPP 1 0.002655 0.19770 -786.51 ## - growthform:Family 1 0.004059 0.19910 -785.36 ## - growthform:long 1 0.004503 0.19954 -784.99 ## - temp:NPP 1 0.008428 0.20347 -781.82 ## - lat:long 1 0.012808 0.20785 -778.35 ## - alt:temp 1 0.034897 0.22994 -761.88 ## - growthform:NPP 2 0.055667 0.25071 -749.79 ## - long:temp 1 0.054907 0.24995 -748.28 ## - long:NPP 1 0.069899 0.26494 -738.79 ## - growthform:lat 1 0.072656 0.26770 -737.10 ## - Family:lat 2 0.180837 0.37588 -683.78 ## - Family:alt 3 0.203022 0.39806 -676.43 ## - Family:temp 3 0.209489 0.40453 -673.80 ## - lat:temp 1 0.231841 0.42688 -661.03 ## - Family:long 2 0.285742 0.48078 -643.65 ## ## Step: AIC=-788.71 ## loght ~ growthform + Family + lat + long + alt + temp + NPP + ## growthform:Family + growthform:lat + growthform:long + growthform:NPP + ## Family:lat + Family:long + Family:alt + Family:temp + lat:long + ## lat:temp + lat:NPP + long:alt + long:temp + long:NPP + alt:temp + ## alt:NPP + temp:NPP ## ## Df Sum of Sq RSS AIC ## - alt:NPP 1 0.00095 0.19600 -789.91 ## - long:alt 1 0.00158 0.19663 -789.39 ## &lt;none&gt; 0.19505 -788.71 ## - lat:NPP 1 0.00444 0.19949 -787.04 ## - growthform:Family 1 0.00448 0.19953 -787.00 ## - growthform:long 1 0.00481 0.19985 -786.74 ## - temp:NPP 1 0.01041 0.20546 -782.23 ## - lat:long 1 0.02765 0.22270 -769.09 ## - alt:temp 1 0.05194 0.24699 -752.22 ## - growthform:NPP 2 0.05721 0.25226 -750.78 ## - long:NPP 1 0.07015 0.26520 -740.62 ## - growthform:lat 1 0.09511 0.29016 -725.96 ## - long:temp 1 0.10324 0.29829 -721.46 ## - Family:temp 3 0.27353 0.46858 -651.84 ## - Family:alt 3 0.28127 0.47631 -649.17 ## - Family:lat 2 0.27637 0.47142 -648.86 ## - lat:temp 1 0.32113 0.51618 -632.07 ## - Family:long 2 0.37967 0.57472 -616.56 ## ## Step: AIC=-789.91 ## loght ~ growthform + Family + lat + long + alt + temp + NPP + ## growthform:Family + growthform:lat + growthform:long + growthform:NPP + ## Family:lat + Family:long + Family:alt + Family:temp + lat:long + ## lat:temp + lat:NPP + long:alt + long:temp + long:NPP + alt:temp + ## temp:NPP ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.19600 -789.91 ## - growthform:long 1 0.00388 0.19988 -788.72 ## - growthform:Family 1 0.00578 0.20178 -787.18 ## - long:alt 1 0.00727 0.20327 -785.97 ## - lat:NPP 1 0.01680 0.21280 -778.51 ## - lat:long 1 0.04395 0.23995 -758.94 ## - growthform:NPP 2 0.05706 0.25306 -752.26 ## - temp:NPP 1 0.08109 0.27709 -735.48 ## - growthform:lat 1 0.10167 0.29767 -723.80 ## - long:temp 1 0.16119 0.35719 -694.09 ## - long:NPP 1 0.16687 0.36287 -691.51 ## - alt:temp 1 0.22876 0.42476 -665.85 ## - Family:temp 3 0.32442 0.52042 -636.74 ## - Family:lat 2 0.31910 0.51510 -636.42 ## - Family:alt 3 0.37962 0.57562 -620.31 ## - Family:long 2 0.48838 0.68438 -590.10 ## - lat:temp 1 0.53630 0.73230 -577.07 summary(selectedModel) ## ## Call: ## lm(formula = loght ~ growthform + Family + lat + long + alt + ## temp + NPP + growthform:Family + growthform:lat + growthform:long + ## growthform:NPP + Family:lat + Family:long + Family:alt + ## Family:temp + lat:long + lat:temp + lat:NPP + long:alt + ## long:temp + long:NPP + alt:temp + temp:NPP, data = plantHeight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.1897 0.0000 0.0000 0.0000 0.1998 ## ## Coefficients: (521 not defined because of singularities) ## Estimate Std. Error t value ## (Intercept) -1.612e+01 2.152e+01 -0.749 ## growthformHerb 1.598e+02 1.822e+02 0.877 ## growthformHerb/Shrub -2.013e-01 3.854e+00 -0.052 ## growthformShrub -1.992e+02 2.590e+02 -0.769 ## growthformShrub/Tree -1.126e+02 1.952e+02 -0.577 ## growthformTree 2.053e+01 1.530e+01 1.342 ## FamilyAsteraceae -3.562e+02 1.352e+02 -2.636 ## FamilyAtherospermataceae -2.866e+01 2.295e+01 -1.248 ## FamilyBalsaminaceae 1.530e+02 1.927e+02 0.794 ## FamilyBetulaceae -3.010e+01 1.056e+02 -0.285 ## FamilyBrassicaceae -1.271e+02 1.910e+02 -0.665 ## FamilyCactaceae 2.620e+02 2.820e+02 0.929 ## FamilyCasuarinaceae 1.995e+02 2.582e+02 0.773 ## FamilyChloranthaceae 7.419e-01 9.460e-01 0.784 ## FamilyChrysobalanaceae 5.392e+00 8.314e+00 0.649 ## FamilyCistaceae 1.999e+02 2.586e+02 0.773 ## FamilyCornaceae 3.361e+02 4.220e+02 0.796 ## FamilyCrassulaceae 1.954e+02 2.551e+02 0.766 ## FamilyCunoniaceae 1.716e-01 3.008e-01 0.571 ## FamilyCupressaceae -3.435e+01 3.185e+01 -1.079 ## FamilyCyperaceae -1.814e+01 2.054e+01 -0.883 ## FamilyDennstaedtiaceae NA NA NA ## FamilyDicksoniaceae 1.878e+02 2.490e+02 0.754 ## FamilyDipterocarpaceae 4.334e+00 4.919e+00 0.881 ## FamilyEbenaceae 5.281e+00 5.975e+00 0.884 ## FamilyElaeocarpaceae 5.440e-01 5.242e-01 1.038 ## FamilyEricaceae 2.186e+02 2.792e+02 0.783 ## FamilyEuphorbiaceae 3.472e+00 4.301e+00 0.807 ## FamilyFabaceae - C 2.644e+00 3.387e+00 0.781 ## FamilyFabaceae - M -2.768e+00 2.238e+00 -1.237 ## FamilyFabaceae - P 5.128e+01 5.128e+01 1.000 ## FamilyFagaceae -2.036e+00 7.802e+00 -0.261 ## FamilyGentianaceae -1.963e+02 2.264e+02 -0.867 ## FamilyHeliconiaceae -1.193e+02 1.413e+02 -0.845 ## FamilyJuglandaceae -2.223e+01 2.069e+01 -1.074 ## FamilyJuncaginaceae 2.855e+02 3.596e+02 0.794 ## FamilyLamiaceae 1.876e+02 2.522e+02 0.744 ## FamilyLauraceae -2.595e+00 4.184e+00 -0.620 ## FamilyMaesaceae 2.097e+02 2.652e+02 0.791 ## FamilyMalvaceae 3.153e+00 4.933e+00 0.639 ## FamilyMelastomataceae 2.135e+02 2.701e+02 0.790 ## FamilyMoraceae -5.805e+00 6.329e+00 -0.917 ## FamilyMyristicaceae 8.819e-01 8.987e-01 0.981 ## FamilyMyrsinaceae -9.893e+00 1.109e+01 -0.892 ## FamilyMyrtaceae -3.193e+01 1.040e+01 -3.070 ## FamilyOchnaceae 2.125e+02 2.692e+02 0.789 ## FamilyOnagraceae NA NA NA ## FamilyOrobanchaceae -1.075e+02 1.752e+02 -0.613 ## FamilyPhyllanthaceae -1.211e+01 1.357e+01 -0.892 ## FamilyPicrodendraceae -1.256e+01 1.307e+01 -0.961 ## FamilyPinaceae 1.522e+01 8.029e+00 1.895 ## FamilyPoaceae 1.059e+02 1.625e+02 0.652 ## FamilyPolemoniaceae -1.369e+02 1.613e+02 -0.849 ## FamilyPolygonaceae -1.292e+02 1.622e+02 -0.796 ## FamilyProteaceae 1.685e+01 9.511e+00 1.772 ## FamilyRanunculaceae 1.136e+02 1.438e+02 0.790 ## FamilyRhamnaceae 1.944e+02 2.570e+02 0.756 ## FamilyRosaceae 1.148e+01 3.983e+00 2.883 ## FamilyRubiaceae 2.165e+02 2.702e+02 0.801 ## FamilyRutaceae 1.979e+02 2.555e+02 0.774 ## FamilySalicaceae 2.326e+02 2.849e+02 0.816 ## FamilySapindaceae 4.753e+00 8.974e+00 0.530 ## FamilySapotaceae -1.511e+01 1.313e+01 -1.151 ## FamilyScrophulariaceae 2.099e+02 2.660e+02 0.789 ## FamilyThymelaeaceae -9.324e+00 9.227e+00 -1.010 ## FamilyUlmaceae 1.783e+01 1.401e+01 1.273 ## FamilyUrticaceae 8.939e+00 8.901e+00 1.004 ## FamilyViolaceae 2.145e+02 2.712e+02 0.791 ## FamilyXanthorrhoeaceae 2.016e+02 2.596e+02 0.777 ## FamilyZygophyllaceae 1.871e+02 2.474e+02 0.756 ## lat 6.678e-01 6.451e-01 1.035 ## long 6.846e-03 3.808e-02 0.180 ## alt 5.440e-03 8.816e-03 0.617 ## temp -1.851e-01 2.343e-01 -0.790 ## NPP 2.978e-03 2.716e-03 1.097 ## growthformHerb:FamilyAsteraceae -1.535e+01 4.924e+01 -0.312 ## growthformHerb/Shrub:FamilyAsteraceae NA NA NA ## growthformShrub:FamilyAsteraceae NA NA NA ## growthformShrub/Tree:FamilyAsteraceae NA NA NA ## growthformTree:FamilyAsteraceae NA NA NA ## growthformHerb:FamilyAtherospermataceae NA NA NA ## growthformHerb/Shrub:FamilyAtherospermataceae NA NA NA ## growthformShrub:FamilyAtherospermataceae NA NA NA ## growthformShrub/Tree:FamilyAtherospermataceae NA NA NA ## growthformTree:FamilyAtherospermataceae NA NA NA ## growthformHerb:FamilyBalsaminaceae NA NA NA ## growthformHerb/Shrub:FamilyBalsaminaceae NA NA NA ## growthformShrub:FamilyBalsaminaceae NA NA NA ## growthformShrub/Tree:FamilyBalsaminaceae NA NA NA ## growthformTree:FamilyBalsaminaceae NA NA NA ## growthformHerb:FamilyBetulaceae NA NA NA ## growthformHerb/Shrub:FamilyBetulaceae NA NA NA ## growthformShrub:FamilyBetulaceae 2.035e+02 2.795e+02 0.728 ## growthformShrub/Tree:FamilyBetulaceae NA NA NA ## growthformTree:FamilyBetulaceae NA NA NA ## growthformHerb:FamilyBrassicaceae NA NA NA ## growthformHerb/Shrub:FamilyBrassicaceae NA NA NA ## growthformShrub:FamilyBrassicaceae NA NA NA ## growthformShrub/Tree:FamilyBrassicaceae NA NA NA ## growthformTree:FamilyBrassicaceae NA NA NA ## growthformHerb:FamilyCactaceae NA NA NA ## growthformHerb/Shrub:FamilyCactaceae NA NA NA ## growthformShrub:FamilyCactaceae NA NA NA ## growthformShrub/Tree:FamilyCactaceae NA NA NA ## growthformTree:FamilyCactaceae NA NA NA ## growthformHerb:FamilyCasuarinaceae NA NA NA ## growthformHerb/Shrub:FamilyCasuarinaceae NA NA NA ## growthformShrub:FamilyCasuarinaceae NA NA NA ## growthformShrub/Tree:FamilyCasuarinaceae NA NA NA ## growthformTree:FamilyCasuarinaceae NA NA NA ## growthformHerb:FamilyChloranthaceae NA NA NA ## growthformHerb/Shrub:FamilyChloranthaceae NA NA NA ## growthformShrub:FamilyChloranthaceae NA NA NA ## growthformShrub/Tree:FamilyChloranthaceae NA NA NA ## growthformTree:FamilyChloranthaceae NA NA NA ## growthformHerb:FamilyChrysobalanaceae NA NA NA ## growthformHerb/Shrub:FamilyChrysobalanaceae NA NA NA ## growthformShrub:FamilyChrysobalanaceae NA NA NA ## growthformShrub/Tree:FamilyChrysobalanaceae NA NA NA ## growthformTree:FamilyChrysobalanaceae NA NA NA ## growthformHerb:FamilyCistaceae NA NA NA ## growthformHerb/Shrub:FamilyCistaceae NA NA NA ## growthformShrub:FamilyCistaceae NA NA NA ## growthformShrub/Tree:FamilyCistaceae NA NA NA ## growthformTree:FamilyCistaceae NA NA NA ## growthformHerb:FamilyCornaceae NA NA NA ## growthformHerb/Shrub:FamilyCornaceae NA NA NA ## growthformShrub:FamilyCornaceae NA NA NA ## growthformShrub/Tree:FamilyCornaceae NA NA NA ## growthformTree:FamilyCornaceae NA NA NA ## growthformHerb:FamilyCrassulaceae NA NA NA ## growthformHerb/Shrub:FamilyCrassulaceae NA NA NA ## growthformShrub:FamilyCrassulaceae NA NA NA ## growthformShrub/Tree:FamilyCrassulaceae NA NA NA ## growthformTree:FamilyCrassulaceae NA NA NA ## growthformHerb:FamilyCunoniaceae NA NA NA ## growthformHerb/Shrub:FamilyCunoniaceae NA NA NA ## growthformShrub:FamilyCunoniaceae NA NA NA ## growthformShrub/Tree:FamilyCunoniaceae 2.124e+02 2.694e+02 0.789 ## growthformTree:FamilyCunoniaceae NA NA NA ## growthformHerb:FamilyCupressaceae NA NA NA ## growthformHerb/Shrub:FamilyCupressaceae NA NA NA ## growthformShrub:FamilyCupressaceae 2.214e+02 2.802e+02 0.790 ## growthformShrub/Tree:FamilyCupressaceae NA NA NA ## growthformTree:FamilyCupressaceae NA NA NA ## growthformHerb:FamilyCyperaceae NA NA NA ## growthformHerb/Shrub:FamilyCyperaceae NA NA NA ## growthformShrub:FamilyCyperaceae NA NA NA ## growthformShrub/Tree:FamilyCyperaceae NA NA NA ## growthformTree:FamilyCyperaceae NA NA NA ## growthformHerb:FamilyDennstaedtiaceae NA NA NA ## growthformHerb/Shrub:FamilyDennstaedtiaceae NA NA NA ## growthformShrub:FamilyDennstaedtiaceae NA NA NA ## growthformShrub/Tree:FamilyDennstaedtiaceae NA NA NA ## growthformTree:FamilyDennstaedtiaceae NA NA NA ## growthformHerb:FamilyDicksoniaceae NA NA NA ## growthformHerb/Shrub:FamilyDicksoniaceae NA NA NA ## growthformShrub:FamilyDicksoniaceae NA NA NA ## growthformShrub/Tree:FamilyDicksoniaceae NA NA NA ## growthformTree:FamilyDicksoniaceae NA NA NA ## growthformHerb:FamilyDipterocarpaceae NA NA NA ## growthformHerb/Shrub:FamilyDipterocarpaceae NA NA NA ## growthformShrub:FamilyDipterocarpaceae NA NA NA ## growthformShrub/Tree:FamilyDipterocarpaceae NA NA NA ## growthformTree:FamilyDipterocarpaceae NA NA NA ## growthformHerb:FamilyEbenaceae NA NA NA ## growthformHerb/Shrub:FamilyEbenaceae NA NA NA ## growthformShrub:FamilyEbenaceae NA NA NA ## growthformShrub/Tree:FamilyEbenaceae NA NA NA ## growthformTree:FamilyEbenaceae NA NA NA ## growthformHerb:FamilyElaeocarpaceae NA NA NA ## growthformHerb/Shrub:FamilyElaeocarpaceae NA NA NA ## growthformShrub:FamilyElaeocarpaceae NA NA NA ## growthformShrub/Tree:FamilyElaeocarpaceae NA NA NA ## growthformTree:FamilyElaeocarpaceae NA NA NA ## growthformHerb:FamilyEricaceae NA NA NA ## growthformHerb/Shrub:FamilyEricaceae NA NA NA ## growthformShrub:FamilyEricaceae -1.611e+00 4.441e+00 -0.363 ## growthformShrub/Tree:FamilyEricaceae NA NA NA ## growthformTree:FamilyEricaceae NA NA NA ## growthformHerb:FamilyEuphorbiaceae NA NA NA ## growthformHerb/Shrub:FamilyEuphorbiaceae NA NA NA ## growthformShrub:FamilyEuphorbiaceae 2.201e+02 2.766e+02 0.796 ## growthformShrub/Tree:FamilyEuphorbiaceae NA NA NA ## growthformTree:FamilyEuphorbiaceae NA NA NA ## growthformHerb:FamilyFabaceae - C NA NA NA ## growthformHerb/Shrub:FamilyFabaceae - C NA NA NA ## growthformShrub:FamilyFabaceae - C NA NA NA ## growthformShrub/Tree:FamilyFabaceae - C NA NA NA ## growthformTree:FamilyFabaceae - C NA NA NA ## growthformHerb:FamilyFabaceae - M NA NA NA ## growthformHerb/Shrub:FamilyFabaceae - M NA NA NA ## growthformShrub:FamilyFabaceae - M NA NA NA ## growthformShrub/Tree:FamilyFabaceae - M 2.338e+02 2.857e+02 0.818 ## growthformTree:FamilyFabaceae - M NA NA NA ## growthformHerb:FamilyFabaceae - P 1.161e+02 1.357e+02 0.855 ## growthformHerb/Shrub:FamilyFabaceae - P NA NA NA ## growthformShrub:FamilyFabaceae - P 2.457e+02 3.020e+02 0.814 ## growthformShrub/Tree:FamilyFabaceae - P NA NA NA ## growthformTree:FamilyFabaceae - P NA NA NA ## growthformHerb:FamilyFagaceae NA NA NA ## growthformHerb/Shrub:FamilyFagaceae NA NA NA ## growthformShrub:FamilyFagaceae 2.144e+02 2.735e+02 0.784 ## growthformShrub/Tree:FamilyFagaceae NA NA NA ## growthformTree:FamilyFagaceae NA NA NA ## growthformHerb:FamilyGentianaceae NA NA NA ## growthformHerb/Shrub:FamilyGentianaceae NA NA NA ## growthformShrub:FamilyGentianaceae NA NA NA ## growthformShrub/Tree:FamilyGentianaceae NA NA NA ## growthformTree:FamilyGentianaceae NA NA NA ## growthformHerb:FamilyHeliconiaceae NA NA NA ## growthformHerb/Shrub:FamilyHeliconiaceae NA NA NA ## growthformShrub:FamilyHeliconiaceae NA NA NA ## growthformShrub/Tree:FamilyHeliconiaceae NA NA NA ## growthformTree:FamilyHeliconiaceae NA NA NA ## growthformHerb:FamilyJuglandaceae NA NA NA ## growthformHerb/Shrub:FamilyJuglandaceae NA NA NA ## growthformShrub:FamilyJuglandaceae NA NA NA ## growthformShrub/Tree:FamilyJuglandaceae NA NA NA ## growthformTree:FamilyJuglandaceae NA NA NA ## growthformHerb:FamilyJuncaginaceae NA NA NA ## growthformHerb/Shrub:FamilyJuncaginaceae NA NA NA ## growthformShrub:FamilyJuncaginaceae NA NA NA ## growthformShrub/Tree:FamilyJuncaginaceae NA NA NA ## growthformTree:FamilyJuncaginaceae NA NA NA ## growthformHerb:FamilyLamiaceae NA NA NA ## growthformHerb/Shrub:FamilyLamiaceae NA NA NA ## growthformShrub:FamilyLamiaceae NA NA NA ## growthformShrub/Tree:FamilyLamiaceae NA NA NA ## growthformTree:FamilyLamiaceae NA NA NA ## growthformHerb:FamilyLauraceae NA NA NA ## growthformHerb/Shrub:FamilyLauraceae NA NA NA ## growthformShrub:FamilyLauraceae NA NA NA ## growthformShrub/Tree:FamilyLauraceae NA NA NA ## growthformTree:FamilyLauraceae NA NA NA ## growthformHerb:FamilyMaesaceae NA NA NA ## growthformHerb/Shrub:FamilyMaesaceae NA NA NA ## growthformShrub:FamilyMaesaceae NA NA NA ## growthformShrub/Tree:FamilyMaesaceae NA NA NA ## growthformTree:FamilyMaesaceae NA NA NA ## growthformHerb:FamilyMalvaceae NA NA NA ## growthformHerb/Shrub:FamilyMalvaceae NA NA NA ## growthformShrub:FamilyMalvaceae NA NA NA ## growthformShrub/Tree:FamilyMalvaceae NA NA NA ## growthformTree:FamilyMalvaceae NA NA NA ## growthformHerb:FamilyMelastomataceae NA NA NA ## growthformHerb/Shrub:FamilyMelastomataceae NA NA NA ## growthformShrub:FamilyMelastomataceae NA NA NA ## growthformShrub/Tree:FamilyMelastomataceae NA NA NA ## growthformTree:FamilyMelastomataceae NA NA NA ## growthformHerb:FamilyMoraceae NA NA NA ## growthformHerb/Shrub:FamilyMoraceae NA NA NA ## growthformShrub:FamilyMoraceae NA NA NA ## growthformShrub/Tree:FamilyMoraceae NA NA NA ## growthformTree:FamilyMoraceae NA NA NA ## growthformHerb:FamilyMyristicaceae NA NA NA ## growthformHerb/Shrub:FamilyMyristicaceae NA NA NA ## growthformShrub:FamilyMyristicaceae NA NA NA ## growthformShrub/Tree:FamilyMyristicaceae NA NA NA ## growthformTree:FamilyMyristicaceae NA NA NA ## growthformHerb:FamilyMyrsinaceae NA NA NA ## growthformHerb/Shrub:FamilyMyrsinaceae NA NA NA ## growthformShrub:FamilyMyrsinaceae NA NA NA ## growthformShrub/Tree:FamilyMyrsinaceae 2.252e+02 2.829e+02 0.796 ## growthformTree:FamilyMyrsinaceae NA NA NA ## growthformHerb:FamilyMyrtaceae NA NA NA ## growthformHerb/Shrub:FamilyMyrtaceae NA NA NA ## growthformShrub:FamilyMyrtaceae 2.171e+02 2.710e+02 0.801 ## growthformShrub/Tree:FamilyMyrtaceae NA NA NA ## growthformTree:FamilyMyrtaceae NA NA NA ## growthformHerb:FamilyOchnaceae NA NA NA ## growthformHerb/Shrub:FamilyOchnaceae NA NA NA ## growthformShrub:FamilyOchnaceae NA NA NA ## growthformShrub/Tree:FamilyOchnaceae NA NA NA ## growthformTree:FamilyOchnaceae NA NA NA ## growthformHerb:FamilyOnagraceae NA NA NA ## growthformHerb/Shrub:FamilyOnagraceae NA NA NA ## growthformShrub:FamilyOnagraceae NA NA NA ## growthformShrub/Tree:FamilyOnagraceae NA NA NA ## growthformTree:FamilyOnagraceae NA NA NA ## growthformHerb:FamilyOrobanchaceae NA NA NA ## growthformHerb/Shrub:FamilyOrobanchaceae NA NA NA ## growthformShrub:FamilyOrobanchaceae NA NA NA ## growthformShrub/Tree:FamilyOrobanchaceae NA NA NA ## growthformTree:FamilyOrobanchaceae NA NA NA ## growthformHerb:FamilyPhyllanthaceae NA NA NA ## growthformHerb/Shrub:FamilyPhyllanthaceae NA NA NA ## growthformShrub:FamilyPhyllanthaceae NA NA NA ## growthformShrub/Tree:FamilyPhyllanthaceae NA NA NA ## growthformTree:FamilyPhyllanthaceae NA NA NA ## growthformHerb:FamilyPicrodendraceae NA NA NA ## growthformHerb/Shrub:FamilyPicrodendraceae NA NA NA ## growthformShrub:FamilyPicrodendraceae NA NA NA ## growthformShrub/Tree:FamilyPicrodendraceae NA NA NA ## growthformTree:FamilyPicrodendraceae NA NA NA ## growthformHerb:FamilyPinaceae NA NA NA ## growthformHerb/Shrub:FamilyPinaceae NA NA NA ## growthformShrub:FamilyPinaceae NA NA NA ## growthformShrub/Tree:FamilyPinaceae NA NA NA ## growthformTree:FamilyPinaceae NA NA NA ## growthformHerb:FamilyPoaceae -2.601e+02 3.246e+02 -0.801 ## growthformHerb/Shrub:FamilyPoaceae NA NA NA ## growthformShrub:FamilyPoaceae NA NA NA ## growthformShrub/Tree:FamilyPoaceae NA NA NA ## growthformTree:FamilyPoaceae NA NA NA ## growthformHerb:FamilyPolemoniaceae NA NA NA ## growthformHerb/Shrub:FamilyPolemoniaceae NA NA NA ## growthformShrub:FamilyPolemoniaceae NA NA NA ## growthformShrub/Tree:FamilyPolemoniaceae NA NA NA ## growthformTree:FamilyPolemoniaceae NA NA NA ## growthformHerb:FamilyPolygonaceae NA NA NA ## growthformHerb/Shrub:FamilyPolygonaceae NA NA NA ## growthformShrub:FamilyPolygonaceae NA NA NA ## growthformShrub/Tree:FamilyPolygonaceae NA NA NA ## growthformTree:FamilyPolygonaceae NA NA NA ## growthformHerb:FamilyProteaceae NA NA NA ## growthformHerb/Shrub:FamilyProteaceae NA NA NA ## growthformShrub:FamilyProteaceae 2.174e+02 2.714e+02 0.801 ## growthformShrub/Tree:FamilyProteaceae NA NA NA ## growthformTree:FamilyProteaceae NA NA NA ## growthformHerb:FamilyRanunculaceae NA NA NA ## growthformHerb/Shrub:FamilyRanunculaceae NA NA NA ## growthformShrub:FamilyRanunculaceae NA NA NA ## growthformShrub/Tree:FamilyRanunculaceae NA NA NA ## growthformTree:FamilyRanunculaceae NA NA NA ## growthformHerb:FamilyRhamnaceae NA NA NA ## growthformHerb/Shrub:FamilyRhamnaceae NA NA NA ## growthformShrub:FamilyRhamnaceae NA NA NA ## growthformShrub/Tree:FamilyRhamnaceae NA NA NA ## growthformTree:FamilyRhamnaceae NA NA NA ## growthformHerb:FamilyRosaceae 3.723e+02 4.458e+02 0.835 ## growthformHerb/Shrub:FamilyRosaceae NA NA NA ## growthformShrub:FamilyRosaceae 2.126e+02 2.715e+02 0.783 ## growthformShrub/Tree:FamilyRosaceae NA NA NA ## growthformTree:FamilyRosaceae NA NA NA ## growthformHerb:FamilyRubiaceae NA NA NA ## growthformHerb/Shrub:FamilyRubiaceae NA NA NA ## growthformShrub:FamilyRubiaceae 6.243e+00 3.379e+00 1.848 ## growthformShrub/Tree:FamilyRubiaceae NA NA NA ## growthformTree:FamilyRubiaceae NA NA NA ## growthformHerb:FamilyRutaceae NA NA NA ## growthformHerb/Shrub:FamilyRutaceae NA NA NA ## growthformShrub:FamilyRutaceae NA NA NA ## growthformShrub/Tree:FamilyRutaceae NA NA NA ## growthformTree:FamilyRutaceae NA NA NA ## growthformHerb:FamilySalicaceae NA NA NA ## growthformHerb/Shrub:FamilySalicaceae NA NA NA ## growthformShrub:FamilySalicaceae 9.252e+00 6.665e+00 1.388 ## growthformShrub/Tree:FamilySalicaceae NA NA NA ## growthformTree:FamilySalicaceae NA NA NA ## growthformHerb:FamilySapindaceae NA NA NA ## growthformHerb/Shrub:FamilySapindaceae NA NA NA ## growthformShrub:FamilySapindaceae NA NA NA ## growthformShrub/Tree:FamilySapindaceae NA NA NA ## growthformTree:FamilySapindaceae NA NA NA ## growthformHerb:FamilySapotaceae NA NA NA ## growthformHerb/Shrub:FamilySapotaceae NA NA NA ## growthformShrub:FamilySapotaceae NA NA NA ## growthformShrub/Tree:FamilySapotaceae NA NA NA ## growthformTree:FamilySapotaceae NA NA NA ## growthformHerb:FamilyScrophulariaceae NA NA NA ## growthformHerb/Shrub:FamilyScrophulariaceae NA NA NA ## growthformShrub:FamilyScrophulariaceae NA NA NA ## growthformShrub/Tree:FamilyScrophulariaceae NA NA NA ## growthformTree:FamilyScrophulariaceae NA NA NA ## growthformHerb:FamilyThymelaeaceae NA NA NA ## growthformHerb/Shrub:FamilyThymelaeaceae NA NA NA ## growthformShrub:FamilyThymelaeaceae NA NA NA ## growthformShrub/Tree:FamilyThymelaeaceae NA NA NA ## growthformTree:FamilyThymelaeaceae NA NA NA ## growthformHerb:FamilyUlmaceae NA NA NA ## growthformHerb/Shrub:FamilyUlmaceae NA NA NA ## growthformShrub:FamilyUlmaceae NA NA NA ## growthformShrub/Tree:FamilyUlmaceae NA NA NA ## growthformTree:FamilyUlmaceae NA NA NA ## growthformHerb:FamilyUrticaceae NA NA NA ## growthformHerb/Shrub:FamilyUrticaceae NA NA NA ## growthformShrub:FamilyUrticaceae NA NA NA ## growthformShrub/Tree:FamilyUrticaceae NA NA NA ## growthformTree:FamilyUrticaceae NA NA NA ## growthformHerb:FamilyViolaceae NA NA NA ## growthformHerb/Shrub:FamilyViolaceae NA NA NA ## growthformShrub:FamilyViolaceae NA NA NA ## growthformShrub/Tree:FamilyViolaceae NA NA NA ## growthformTree:FamilyViolaceae NA NA NA ## growthformHerb:FamilyXanthorrhoeaceae NA NA NA ## growthformHerb/Shrub:FamilyXanthorrhoeaceae NA NA NA ## growthformShrub:FamilyXanthorrhoeaceae NA NA NA ## growthformShrub/Tree:FamilyXanthorrhoeaceae NA NA NA ## growthformTree:FamilyXanthorrhoeaceae NA NA NA ## growthformHerb:FamilyZygophyllaceae NA NA NA ## growthformHerb/Shrub:FamilyZygophyllaceae NA NA NA ## growthformShrub:FamilyZygophyllaceae NA NA NA ## growthformShrub/Tree:FamilyZygophyllaceae NA NA NA ## growthformTree:FamilyZygophyllaceae NA NA NA ## growthformHerb:lat -7.667e+00 9.125e+00 -0.840 ## growthformHerb/Shrub:lat NA NA NA ## growthformShrub:lat 7.514e-02 3.299e-02 2.278 ## growthformShrub/Tree:lat -4.602e+00 4.806e+00 -0.957 ## growthformTree:lat NA NA NA ## growthformHerb:long 3.209e-02 3.417e-02 0.939 ## growthformHerb/Shrub:long NA NA NA ## growthformShrub:long -6.040e-03 1.358e-02 -0.445 ## growthformShrub/Tree:long NA NA NA ## growthformTree:long NA NA NA ## growthformHerb:NPP 4.933e-04 4.811e-04 1.025 ## growthformHerb/Shrub:NPP NA NA NA ## growthformShrub:NPP 6.473e-04 3.846e-04 1.683 ## growthformShrub/Tree:NPP NA NA NA ## growthformTree:NPP NA NA NA ## FamilyAsteraceae:lat 9.729e+00 8.577e+00 1.134 ## FamilyAtherospermataceae:lat NA NA NA ## FamilyBalsaminaceae:lat NA NA NA ## FamilyBetulaceae:lat -5.249e-01 1.875e+00 -0.280 ## FamilyBrassicaceae:lat 6.681e+00 9.160e+00 0.729 ## FamilyCactaceae:lat -1.966e+00 8.672e-01 -2.267 ## FamilyCasuarinaceae:lat NA NA NA ## FamilyChloranthaceae:lat NA NA NA ## FamilyChrysobalanaceae:lat -7.988e-01 1.363e+00 -0.586 ## FamilyCistaceae:lat NA NA NA ## FamilyCornaceae:lat NA NA NA ## FamilyCrassulaceae:lat NA NA NA ## FamilyCunoniaceae:lat NA NA NA ## FamilyCupressaceae:lat NA NA NA ## FamilyCyperaceae:lat NA NA NA ## FamilyDennstaedtiaceae:lat NA NA NA ## FamilyDicksoniaceae:lat NA NA NA ## FamilyDipterocarpaceae:lat NA NA NA ## FamilyEbenaceae:lat -2.703e-01 3.089e-01 -0.875 ## FamilyElaeocarpaceae:lat NA NA NA ## FamilyEricaceae:lat -8.632e-01 6.646e-01 -1.299 ## FamilyEuphorbiaceae:lat -7.697e-01 7.758e-01 -0.992 ## FamilyFabaceae - C:lat -6.439e-01 7.319e-01 -0.880 ## FamilyFabaceae - M:lat NA NA NA ## FamilyFabaceae - P:lat -2.849e+00 2.838e+00 -1.004 ## FamilyFagaceae:lat -6.391e-01 6.767e-01 -0.944 ## FamilyGentianaceae:lat 7.953e+00 9.600e+00 0.828 ## FamilyHeliconiaceae:lat NA NA NA ## FamilyJuglandaceae:lat NA NA NA ## FamilyJuncaginaceae:lat NA NA NA ## FamilyLamiaceae:lat NA NA NA ## FamilyLauraceae:lat NA NA NA ## FamilyMaesaceae:lat NA NA NA ## FamilyMalvaceae:lat -5.759e-01 7.352e-01 -0.783 ## FamilyMelastomataceae:lat NA NA NA ## FamilyMoraceae:lat NA NA NA ## FamilyMyristicaceae:lat NA NA NA ## FamilyMyrsinaceae:lat NA NA NA ## FamilyMyrtaceae:lat -2.630e-01 6.091e-01 -0.432 ## FamilyOchnaceae:lat NA NA NA ## FamilyOnagraceae:lat NA NA NA ## FamilyOrobanchaceae:lat 6.419e+00 8.683e+00 0.739 ## FamilyPhyllanthaceae:lat NA NA NA ## FamilyPicrodendraceae:lat NA NA NA ## FamilyPinaceae:lat -1.017e+00 6.698e-01 -1.518 ## FamilyPoaceae:lat 7.161e+00 8.523e+00 0.840 ## FamilyPolemoniaceae:lat 6.911e+00 8.461e+00 0.817 ## FamilyPolygonaceae:lat 6.776e+00 8.505e+00 0.797 ## FamilyProteaceae:lat -1.078e+00 6.906e-01 -1.560 ## FamilyRanunculaceae:lat NA NA NA ## FamilyRhamnaceae:lat NA NA NA ## FamilyRosaceae:lat -8.875e-01 5.851e-01 -1.517 ## FamilyRubiaceae:lat -1.008e+00 7.723e-01 -1.305 ## FamilyRutaceae:lat NA NA NA ## FamilySalicaceae:lat -1.327e+00 1.062e+00 -1.249 ## FamilySapindaceae:lat -7.032e-01 7.657e-01 -0.918 ## FamilySapotaceae:lat NA NA NA ## FamilyScrophulariaceae:lat NA NA NA ## FamilyThymelaeaceae:lat NA NA NA ## FamilyUlmaceae:lat -1.490e+00 1.311e+00 -1.137 ## FamilyUrticaceae:lat -1.170e+00 1.217e+00 -0.962 ## FamilyViolaceae:lat NA NA NA ## FamilyXanthorrhoeaceae:lat NA NA NA ## FamilyZygophyllaceae:lat NA NA NA ## FamilyAsteraceae:long 3.953e-02 8.917e-03 4.433 ## FamilyAtherospermataceae:long NA NA NA ## FamilyBalsaminaceae:long NA NA NA ## FamilyBetulaceae:long 1.284e+00 1.382e+00 0.929 ## FamilyBrassicaceae:long NA NA NA ## FamilyCactaceae:long 3.726e-02 2.427e-02 1.535 ## FamilyCasuarinaceae:long NA NA NA ## FamilyChloranthaceae:long NA NA NA ## FamilyChrysobalanaceae:long NA NA NA ## FamilyCistaceae:long NA NA NA ## FamilyCornaceae:long NA NA NA ## FamilyCrassulaceae:long NA NA NA ## FamilyCunoniaceae:long NA NA NA ## FamilyCupressaceae:long NA NA NA ## FamilyCyperaceae:long NA NA NA ## FamilyDennstaedtiaceae:long NA NA NA ## FamilyDicksoniaceae:long NA NA NA ## FamilyDipterocarpaceae:long NA NA NA ## FamilyEbenaceae:long NA NA NA ## FamilyElaeocarpaceae:long NA NA NA ## FamilyEricaceae:long 6.409e-02 2.972e-02 2.156 ## FamilyEuphorbiaceae:long 4.371e-02 4.554e-02 0.960 ## FamilyFabaceae - C:long NA NA NA ## FamilyFabaceae - M:long NA NA NA ## FamilyFabaceae - P:long NA NA NA ## FamilyFagaceae:long NA NA NA ## FamilyGentianaceae:long NA NA NA ## FamilyHeliconiaceae:long NA NA NA ## FamilyJuglandaceae:long NA NA NA ## FamilyJuncaginaceae:long NA NA NA ## FamilyLamiaceae:long NA NA NA ## FamilyLauraceae:long NA NA NA ## FamilyMaesaceae:long NA NA NA ## FamilyMalvaceae:long 3.701e-02 4.705e-02 0.787 ## FamilyMelastomataceae:long NA NA NA ## FamilyMoraceae:long NA NA NA ## FamilyMyristicaceae:long NA NA NA ## FamilyMyrsinaceae:long NA NA NA ## FamilyMyrtaceae:long 5.316e-02 3.216e-02 1.653 ## FamilyOchnaceae:long NA NA NA ## FamilyOnagraceae:long NA NA NA ## FamilyOrobanchaceae:long NA NA NA ## FamilyPhyllanthaceae:long NA NA NA ## FamilyPicrodendraceae:long NA NA NA ## FamilyPinaceae:long 1.006e-03 3.483e-02 0.029 ## FamilyPoaceae:long NA NA NA ## FamilyPolemoniaceae:long NA NA NA ## FamilyPolygonaceae:long NA NA NA ## FamilyProteaceae:long 3.544e-02 2.338e-02 1.516 ## FamilyRanunculaceae:long NA NA NA ## FamilyRhamnaceae:long NA NA NA ## FamilyRosaceae:long 6.083e-02 5.503e-02 1.105 ## FamilyRubiaceae:long NA NA NA ## FamilyRutaceae:long NA NA NA ## FamilySalicaceae:long NA NA NA ## FamilySapindaceae:long 4.512e-02 2.639e-02 1.710 ## FamilySapotaceae:long NA NA NA ## FamilyScrophulariaceae:long NA NA NA ## FamilyThymelaeaceae:long NA NA NA ## FamilyUlmaceae:long NA NA NA ## FamilyUrticaceae:long NA NA NA ## FamilyViolaceae:long NA NA NA ## FamilyXanthorrhoeaceae:long NA NA NA ## FamilyZygophyllaceae:long NA NA NA ## FamilyAsteraceae:alt 2.977e-02 1.249e-02 2.384 ## FamilyAtherospermataceae:alt NA NA NA ## FamilyBalsaminaceae:alt NA NA NA ## FamilyBetulaceae:alt NA NA NA ## FamilyBrassicaceae:alt NA NA NA ## FamilyCactaceae:alt -4.211e-03 9.104e-03 -0.463 ## FamilyCasuarinaceae:alt NA NA NA ## FamilyChloranthaceae:alt NA NA NA ## FamilyChrysobalanaceae:alt NA NA NA ## FamilyCistaceae:alt NA NA NA ## FamilyCornaceae:alt NA NA NA ## FamilyCrassulaceae:alt NA NA NA ## FamilyCunoniaceae:alt NA NA NA ## FamilyCupressaceae:alt NA NA NA ## FamilyCyperaceae:alt NA NA NA ## FamilyDennstaedtiaceae:alt NA NA NA ## FamilyDicksoniaceae:alt NA NA NA ## FamilyDipterocarpaceae:alt NA NA NA ## FamilyEbenaceae:alt NA NA NA ## FamilyElaeocarpaceae:alt NA NA NA ## FamilyEricaceae:alt 2.057e-03 9.530e-03 0.216 ## FamilyEuphorbiaceae:alt NA NA NA ## FamilyFabaceae - C:alt NA NA NA ## FamilyFabaceae - M:alt NA NA NA ## FamilyFabaceae - P:alt NA NA NA ## FamilyFagaceae:alt NA NA NA ## FamilyGentianaceae:alt NA NA NA ## FamilyHeliconiaceae:alt NA NA NA ## FamilyJuglandaceae:alt NA NA NA ## FamilyJuncaginaceae:alt NA NA NA ## FamilyLamiaceae:alt NA NA NA ## FamilyLauraceae:alt NA NA NA ## FamilyMaesaceae:alt NA NA NA ## FamilyMalvaceae:alt NA NA NA ## FamilyMelastomataceae:alt NA NA NA ## FamilyMoraceae:alt NA NA NA ## FamilyMyristicaceae:alt NA NA NA ## FamilyMyrsinaceae:alt NA NA NA ## FamilyMyrtaceae:alt -2.001e-03 8.486e-03 -0.236 ## FamilyOchnaceae:alt NA NA NA ## FamilyOnagraceae:alt NA NA NA ## FamilyOrobanchaceae:alt NA NA NA ## FamilyPhyllanthaceae:alt NA NA NA ## FamilyPicrodendraceae:alt NA NA NA ## FamilyPinaceae:alt -7.229e-03 8.920e-03 -0.810 ## FamilyPoaceae:alt -3.846e-03 8.857e-03 -0.434 ## FamilyPolemoniaceae:alt NA NA NA ## FamilyPolygonaceae:alt NA NA NA ## FamilyProteaceae:alt -8.882e-03 8.984e-03 -0.989 ## FamilyRanunculaceae:alt NA NA NA ## FamilyRhamnaceae:alt NA NA NA ## FamilyRosaceae:alt NA NA NA ## FamilyRubiaceae:alt NA NA NA ## FamilyRutaceae:alt NA NA NA ## FamilySalicaceae:alt NA NA NA ## FamilySapindaceae:alt NA NA NA ## FamilySapotaceae:alt NA NA NA ## FamilyScrophulariaceae:alt NA NA NA ## FamilyThymelaeaceae:alt NA NA NA ## FamilyUlmaceae:alt NA NA NA ## FamilyUrticaceae:alt NA NA NA ## FamilyViolaceae:alt NA NA NA ## FamilyXanthorrhoeaceae:alt NA NA NA ## FamilyZygophyllaceae:alt NA NA NA ## FamilyAsteraceae:temp 9.741e+00 2.649e+00 3.678 ## FamilyAtherospermataceae:temp NA NA NA ## FamilyBalsaminaceae:temp NA NA NA ## FamilyBetulaceae:temp NA NA NA ## FamilyBrassicaceae:temp NA NA NA ## FamilyCactaceae:temp NA NA NA ## FamilyCasuarinaceae:temp NA NA NA ## FamilyChloranthaceae:temp NA NA NA ## FamilyChrysobalanaceae:temp NA NA NA ## FamilyCistaceae:temp NA NA NA ## FamilyCornaceae:temp NA NA NA ## FamilyCrassulaceae:temp NA NA NA ## FamilyCunoniaceae:temp NA NA NA ## FamilyCupressaceae:temp NA NA NA ## FamilyCyperaceae:temp NA NA NA ## FamilyDennstaedtiaceae:temp NA NA NA ## FamilyDicksoniaceae:temp NA NA NA ## FamilyDipterocarpaceae:temp NA NA NA ## FamilyEbenaceae:temp NA NA NA ## FamilyElaeocarpaceae:temp NA NA NA ## FamilyEricaceae:temp NA NA NA ## FamilyEuphorbiaceae:temp NA NA NA ## FamilyFabaceae - C:temp NA NA NA ## FamilyFabaceae - M:temp NA NA NA ## FamilyFabaceae - P:temp NA NA NA ## FamilyFagaceae:temp NA NA NA ## FamilyGentianaceae:temp NA NA NA ## FamilyHeliconiaceae:temp NA NA NA ## FamilyJuglandaceae:temp NA NA NA ## FamilyJuncaginaceae:temp NA NA NA ## FamilyLamiaceae:temp NA NA NA ## FamilyLauraceae:temp NA NA NA ## FamilyMaesaceae:temp NA NA NA ## FamilyMalvaceae:temp NA NA NA ## FamilyMelastomataceae:temp NA NA NA ## FamilyMoraceae:temp NA NA NA ## FamilyMyristicaceae:temp NA NA NA ## FamilyMyrsinaceae:temp NA NA NA ## FamilyMyrtaceae:temp 1.113e+00 3.408e-01 3.266 ## FamilyOchnaceae:temp NA NA NA ## FamilyOnagraceae:temp NA NA NA ## FamilyOrobanchaceae:temp NA NA NA ## FamilyPhyllanthaceae:temp NA NA NA ## FamilyPicrodendraceae:temp NA NA NA ## FamilyPinaceae:temp NA NA NA ## FamilyPoaceae:temp 6.425e-01 2.770e-01 2.320 ## FamilyPolemoniaceae:temp NA NA NA ## FamilyPolygonaceae:temp NA NA NA ## FamilyProteaceae:temp NA NA NA ## FamilyRanunculaceae:temp NA NA NA ## FamilyRhamnaceae:temp NA NA NA ## FamilyRosaceae:temp NA NA NA ## FamilyRubiaceae:temp NA NA NA ## FamilyRutaceae:temp NA NA NA ## FamilySalicaceae:temp NA NA NA ## FamilySapindaceae:temp NA NA NA ## FamilySapotaceae:temp NA NA NA ## FamilyScrophulariaceae:temp NA NA NA ## FamilyThymelaeaceae:temp NA NA NA ## FamilyUlmaceae:temp NA NA NA ## FamilyUrticaceae:temp NA NA NA ## FamilyViolaceae:temp NA NA NA ## FamilyXanthorrhoeaceae:temp NA NA NA ## FamilyZygophyllaceae:temp NA NA NA ## lat:long -4.850e-04 3.239e-04 -1.497 ## lat:temp -5.288e-03 1.011e-03 -5.231 ## lat:NPP -4.384e-05 4.734e-05 -0.926 ## long:alt -1.635e-06 2.683e-06 -0.609 ## long:temp -1.650e-03 5.755e-04 -2.868 ## long:NPP 7.363e-06 2.523e-06 2.918 ## alt:temp -5.835e-05 1.708e-05 -3.416 ## temp:NPP -1.631e-04 8.021e-05 -2.034 ## Pr(&gt;|t|) ## (Intercept) 0.471210 ## growthformHerb 0.401018 ## growthformHerb/Shrub 0.959364 ## growthformShrub 0.459650 ## growthformShrub/Tree 0.576973 ## growthformTree 0.209409 ## FamilyAsteraceae 0.024915 * ## FamilyAtherospermataceae 0.240297 ## FamilyBalsaminaceae 0.445806 ## FamilyBetulaceae 0.781378 ## FamilyBrassicaceae 0.520831 ## FamilyCactaceae 0.374789 ## FamilyCasuarinaceae 0.457542 ## FamilyChloranthaceae 0.451076 ## FamilyChrysobalanaceae 0.531209 ## FamilyCistaceae 0.457466 ## FamilyCornaceae 0.444251 ## FamilyCrassulaceae 0.461491 ## FamilyCunoniaceae 0.580889 ## FamilyCupressaceae 0.306038 ## FamilyCyperaceae 0.397873 ## FamilyDennstaedtiaceae NA ## FamilyDicksoniaceae 0.468095 ## FamilyDipterocarpaceae 0.398966 ## FamilyEbenaceae 0.397522 ## FamilyElaeocarpaceae 0.323838 ## FamilyEricaceae 0.451810 ## FamilyEuphorbiaceae 0.438290 ## FamilyFabaceae - C 0.453083 ## FamilyFabaceae - M 0.244394 ## FamilyFabaceae - P 0.340876 ## FamilyFagaceae 0.799425 ## FamilyGentianaceae 0.406314 ## FamilyHeliconiaceae 0.418080 ## FamilyJuglandaceae 0.307939 ## FamilyJuncaginaceae 0.445752 ## FamilyLamiaceae 0.474074 ## FamilyLauraceae 0.548932 ## FamilyMaesaceae 0.447295 ## FamilyMalvaceae 0.537119 ## FamilyMelastomataceae 0.447635 ## FamilyMoraceae 0.380649 ## FamilyMyristicaceae 0.349574 ## FamilyMyrsinaceae 0.393440 ## FamilyMyrtaceae 0.011832 * ## FamilyOchnaceae 0.448174 ## FamilyOnagraceae NA ## FamilyOrobanchaceae 0.553364 ## FamilyPhyllanthaceae 0.393311 ## FamilyPicrodendraceae 0.359305 ## FamilyPinaceae 0.087314 . ## FamilyPoaceae 0.529061 ## FamilyPolemoniaceae 0.415849 ## FamilyPolygonaceae 0.444282 ## FamilyProteaceae 0.106800 ## FamilyRanunculaceae 0.447850 ## FamilyRhamnaceae 0.466950 ## FamilyRosaceae 0.016301 * ## FamilyRubiaceae 0.441496 ## FamilyRutaceae 0.456598 ## FamilySalicaceae 0.433258 ## FamilySapindaceae 0.607885 ## FamilySapotaceae 0.276654 ## FamilyScrophulariaceae 0.448464 ## FamilyThymelaeaceae 0.336091 ## FamilyUlmaceae 0.231765 ## FamilyUrticaceae 0.338942 ## FamilyViolaceae 0.447326 ## FamilyXanthorrhoeaceae 0.455414 ## FamilyZygophyllaceae 0.467051 ## lat 0.325004 ## long 0.860913 ## alt 0.550965 ## temp 0.447922 ## NPP 0.298512 ## growthformHerb:FamilyAsteraceae 0.761650 ## growthformHerb/Shrub:FamilyAsteraceae NA ## growthformShrub:FamilyAsteraceae NA ## growthformShrub/Tree:FamilyAsteraceae NA ## growthformTree:FamilyAsteraceae NA ## growthformHerb:FamilyAtherospermataceae NA ## growthformHerb/Shrub:FamilyAtherospermataceae NA ## growthformShrub:FamilyAtherospermataceae NA ## growthformShrub/Tree:FamilyAtherospermataceae NA ## growthformTree:FamilyAtherospermataceae NA ## growthformHerb:FamilyBalsaminaceae NA ## growthformHerb/Shrub:FamilyBalsaminaceae NA ## growthformShrub:FamilyBalsaminaceae NA ## growthformShrub/Tree:FamilyBalsaminaceae NA ## growthformTree:FamilyBalsaminaceae NA ## growthformHerb:FamilyBetulaceae NA ## growthformHerb/Shrub:FamilyBetulaceae NA ## growthformShrub:FamilyBetulaceae 0.483172 ## growthformShrub/Tree:FamilyBetulaceae NA ## growthformTree:FamilyBetulaceae NA ## growthformHerb:FamilyBrassicaceae NA ## growthformHerb/Shrub:FamilyBrassicaceae NA ## growthformShrub:FamilyBrassicaceae NA ## growthformShrub/Tree:FamilyBrassicaceae NA ## growthformTree:FamilyBrassicaceae NA ## growthformHerb:FamilyCactaceae NA ## growthformHerb/Shrub:FamilyCactaceae NA ## growthformShrub:FamilyCactaceae NA ## growthformShrub/Tree:FamilyCactaceae NA ## growthformTree:FamilyCactaceae NA ## growthformHerb:FamilyCasuarinaceae NA ## growthformHerb/Shrub:FamilyCasuarinaceae NA ## growthformShrub:FamilyCasuarinaceae NA ## growthformShrub/Tree:FamilyCasuarinaceae NA ## growthformTree:FamilyCasuarinaceae NA ## growthformHerb:FamilyChloranthaceae NA ## growthformHerb/Shrub:FamilyChloranthaceae NA ## growthformShrub:FamilyChloranthaceae NA ## growthformShrub/Tree:FamilyChloranthaceae NA ## growthformTree:FamilyChloranthaceae NA ## growthformHerb:FamilyChrysobalanaceae NA ## growthformHerb/Shrub:FamilyChrysobalanaceae NA ## growthformShrub:FamilyChrysobalanaceae NA ## growthformShrub/Tree:FamilyChrysobalanaceae NA ## growthformTree:FamilyChrysobalanaceae NA ## growthformHerb:FamilyCistaceae NA ## growthformHerb/Shrub:FamilyCistaceae NA ## growthformShrub:FamilyCistaceae NA ## growthformShrub/Tree:FamilyCistaceae NA ## growthformTree:FamilyCistaceae NA ## growthformHerb:FamilyCornaceae NA ## growthformHerb/Shrub:FamilyCornaceae NA ## growthformShrub:FamilyCornaceae NA ## growthformShrub/Tree:FamilyCornaceae NA ## growthformTree:FamilyCornaceae NA ## growthformHerb:FamilyCrassulaceae NA ## growthformHerb/Shrub:FamilyCrassulaceae NA ## growthformShrub:FamilyCrassulaceae NA ## growthformShrub/Tree:FamilyCrassulaceae NA ## growthformTree:FamilyCrassulaceae NA ## growthformHerb:FamilyCunoniaceae NA ## growthformHerb/Shrub:FamilyCunoniaceae NA ## growthformShrub:FamilyCunoniaceae NA ## growthformShrub/Tree:FamilyCunoniaceae 0.448590 ## growthformTree:FamilyCunoniaceae NA ## growthformHerb:FamilyCupressaceae NA ## growthformHerb/Shrub:FamilyCupressaceae NA ## growthformShrub:FamilyCupressaceae 0.447817 ## growthformShrub/Tree:FamilyCupressaceae NA ## growthformTree:FamilyCupressaceae NA ## growthformHerb:FamilyCyperaceae NA ## growthformHerb/Shrub:FamilyCyperaceae NA ## growthformShrub:FamilyCyperaceae NA ## growthformShrub/Tree:FamilyCyperaceae NA ## growthformTree:FamilyCyperaceae NA ## growthformHerb:FamilyDennstaedtiaceae NA ## growthformHerb/Shrub:FamilyDennstaedtiaceae NA ## growthformShrub:FamilyDennstaedtiaceae NA ## growthformShrub/Tree:FamilyDennstaedtiaceae NA ## growthformTree:FamilyDennstaedtiaceae NA ## growthformHerb:FamilyDicksoniaceae NA ## growthformHerb/Shrub:FamilyDicksoniaceae NA ## growthformShrub:FamilyDicksoniaceae NA ## growthformShrub/Tree:FamilyDicksoniaceae NA ## growthformTree:FamilyDicksoniaceae NA ## growthformHerb:FamilyDipterocarpaceae NA ## growthformHerb/Shrub:FamilyDipterocarpaceae NA ## growthformShrub:FamilyDipterocarpaceae NA ## growthformShrub/Tree:FamilyDipterocarpaceae NA ## growthformTree:FamilyDipterocarpaceae NA ## growthformHerb:FamilyEbenaceae NA ## growthformHerb/Shrub:FamilyEbenaceae NA ## growthformShrub:FamilyEbenaceae NA ## growthformShrub/Tree:FamilyEbenaceae NA ## growthformTree:FamilyEbenaceae NA ## growthformHerb:FamilyElaeocarpaceae NA ## growthformHerb/Shrub:FamilyElaeocarpaceae NA ## growthformShrub:FamilyElaeocarpaceae NA ## growthformShrub/Tree:FamilyElaeocarpaceae NA ## growthformTree:FamilyElaeocarpaceae NA ## growthformHerb:FamilyEricaceae NA ## growthformHerb/Shrub:FamilyEricaceae NA ## growthformShrub:FamilyEricaceae 0.724294 ## growthformShrub/Tree:FamilyEricaceae NA ## growthformTree:FamilyEricaceae NA ## growthformHerb:FamilyEuphorbiaceae NA ## growthformHerb/Shrub:FamilyEuphorbiaceae NA ## growthformShrub:FamilyEuphorbiaceae 0.444658 ## growthformShrub/Tree:FamilyEuphorbiaceae NA ## growthformTree:FamilyEuphorbiaceae NA ## growthformHerb:FamilyFabaceae - C NA ## growthformHerb/Shrub:FamilyFabaceae - C NA ## growthformShrub:FamilyFabaceae - C NA ## growthformShrub/Tree:FamilyFabaceae - C NA ## growthformTree:FamilyFabaceae - C NA ## growthformHerb:FamilyFabaceae - M NA ## growthformHerb/Shrub:FamilyFabaceae - M NA ## growthformShrub:FamilyFabaceae - M NA ## growthformShrub/Tree:FamilyFabaceae - M 0.432247 ## growthformTree:FamilyFabaceae - M NA ## growthformHerb:FamilyFabaceae - P 0.412368 ## growthformHerb/Shrub:FamilyFabaceae - P NA ## growthformShrub:FamilyFabaceae - P 0.434788 ## growthformShrub/Tree:FamilyFabaceae - P NA ## growthformTree:FamilyFabaceae - P NA ## growthformHerb:FamilyFagaceae NA ## growthformHerb/Shrub:FamilyFagaceae NA ## growthformShrub:FamilyFagaceae 0.451214 ## growthformShrub/Tree:FamilyFagaceae NA ## growthformTree:FamilyFagaceae NA ## growthformHerb:FamilyGentianaceae NA ## growthformHerb/Shrub:FamilyGentianaceae NA ## growthformShrub:FamilyGentianaceae NA ## growthformShrub/Tree:FamilyGentianaceae NA ## growthformTree:FamilyGentianaceae NA ## growthformHerb:FamilyHeliconiaceae NA ## growthformHerb/Shrub:FamilyHeliconiaceae NA ## growthformShrub:FamilyHeliconiaceae NA ## growthformShrub/Tree:FamilyHeliconiaceae NA ## growthformTree:FamilyHeliconiaceae NA ## growthformHerb:FamilyJuglandaceae NA ## growthformHerb/Shrub:FamilyJuglandaceae NA ## growthformShrub:FamilyJuglandaceae NA ## growthformShrub/Tree:FamilyJuglandaceae NA ## growthformTree:FamilyJuglandaceae NA ## growthformHerb:FamilyJuncaginaceae NA ## growthformHerb/Shrub:FamilyJuncaginaceae NA ## growthformShrub:FamilyJuncaginaceae NA ## growthformShrub/Tree:FamilyJuncaginaceae NA ## growthformTree:FamilyJuncaginaceae NA ## growthformHerb:FamilyLamiaceae NA ## growthformHerb/Shrub:FamilyLamiaceae NA ## growthformShrub:FamilyLamiaceae NA ## growthformShrub/Tree:FamilyLamiaceae NA ## growthformTree:FamilyLamiaceae NA ## growthformHerb:FamilyLauraceae NA ## growthformHerb/Shrub:FamilyLauraceae NA ## growthformShrub:FamilyLauraceae NA ## growthformShrub/Tree:FamilyLauraceae NA ## growthformTree:FamilyLauraceae NA ## growthformHerb:FamilyMaesaceae NA ## growthformHerb/Shrub:FamilyMaesaceae NA ## growthformShrub:FamilyMaesaceae NA ## growthformShrub/Tree:FamilyMaesaceae NA ## growthformTree:FamilyMaesaceae NA ## growthformHerb:FamilyMalvaceae NA ## growthformHerb/Shrub:FamilyMalvaceae NA ## growthformShrub:FamilyMalvaceae NA ## growthformShrub/Tree:FamilyMalvaceae NA ## growthformTree:FamilyMalvaceae NA ## growthformHerb:FamilyMelastomataceae NA ## growthformHerb/Shrub:FamilyMelastomataceae NA ## growthformShrub:FamilyMelastomataceae NA ## growthformShrub/Tree:FamilyMelastomataceae NA ## growthformTree:FamilyMelastomataceae NA ## growthformHerb:FamilyMoraceae NA ## growthformHerb/Shrub:FamilyMoraceae NA ## growthformShrub:FamilyMoraceae NA ## growthformShrub/Tree:FamilyMoraceae NA ## growthformTree:FamilyMoraceae NA ## growthformHerb:FamilyMyristicaceae NA ## growthformHerb/Shrub:FamilyMyristicaceae NA ## growthformShrub:FamilyMyristicaceae NA ## growthformShrub/Tree:FamilyMyristicaceae NA ## growthformTree:FamilyMyristicaceae NA ## growthformHerb:FamilyMyrsinaceae NA ## growthformHerb/Shrub:FamilyMyrsinaceae NA ## growthformShrub:FamilyMyrsinaceae NA ## growthformShrub/Tree:FamilyMyrsinaceae 0.444600 ## growthformTree:FamilyMyrsinaceae NA ## growthformHerb:FamilyMyrtaceae NA ## growthformHerb/Shrub:FamilyMyrtaceae NA ## growthformShrub:FamilyMyrtaceae 0.441684 ## growthformShrub/Tree:FamilyMyrtaceae NA ## growthformTree:FamilyMyrtaceae NA ## growthformHerb:FamilyOchnaceae NA ## growthformHerb/Shrub:FamilyOchnaceae NA ## growthformShrub:FamilyOchnaceae NA ## growthformShrub/Tree:FamilyOchnaceae NA ## growthformTree:FamilyOchnaceae NA ## growthformHerb:FamilyOnagraceae NA ## growthformHerb/Shrub:FamilyOnagraceae NA ## growthformShrub:FamilyOnagraceae NA ## growthformShrub/Tree:FamilyOnagraceae NA ## growthformTree:FamilyOnagraceae NA ## growthformHerb:FamilyOrobanchaceae NA ## growthformHerb/Shrub:FamilyOrobanchaceae NA ## growthformShrub:FamilyOrobanchaceae NA ## growthformShrub/Tree:FamilyOrobanchaceae NA ## growthformTree:FamilyOrobanchaceae NA ## growthformHerb:FamilyPhyllanthaceae NA ## growthformHerb/Shrub:FamilyPhyllanthaceae NA ## growthformShrub:FamilyPhyllanthaceae NA ## growthformShrub/Tree:FamilyPhyllanthaceae NA ## growthformTree:FamilyPhyllanthaceae NA ## growthformHerb:FamilyPicrodendraceae NA ## growthformHerb/Shrub:FamilyPicrodendraceae NA ## growthformShrub:FamilyPicrodendraceae NA ## growthformShrub/Tree:FamilyPicrodendraceae NA ## growthformTree:FamilyPicrodendraceae NA ## growthformHerb:FamilyPinaceae NA ## growthformHerb/Shrub:FamilyPinaceae NA ## growthformShrub:FamilyPinaceae NA ## growthformShrub/Tree:FamilyPinaceae NA ## growthformTree:FamilyPinaceae NA ## growthformHerb:FamilyPoaceae 0.441587 ## growthformHerb/Shrub:FamilyPoaceae NA ## growthformShrub:FamilyPoaceae NA ## growthformShrub/Tree:FamilyPoaceae NA ## growthformTree:FamilyPoaceae NA ## growthformHerb:FamilyPolemoniaceae NA ## growthformHerb/Shrub:FamilyPolemoniaceae NA ## growthformShrub:FamilyPolemoniaceae NA ## growthformShrub/Tree:FamilyPolemoniaceae NA ## growthformTree:FamilyPolemoniaceae NA ## growthformHerb:FamilyPolygonaceae NA ## growthformHerb/Shrub:FamilyPolygonaceae NA ## growthformShrub:FamilyPolygonaceae NA ## growthformShrub/Tree:FamilyPolygonaceae NA ## growthformTree:FamilyPolygonaceae NA ## growthformHerb:FamilyProteaceae NA ## growthformHerb/Shrub:FamilyProteaceae NA ## growthformShrub:FamilyProteaceae 0.441738 ## growthformShrub/Tree:FamilyProteaceae NA ## growthformTree:FamilyProteaceae NA ## growthformHerb:FamilyRanunculaceae NA ## growthformHerb/Shrub:FamilyRanunculaceae NA ## growthformShrub:FamilyRanunculaceae NA ## growthformShrub/Tree:FamilyRanunculaceae NA ## growthformTree:FamilyRanunculaceae NA ## growthformHerb:FamilyRhamnaceae NA ## growthformHerb/Shrub:FamilyRhamnaceae NA ## growthformShrub:FamilyRhamnaceae NA ## growthformShrub/Tree:FamilyRhamnaceae NA ## growthformTree:FamilyRhamnaceae NA ## growthformHerb:FamilyRosaceae 0.423206 ## growthformHerb/Shrub:FamilyRosaceae NA ## growthformShrub:FamilyRosaceae 0.451569 ## growthformShrub/Tree:FamilyRosaceae NA ## growthformTree:FamilyRosaceae NA ## growthformHerb:FamilyRubiaceae NA ## growthformHerb/Shrub:FamilyRubiaceae NA ## growthformShrub:FamilyRubiaceae 0.094423 . ## growthformShrub/Tree:FamilyRubiaceae NA ## growthformTree:FamilyRubiaceae NA ## growthformHerb:FamilyRutaceae NA ## growthformHerb/Shrub:FamilyRutaceae NA ## growthformShrub:FamilyRutaceae NA ## growthformShrub/Tree:FamilyRutaceae NA ## growthformTree:FamilyRutaceae NA ## growthformHerb:FamilySalicaceae NA ## growthformHerb/Shrub:FamilySalicaceae NA ## growthformShrub:FamilySalicaceae 0.195221 ## growthformShrub/Tree:FamilySalicaceae NA ## growthformTree:FamilySalicaceae NA ## growthformHerb:FamilySapindaceae NA ## growthformHerb/Shrub:FamilySapindaceae NA ## growthformShrub:FamilySapindaceae NA ## growthformShrub/Tree:FamilySapindaceae NA ## growthformTree:FamilySapindaceae NA ## growthformHerb:FamilySapotaceae NA ## growthformHerb/Shrub:FamilySapotaceae NA ## growthformShrub:FamilySapotaceae NA ## growthformShrub/Tree:FamilySapotaceae NA ## growthformTree:FamilySapotaceae NA ## growthformHerb:FamilyScrophulariaceae NA ## growthformHerb/Shrub:FamilyScrophulariaceae NA ## growthformShrub:FamilyScrophulariaceae NA ## growthformShrub/Tree:FamilyScrophulariaceae NA ## growthformTree:FamilyScrophulariaceae NA ## growthformHerb:FamilyThymelaeaceae NA ## growthformHerb/Shrub:FamilyThymelaeaceae NA ## growthformShrub:FamilyThymelaeaceae NA ## growthformShrub/Tree:FamilyThymelaeaceae NA ## growthformTree:FamilyThymelaeaceae NA ## growthformHerb:FamilyUlmaceae NA ## growthformHerb/Shrub:FamilyUlmaceae NA ## growthformShrub:FamilyUlmaceae NA ## growthformShrub/Tree:FamilyUlmaceae NA ## growthformTree:FamilyUlmaceae NA ## growthformHerb:FamilyUrticaceae NA ## growthformHerb/Shrub:FamilyUrticaceae NA ## growthformShrub:FamilyUrticaceae NA ## growthformShrub/Tree:FamilyUrticaceae NA ## growthformTree:FamilyUrticaceae NA ## growthformHerb:FamilyViolaceae NA ## growthformHerb/Shrub:FamilyViolaceae NA ## growthformShrub:FamilyViolaceae NA ## growthformShrub/Tree:FamilyViolaceae NA ## growthformTree:FamilyViolaceae NA ## growthformHerb:FamilyXanthorrhoeaceae NA ## growthformHerb/Shrub:FamilyXanthorrhoeaceae NA ## growthformShrub:FamilyXanthorrhoeaceae NA ## growthformShrub/Tree:FamilyXanthorrhoeaceae NA ## growthformTree:FamilyXanthorrhoeaceae NA ## growthformHerb:FamilyZygophyllaceae NA ## growthformHerb/Shrub:FamilyZygophyllaceae NA ## growthformShrub:FamilyZygophyllaceae NA ## growthformShrub/Tree:FamilyZygophyllaceae NA ## growthformTree:FamilyZygophyllaceae NA ## growthformHerb:lat 0.420417 ## growthformHerb/Shrub:lat NA ## growthformShrub:lat 0.045980 * ## growthformShrub/Tree:lat 0.360912 ## growthformTree:lat NA ## growthformHerb:long 0.369731 ## growthformHerb/Shrub:long NA ## growthformShrub:long 0.665856 ## growthformShrub/Tree:long NA ## growthformTree:long NA ## growthformHerb:NPP 0.329320 ## growthformHerb/Shrub:NPP NA ## growthformShrub:NPP 0.123317 ## growthformShrub/Tree:NPP NA ## growthformTree:NPP NA ## FamilyAsteraceae:lat 0.283138 ## FamilyAtherospermataceae:lat NA ## FamilyBalsaminaceae:lat NA ## FamilyBetulaceae:lat 0.785252 ## FamilyBrassicaceae:lat 0.482497 ## FamilyCactaceae:lat 0.046821 * ## FamilyCasuarinaceae:lat NA ## FamilyChloranthaceae:lat NA ## FamilyChrysobalanaceae:lat 0.570866 ## FamilyCistaceae:lat NA ## FamilyCornaceae:lat NA ## FamilyCrassulaceae:lat NA ## FamilyCunoniaceae:lat NA ## FamilyCupressaceae:lat NA ## FamilyCyperaceae:lat NA ## FamilyDennstaedtiaceae:lat NA ## FamilyDicksoniaceae:lat NA ## FamilyDipterocarpaceae:lat NA ## FamilyEbenaceae:lat 0.402213 ## FamilyElaeocarpaceae:lat NA ## FamilyEricaceae:lat 0.223129 ## FamilyEuphorbiaceae:lat 0.344510 ## FamilyFabaceae - C:lat 0.399668 ## FamilyFabaceae - M:lat NA ## FamilyFabaceae - P:lat 0.339143 ## FamilyFagaceae:lat 0.367245 ## FamilyGentianaceae:lat 0.426756 ## FamilyHeliconiaceae:lat NA ## FamilyJuglandaceae:lat NA ## FamilyJuncaginaceae:lat NA ## FamilyLamiaceae:lat NA ## FamilyLauraceae:lat NA ## FamilyMaesaceae:lat NA ## FamilyMalvaceae:lat 0.451583 ## FamilyMelastomataceae:lat NA ## FamilyMoraceae:lat NA ## FamilyMyristicaceae:lat NA ## FamilyMyrsinaceae:lat NA ## FamilyMyrtaceae:lat 0.675074 ## FamilyOchnaceae:lat NA ## FamilyOnagraceae:lat NA ## FamilyOrobanchaceae:lat 0.476749 ## FamilyPhyllanthaceae:lat NA ## FamilyPicrodendraceae:lat NA ## FamilyPinaceae:lat 0.159861 ## FamilyPoaceae:lat 0.420438 ## FamilyPolemoniaceae:lat 0.433041 ## FamilyPolygonaceae:lat 0.444087 ## FamilyProteaceae:lat 0.149712 ## FamilyRanunculaceae:lat NA ## FamilyRhamnaceae:lat NA ## FamilyRosaceae:lat 0.160271 ## FamilyRubiaceae:lat 0.221123 ## FamilyRutaceae:lat NA ## FamilySalicaceae:lat 0.240215 ## FamilySapindaceae:lat 0.380059 ## FamilySapotaceae:lat NA ## FamilyScrophulariaceae:lat NA ## FamilyThymelaeaceae:lat NA ## FamilyUlmaceae:lat 0.282140 ## FamilyUrticaceae:lat 0.358817 ## FamilyViolaceae:lat NA ## FamilyXanthorrhoeaceae:lat NA ## FamilyZygophyllaceae:lat NA ## FamilyAsteraceae:long 0.001268 ** ## FamilyAtherospermataceae:long NA ## FamilyBalsaminaceae:long NA ## FamilyBetulaceae:long 0.374743 ## FamilyBrassicaceae:long NA ## FamilyCactaceae:long 0.155711 ## FamilyCasuarinaceae:long NA ## FamilyChloranthaceae:long NA ## FamilyChrysobalanaceae:long NA ## FamilyCistaceae:long NA ## FamilyCornaceae:long NA ## FamilyCrassulaceae:long NA ## FamilyCunoniaceae:long NA ## FamilyCupressaceae:long NA ## FamilyCyperaceae:long NA ## FamilyDennstaedtiaceae:long NA ## FamilyDicksoniaceae:long NA ## FamilyDipterocarpaceae:long NA ## FamilyEbenaceae:long NA ## FamilyElaeocarpaceae:long NA ## FamilyEricaceae:long 0.056466 . ## FamilyEuphorbiaceae:long 0.359718 ## FamilyFabaceae - C:long NA ## FamilyFabaceae - M:long NA ## FamilyFabaceae - P:long NA ## FamilyFagaceae:long NA ## FamilyGentianaceae:long NA ## FamilyHeliconiaceae:long NA ## FamilyJuglandaceae:long NA ## FamilyJuncaginaceae:long NA ## FamilyLamiaceae:long NA ## FamilyLauraceae:long NA ## FamilyMaesaceae:long NA ## FamilyMalvaceae:long 0.449768 ## FamilyMelastomataceae:long NA ## FamilyMoraceae:long NA ## FamilyMyristicaceae:long NA ## FamilyMyrsinaceae:long NA ## FamilyMyrtaceae:long 0.129339 ## FamilyOchnaceae:long NA ## FamilyOnagraceae:long NA ## FamilyOrobanchaceae:long NA ## FamilyPhyllanthaceae:long NA ## FamilyPicrodendraceae:long NA ## FamilyPinaceae:long 0.977523 ## FamilyPoaceae:long NA ## FamilyPolemoniaceae:long NA ## FamilyPolygonaceae:long NA ## FamilyProteaceae:long 0.160569 ## FamilyRanunculaceae:long NA ## FamilyRhamnaceae:long NA ## FamilyRosaceae:long 0.294900 ## FamilyRubiaceae:long NA ## FamilyRutaceae:long NA ## FamilySalicaceae:long NA ## FamilySapindaceae:long 0.118118 ## FamilySapotaceae:long NA ## FamilyScrophulariaceae:long NA ## FamilyThymelaeaceae:long NA ## FamilyUlmaceae:long NA ## FamilyUrticaceae:long NA ## FamilyViolaceae:long NA ## FamilyXanthorrhoeaceae:long NA ## FamilyZygophyllaceae:long NA ## FamilyAsteraceae:alt 0.038360 * ## FamilyAtherospermataceae:alt NA ## FamilyBalsaminaceae:alt NA ## FamilyBetulaceae:alt NA ## FamilyBrassicaceae:alt NA ## FamilyCactaceae:alt 0.653610 ## FamilyCasuarinaceae:alt NA ## FamilyChloranthaceae:alt NA ## FamilyChrysobalanaceae:alt NA ## FamilyCistaceae:alt NA ## FamilyCornaceae:alt NA ## FamilyCrassulaceae:alt NA ## FamilyCunoniaceae:alt NA ## FamilyCupressaceae:alt NA ## FamilyCyperaceae:alt NA ## FamilyDennstaedtiaceae:alt NA ## FamilyDicksoniaceae:alt NA ## FamilyDipterocarpaceae:alt NA ## FamilyEbenaceae:alt NA ## FamilyElaeocarpaceae:alt NA ## FamilyEricaceae:alt 0.833476 ## FamilyEuphorbiaceae:alt NA ## FamilyFabaceae - C:alt NA ## FamilyFabaceae - M:alt NA ## FamilyFabaceae - P:alt NA ## FamilyFagaceae:alt NA ## FamilyGentianaceae:alt NA ## FamilyHeliconiaceae:alt NA ## FamilyJuglandaceae:alt NA ## FamilyJuncaginaceae:alt NA ## FamilyLamiaceae:alt NA ## FamilyLauraceae:alt NA ## FamilyMaesaceae:alt NA ## FamilyMalvaceae:alt NA ## FamilyMelastomataceae:alt NA ## FamilyMoraceae:alt NA ## FamilyMyristicaceae:alt NA ## FamilyMyrsinaceae:alt NA ## FamilyMyrtaceae:alt 0.818358 ## FamilyOchnaceae:alt NA ## FamilyOnagraceae:alt NA ## FamilyOrobanchaceae:alt NA ## FamilyPhyllanthaceae:alt NA ## FamilyPicrodendraceae:alt NA ## FamilyPinaceae:alt 0.436576 ## FamilyPoaceae:alt 0.673346 ## FamilyPolemoniaceae:alt NA ## FamilyPolygonaceae:alt NA ## FamilyProteaceae:alt 0.346160 ## FamilyRanunculaceae:alt NA ## FamilyRhamnaceae:alt NA ## FamilyRosaceae:alt NA ## FamilyRubiaceae:alt NA ## FamilyRutaceae:alt NA ## FamilySalicaceae:alt NA ## FamilySapindaceae:alt NA ## FamilySapotaceae:alt NA ## FamilyScrophulariaceae:alt NA ## FamilyThymelaeaceae:alt NA ## FamilyUlmaceae:alt NA ## FamilyUrticaceae:alt NA ## FamilyViolaceae:alt NA ## FamilyXanthorrhoeaceae:alt NA ## FamilyZygophyllaceae:alt NA ## FamilyAsteraceae:temp 0.004264 ** ## FamilyAtherospermataceae:temp NA ## FamilyBalsaminaceae:temp NA ## FamilyBetulaceae:temp NA ## FamilyBrassicaceae:temp NA ## FamilyCactaceae:temp NA ## FamilyCasuarinaceae:temp NA ## FamilyChloranthaceae:temp NA ## FamilyChrysobalanaceae:temp NA ## FamilyCistaceae:temp NA ## FamilyCornaceae:temp NA ## FamilyCrassulaceae:temp NA ## FamilyCunoniaceae:temp NA ## FamilyCupressaceae:temp NA ## FamilyCyperaceae:temp NA ## FamilyDennstaedtiaceae:temp NA ## FamilyDicksoniaceae:temp NA ## FamilyDipterocarpaceae:temp NA ## FamilyEbenaceae:temp NA ## FamilyElaeocarpaceae:temp NA ## FamilyEricaceae:temp NA ## FamilyEuphorbiaceae:temp NA ## FamilyFabaceae - C:temp NA ## FamilyFabaceae - M:temp NA ## FamilyFabaceae - P:temp NA ## FamilyFagaceae:temp NA ## FamilyGentianaceae:temp NA ## FamilyHeliconiaceae:temp NA ## FamilyJuglandaceae:temp NA ## FamilyJuncaginaceae:temp NA ## FamilyLamiaceae:temp NA ## FamilyLauraceae:temp NA ## FamilyMaesaceae:temp NA ## FamilyMalvaceae:temp NA ## FamilyMelastomataceae:temp NA ## FamilyMoraceae:temp NA ## FamilyMyristicaceae:temp NA ## FamilyMyrsinaceae:temp NA ## FamilyMyrtaceae:temp 0.008481 ** ## FamilyOchnaceae:temp NA ## FamilyOnagraceae:temp NA ## FamilyOrobanchaceae:temp NA ## FamilyPhyllanthaceae:temp NA ## FamilyPicrodendraceae:temp NA ## FamilyPinaceae:temp NA ## FamilyPoaceae:temp 0.042787 * ## FamilyPolemoniaceae:temp NA ## FamilyPolygonaceae:temp NA ## FamilyProteaceae:temp NA ## FamilyRanunculaceae:temp NA ## FamilyRhamnaceae:temp NA ## FamilyRosaceae:temp NA ## FamilyRubiaceae:temp NA ## FamilyRutaceae:temp NA ## FamilySalicaceae:temp NA ## FamilySapindaceae:temp NA ## FamilySapotaceae:temp NA ## FamilyScrophulariaceae:temp NA ## FamilyThymelaeaceae:temp NA ## FamilyUlmaceae:temp NA ## FamilyUrticaceae:temp NA ## FamilyViolaceae:temp NA ## FamilyXanthorrhoeaceae:temp NA ## FamilyZygophyllaceae:temp NA ## lat:long 0.165174 ## lat:temp 0.000384 *** ## lat:NPP 0.376311 ## long:alt 0.555973 ## long:temp 0.016734 * ## long:NPP 0.015356 * ## alt:temp 0.006587 ** ## temp:NPP 0.069341 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.14 on 10 degrees of freedom ## (15 observations deleted due to missingness) ## Multiple R-squared: 0.9981, Adjusted R-squared: 0.9695 ## F-statistic: 34.87 on 152 and 10 DF, p-value: 5.06e-07 R2 = 0.99 … very high. In general, AIC should not overfit. In practice, however, it can overfit if there are unmodelled correlation in the data, or if you use variables that are (indirectly) identical to your response. 3.4.2 Case study: Life satisfaction The following data set contains information about life satisfaction (lebensz_org) in Germany, based on the socio-economic panel. library(EcoData) ?soep Task Perform a causal analysis of the effect of income on life satisfaction, considering possible confounding / mediation / colliders. Solution Nearly all other variables are confounders, gesund_org could als be a collider Might consider splitting data into single households, families, as effects could be very different. Alternatively, could add interactions with single, families and / or time to see if effects of income are different A possible simple model is fit &lt;- lm(lebensz_org ~ sqrt(einkommenj1) + syear + sex + alter + anz_pers + bildung + erwerb + gesund_org, data = soep) summary(fit) ## ## Call: ## lm(formula = lebensz_org ~ sqrt(einkommenj1) + syear + sex + ## alter + anz_pers + bildung + erwerb + gesund_org, data = soep) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.7735 -0.7843 0.0966 0.9387 4.9146 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.257e+01 1.443e+01 -2.256 0.024072 * ## sqrt(einkommenj1) 4.741e-04 1.661e-04 2.855 0.004307 ** ## syear 2.032e-02 7.158e-03 2.838 0.004541 ** ## sex 6.830e-02 2.073e-02 3.296 0.000984 *** ## alter 1.272e-02 7.116e-04 17.882 &lt; 2e-16 *** ## anz_pers 7.040e-02 7.753e-03 9.081 &lt; 2e-16 *** ## bildung 2.027e-02 3.762e-03 5.387 7.23e-08 *** ## erwerb -1.267e-02 8.762e-03 -1.446 0.148277 ## gesund_org -8.283e-01 1.139e-02 -72.728 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.479 on 21611 degrees of freedom ## (1902 observations deleted due to missingness) ## Multiple R-squared: 0.2172, Adjusted R-squared: 0.2169 ## F-statistic: 749.6 on 8 and 21611 DF, p-value: &lt; 2.2e-16 Note that you shouldn’t interpret the other variables (Table II fallacy) in a causal analysis, because the other variables aren’t analyzed / corrected for confounding. "],["heteroskedasticity.html", "4 Heteroskedasticity and Grouped Data (Random Effects) 4.1 Adjusting the Functional Form 4.2 Modelling Variance Terms 4.3 Non-normality and Outliers 4.4 Random and Mixed Effects - Motivation 4.5 Case studies", " 4 Heteroskedasticity and Grouped Data (Random Effects) In the last chapter, we have discussed how to set up a basic lm, including the selection of predictors according to the purpose of the modelling (prediction, causal analysis). Remember in particular that for a causal analysis, which I consider the standard case in the scineces, first, think about the problem and your question an decide on a base structure. Ideally, you do this by: Writing down your scientific questions (e.g. Ozone ~ Wind) Then add confounders / mediators if needed. Remember to make a difference between variables controlled for confounding, and other confounders (which are typically not controlled for confounding). We may have to use some model selection, but in fact with a good analysis plan this is rarely necessary for a causal analysis. After having arrived at such a base structure, we will have to check if the model is appropriate for the analysis. Yesterday, we already discussed about residual checks and we discussed that the 4 standard residual plots check for 4 different problems. Residuals vs Fitted = Functional relationship. Normal Q-Q = Normality of residuals. Scale - Location = Variance homogeneity. Residuals vs Leverage = Should we worry about certain outliers? Here an example for a linear regression of Ozone against Wind: fit = lm(Ozone ~ Temp , data = airquality) plot(Ozone ~ Temp, data = airquality) abline(fit) par(mfrow = c(2, 2)) plot(fit) The usual strategy now is to First get the functional relationship right, so that the model correctly describe the mean Then adjust the model assumptions regardigng distribution, variance and outliers. We will go through these steps now, and on the way also learn how to deal with heteroskedasticity, outliers, weird distributions and grouped data (random or mixed models). 4.1 Adjusting the Functional Form In the residual ~ fitted plot above, we can clearly see a pattern, which means that our model has a systematic misfit. Note that in a multiple regression, you should also check res ~ predictor for all predictors, because patterns of misfit often show up more clearly when plotted against the single predictors. What should we do if we see a pattern? Here a few strategies that you might want to consider: 4.1.1 Changing the regression formular The easiest strategy is to add complexity to the polynomial, e.g. quadratic terms, interactions etc. library(effects) fit = lm(Ozone ~ Wind * Temp + I(Wind^2) + I(Temp^2), data = airquality) plot(allEffects(fit, partial.residuals = T), selection = 1) plot(allEffects(fit, partial.residuals = T), selection = 2) plot(allEffects(fit, partial.residuals = T), selection = 3) and see if the residuals are getting better. To avoid doing this totally randomly, it may be useful to plot residuals against individual predictors by hand! 4.1.2 Generalized additive models (GAMs) Another options are GAMs = Generalized Additive Models. The idea is to fit a smooth function to data, to automatically find the “right” functional form. The smoothness of the function is automatically optimized. library(mgcv) fit = gam(Ozone ~ s(Wind) + s(Temp) + s(Solar.R) , data = airquality) summary(fit) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ s(Wind) + s(Temp) + s(Solar.R) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.663 25.32 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Wind) 2.910 3.657 13.695 &lt; 2e-16 *** ## s(Temp) 3.833 4.753 11.613 &lt; 2e-16 *** ## s(Solar.R) 2.760 3.447 3.967 0.00858 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.723 Deviance explained = 74.7% ## GCV = 338.9 Scale est. = 306.83 n = 111 # allEffects doesn&#39;t work here. plot(fit, pages = 0, residuals = T, pch = 20, lwd = 1.8, cex = 0.7, col = c(&quot;black&quot;, rep(&quot;red&quot;, length(fit$residuals)))) AIC(fit) ## [1] 962.596 Comparison to normal lm(): fit = lm(Ozone ~ Wind + Temp + Solar.R , data = airquality) AIC(fit) ## [1] 998.7171 Spline interaction is called a tensor spline: fit = gam(Ozone ~ te(Wind, Temp) + s(Solar.R) , data = airquality) summary(fit) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ te(Wind, Temp) + s(Solar.R) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.403 30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## te(Wind,Temp) 13.176 14.87 22.151 &lt;2e-16 *** ## s(Solar.R) 2.822 3.52 3.177 0.0237 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.803 Deviance explained = 83.1% ## GCV = 258.06 Scale est. = 218.54 n = 111 plot(fit, pages = 0, residuals = T, pch = 20, lwd = 1.9, cex = 0.4) AIC(fit) ## [1] 930.5047 GAMs are particularly useful for confounders. If you have confounders, you usually don’t care that the fitted relationship is a bit hard to interpret, you just want the confounder effect to be removed. So, if you want to fit the causal relationship between Ozone ~ Wind, account for the other variables, a good strategy might be: fit = gam(Ozone ~ Wind + s(Temp) + s(Solar.R) , data = airquality) summary(fit) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ Wind + s(Temp) + s(Solar.R) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.2181 6.3166 11.433 &lt; 2e-16 *** ## Wind -3.0302 0.6082 -4.982 2.55e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Temp) 3.358 4.184 14.972 &lt;2e-16 *** ## s(Solar.R) 2.843 3.551 3.721 0.0115 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.664 Deviance explained = 68.6% ## GCV = 402.11 Scale est. = 372.4 n = 111 In this way, you still get a nicely interpretable linear effect for Wind, but you don’t have to worry about the functional form of the other predictors. 4.1.3 Exercise functional form Task Assume our base model is fit &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality) and we are mainly interested in the effect of wind (Temp - Solar.R are added as confounders). Adjust the functional form until the mean is fitted well! Solution 4.2 Modelling Variance Terms After we have fixed the functional form, we want to look at the distribution of the residuals. We said yesterday that you can try to get them more normal by applying an appropriate transformation, e.g. the logarithm or square root. Without transformation, we often find that data shows heteroskedasticity, i.e. the residual variance changes with some predictor or the mean estimate (see also Scale - Location plot). Maybe your experimental data looks like this: set.seed(125) data = data.frame(treatment = factor(rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 15))) data$observation = c(7, 2 ,4)[as.numeric(data$treatment)] + rnorm( length(data$treatment), sd = as.numeric(data$treatment)^2 ) boxplot(observation ~ treatment, data = data) Especially p-values and confidence intervals of lm() and ANOVA can react quite strongly to such differences in residual variation. So, running a standard lm() / ANOVA on this data is not a good idea - in this case, we see that all regression effects are not significant, as is the ANOVA, suggesting that there is no difference between groups. fit = lm(observation ~ treatment, data = data) summary(fit) ## ## Call: ## lm(formula = observation ~ treatment, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.2897 -1.0514 0.3531 2.4465 19.8602 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.043 1.731 4.069 0.000204 *** ## treatmentB -3.925 2.448 -1.603 0.116338 ## treatmentC -1.302 2.448 -0.532 0.597601 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.704 on 42 degrees of freedom ## Multiple R-squared: 0.05973, Adjusted R-squared: 0.01495 ## F-statistic: 1.334 on 2 and 42 DF, p-value: 0.2744 summary(aov(fit)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 119.9 59.95 1.334 0.274 ## Residuals 42 1887.6 44.94 So, what can we do? 4.2.1 Transformation One option is to search for a transformation of the response that improves the problem - If heteroskedasticity correlates with the mean value, one can typically decrease it by some sqrt or log transformation, but often difficult, because this may also conflict with keeping the distribution normal. 4.2.2 Model the variance The second, more general option, is to model the variance - Modelling the variance to fit a model where the variance is not fixed. The basic option in R is nlme::gls. GLS = Generalized Least Squares. In this function, you can specify a dependency of the residual variance on a predictor or the response. See options via ?varFunc. In our case, we will use the varIdent option, which allows to specify a different variance per treatment. library(nlme) fit = gls(observation ~ treatment, data = data, weights = varIdent(form = ~ 1 | treatment)) summary(fit) ## Generalized least squares fit by REML ## Model: observation ~ treatment ## Data: data ## AIC BIC logLik ## 243.9258 254.3519 -115.9629 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | treatment ## Parameter estimates: ## A B C ## 1.000000 4.714512 11.821868 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 7.042667 0.2348387 29.989388 0.0000 ## treatmentB -3.925011 1.1317816 -3.467994 0.0012 ## treatmentC -1.302030 2.7861462 -0.467323 0.6427 ## ## Correlation: ## (Intr) trtmnB ## treatmentB -0.207 ## treatmentC -0.084 0.017 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.4587934 -0.6241702 0.1687727 0.6524558 1.9480169 ## ## Residual standard error: 0.9095262 ## Degrees of freedom: 45 total; 42 residual If you check the ANOVA, also the ANOVA is significant! anova(fit) ## Denom. DF: 42 ## numDF F-value p-value ## (Intercept) 1 899.3761 &lt;.0001 ## treatment 2 6.0962 0.0047 The second option for modeling variances is to use the glmmTMB.{R} package, which we will use quite frequently this week. Here, you can specify an extra regression formula for the dispersion (= residual variance). If we fit this: library(glmmTMB) fit = glmmTMB(observation ~ treatment, data = data, dispformula = ~ treatment) We get 2 regression tables as outputs - one for the effects, and one for the dispersion (= residual variance). We see, as expected, that the dispersion is higher in groups B and C compared to A. An advantage over gls is that we get confidence intervals and p-values for these differences on top! summary(fit) ## Family: gaussian ( identity ) ## Formula: observation ~ treatment ## Dispersion: ~treatment ## Data: data ## ## AIC BIC logLik deviance df.resid ## 248.7 259.5 -118.3 236.7 39 ## ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.0427 0.2269 31.042 &lt; 2e-16 *** ## treatmentB -3.9250 1.0934 -3.590 0.000331 *** ## treatmentC -1.3020 2.6917 -0.484 0.628582 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Dispersion model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2587 0.3651 -0.708 0.479 ## treatmentB 3.1013 0.5164 6.006 1.91e-09 *** ## treatmentC 4.9399 0.5164 9.566 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.2.3 Exercise variance modelling Take this plot of Ozone ~ Solar.R using the airquality data. Clearly there is heteroskedasticity in the relationship: plot(Ozone ~ Solar.R, data = airquality) We can also see this when we fit the regression model: m1 = lm(Ozone ~ Solar.R, data = airquality) par(mfrow = c(2, 2)) plot(m1) Task We could of course consider other predictors, but let’s say we want to fit this model specifically Try to get the variance stable with a transformation. Use the gls function (package nlme) with the untransformed response to make the variance dependent on Solar.R. Hint: Read in varClasses.{R} and decide how to model this. Use glmmTMB.{R} to model heteroskedasticity. Solution 4.3 Non-normality and Outliers What can we do if, after accounting for the functional relationship, response transformation and variance modelling, residual diagnostic 2 shows non-normality, in particular strong outliers? Here simulated example data with strong outliers / deviations from normality: set.seed(123) n = 100 concentration = runif(n, -1, 1) growth = 2 * concentration + rnorm(n, sd = 0.5) + rbinom(n, 1, 0.05) * rnorm(n, mean = 6*concentration, sd = 6) plot(growth ~ concentration) Fitting the model, we see that the distribution is to wide: fit = lm(growth ~ concentration) par(mfrow = c(2, 2)) plot(fit) What can we do to deal with such distributional problems and outliers? Removing - Bad option, hard to defend, reviewers don’t like this - if at all, better show robustness with and without outlier, but result is sometimes not robust. Change the distribution - Fit a model with a different distribution, i.e. GLM or other. -&gt; We will do this on Wednesday. Robust regressions. Quantile regression - A special type of regression that does not assume a particular residual distribution. Change distribution If we want to change the distribution, we have to go to a GLM, see Wednesday. Robust regression Robust methods generally refer to methods that are robust to violation of assumptions, e.g. outliers. More specifically, standard robust regressions typically downweight datap oints that have a too high influence on the fit. See https://cran.r-project.org/web/views/Robust.html for a list of robust packages in R. # This is the classic method. library(MASS) fit = rlm(growth ~ concentration) summary(fit) ## ## Call: rlm(formula = growth ~ concentration) ## Residuals: ## Min 1Q Median 3Q Max ## -7.1986 -0.3724 0.0377 0.3391 7.0902 ## ## Coefficients: ## Value Std. Error t value ## (Intercept) -0.0978 0.0594 -1.6453 ## concentration 2.0724 0.1048 19.7721 ## ## Residual standard error: 0.534 on 98 degrees of freedom # No p-values and not sure if we can trust the confidence intervals. # Would need to boostrap by hand! # This is another option that gives us p-values directly. library(robustbase) ## Warning: package &#39;robustbase&#39; was built under R version 4.1.1 ## ## Attaching package: &#39;robustbase&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## heart fit = lmrob(growth ~ concentration) summary(fit) ## ## Call: ## lmrob(formula = growth ~ concentration) ## \\--&gt; method = &quot;MM&quot; ## Residuals: ## Min 1Q Median 3Q Max ## -7.2877 -0.4311 -0.0654 0.2788 7.0384 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.04448 0.05160 -0.862 0.391 ## concentration 2.00588 0.08731 22.974 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Robust residual standard error: 0.5549 ## Multiple R-squared: 0.8431, Adjusted R-squared: 0.8415 ## Convergence in 7 IRWLS iterations ## ## Robustness weights: ## 9 observations c(27,40,47,52,56,76,80,91,100) ## are outliers with |weight| = 0 ( &lt; 0.001); ## 5 weights are ~= 1. The remaining 86 ones are summarized as ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.6673 0.9015 0.9703 0.9318 0.9914 0.9989 ## Algorithmic parameters: ## tuning.chi bb tuning.psi refine.tol ## 1.548e+00 5.000e-01 4.685e+00 1.000e-07 ## rel.tol scale.tol solve.tol eps.outlier ## 1.000e-07 1.000e-10 1.000e-07 1.000e-03 ## eps.x warn.limit.reject warn.limit.meanrw ## 1.819e-12 5.000e-01 5.000e-01 ## nResample max.it best.r.s k.fast.s k.max ## 500 50 2 1 200 ## maxit.scale trace.lev mts compute.rd fast.s.large.n ## 200 0 1000 0 2000 ## psi subsampling cov ## &quot;bisquare&quot; &quot;nonsingular&quot; &quot;.vcov.avar1&quot; ## compute.outlier.stats ## &quot;SM&quot; ## seed : int(0) Quantile regression Quantile regressions don’t fit a line with an error spreading around it, but try to fit a quantile (e.g. the 0.5 quantile, the median) regardless of the distribution. Thus, they work even if the usual assumptions don’t hold. library(qgam) dat = data.frame(growth = growth, concentration = concentration) fit = qgam(growth ~ concentration, data = dat, qu = 0.5) ## Estimating learning rate. Each dot corresponds to a loss evaluation. ## qu = 0.5................done summary(fit) ## ## Family: elf ## Link function: identity ## ## Formula: ## growth ~ concentration ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.08167 0.05823 -1.403 0.161 ## concentration 2.04781 0.09500 21.556 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = 0.427 Deviance explained = 48.8% ## -REML = 157.82 Scale est. = 1 n = 100 Summary Actions on real outliers: Robust regression. Remove Actions on different distributions: Transform. Change distribution or quantile regression. 4.4 Random and Mixed Effects - Motivation Random effects are a very common addition to regression models that can be used for any type of grouping (categorical) variable. Lets look at the Month in airquality: airquality$fMonth = as.factor(airquality$Month) Let’s say further that we are only interested in calculating the mean of Ozone: fit = lm(Ozone ~ 1, data = airquality) Problem: If we fit residuals, we see that they are correlated in Month, so we somehow have to account for Month: plot(residuals(fit) ~ airquality$fMonth[as.numeric(row.names(model.frame(fit)))]) A fixed effect model for fMonth would be fit = lm(Ozone ~ fMonth, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ fMonth, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52.115 -16.823 -7.282 13.125 108.038 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.615 5.759 4.101 7.87e-05 *** ## fMonth6 5.829 11.356 0.513 0.609 ## fMonth7 35.500 8.144 4.359 2.93e-05 *** ## fMonth8 36.346 8.144 4.463 1.95e-05 *** ## fMonth9 7.833 7.931 0.988 0.325 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.36 on 111 degrees of freedom ## (37 observations deleted due to missingness) ## Multiple R-squared: 0.2352, Adjusted R-squared: 0.2077 ## F-statistic: 8.536 on 4 and 111 DF, p-value: 4.827e-06 However, using a fixed effect costs a lot of degrees of freedom, and maybe we are not really interested in Month, we just want to correct the correlation in the residuals. Solution: Mixed / random effect models. In a mixed model, we assume (differently to a fixed effect model) that the effect of Month is coming from a normal distribution. In a way, you could say that there are two types of errors: The random effect, which is a normal “error” per group (in this case Month). And the residual error, which comes on top of the random effect. Because of this hierarchical structure, these models are also called “multi-level models” or “hierarchical models”. Nomenclature: No random effect = Fixed effect model. Only random effects + intercept = Random effect model. Random effects + fixed effects = Mixed model. Because grouping naturally occurs in any type of experimental data (batches, blocks, etc.), mixed effect models are the de-facto default for most experimental data! Mind, that grouping even occurs for example, when 2 different persons gather information. 4.4.1 Fitting Random Effects Models To speak about random effects, we will use an example data set containing exam scores of 4,059 students from 65 schools in Inner London. This data set is located in the R package mlmRev.{R}. Response: “normexam” (Normalized exam score). Predictor 1: “standLRT” (Standardised LR test score; Reading test taken when they were 11 years old). Predictor 2: “sex” of the student (F / M). If we analyze this with a simple lm, we get the following response: library(mlmRev) library(effects) mod0 = lm(normexam ~ standLRT + sex , data = Exam) plot(allEffects(mod0)) Random intercept model A random intercept model assumes that each school gets their own intercept. It’s pretty much identical to the fixed effect model lm(normexam ~ standLRT + sex + school), except that instead of giving each school a separate independent intercept, we assume that the school effects come from a common normal distribution. mod1 = lmer(normexam ~ standLRT + sex + (1 | school), data = Exam) summary(mod1) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: normexam ~ standLRT + sex + (1 | school) ## Data: Exam ## ## REML criterion at convergence: 9346.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7120 -0.6314 0.0166 0.6855 3.2735 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school (Intercept) 0.08986 0.2998 ## Residual 0.56252 0.7500 ## Number of obs: 4059, groups: school, 65 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.07639 0.04202 1.818 ## standLRT 0.55947 0.01245 44.930 ## sexM -0.17136 0.03279 -5.226 ## ## Correlation of Fixed Effects: ## (Intr) stnLRT ## standLRT -0.013 ## sexM -0.337 0.061 If we look at the outputs, we see that the effects of school are not explicitly mentioned, i.e. we fit an average over the schools. This is also because we treat the random effects as error rather than as estimates. However, the school mean effects are estimated, and we can make them visible, e.g. via: with(Exam, { randcoef = ranef(mod1)$school[,1] fixedcoef = fixef(mod1) plot(standLRT, normexam) for(i in 1:65){ abline(a = fixedcoef[1] + randcoef[i], b = fixedcoef[2], col = i) } }) Random slope model A random slope model assumes that each school also gets their own slope for a given parameter (per default we will always estimate slope and intercept, but you could overwrite this, not recommended!). Let’s do this for standLRT (you could of course do both as well). mod2 = lmer(normexam ~ standLRT + sex + (standLRT | school), data = Exam) summary(mod2) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: normexam ~ standLRT + sex + (standLRT | school) ## Data: Exam ## ## REML criterion at convergence: 9303.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.8339 -0.6373 0.0245 0.6819 3.4500 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## school (Intercept) 0.08795 0.2966 ## standLRT 0.01514 0.1230 0.53 ## Residual 0.55019 0.7417 ## Number of obs: 4059, groups: school, 65 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.06389 0.04167 1.533 ## standLRT 0.55275 0.02016 27.420 ## sexM -0.17576 0.03228 -5.445 ## ## Correlation of Fixed Effects: ## (Intr) stnLRT ## standLRT 0.356 ## sexM -0.334 0.036 Fitting a random slope on standLRT is pretty much identical to fit the fixed effect model lm(normexam ~ standLRT*school + sex), except that school is a random effect, and therefore parameter estimates for the interaction sex:school are not independent. The results is similar to the random intercept model, except that we have an additional variance term. Here a visualization of the results with(Exam, { randcoefI = ranef(mod2)$school[,1] randcoefS = ranef(mod2)$school[,2] fixedcoef = fixef(mod2) plot(standLRT, normexam) for(i in 1:65){ abline(a = fixedcoef[1] + randcoefI[i] , b = fixedcoef[2] + randcoefS[i], col = i) } }) Syntax cheat sheet: Random intercept: `(1 | group). ONLY random slope for a given fixed effect: (0 + fixedEffect | group). Random slope + intercept + correlation (default): (fixedEffect | group). Random slope + intercept without correlation: (fixedEffect || group), identical to (1 | group) + (0 + fixedEffect | group). Nested random effects: (1 | group / subgroup). If groups are labeled A, B, C, … and subgroups 1, 2, 3, …, this will create labels A1, A2 ,B1, B2, so that you effectively group in subgroups. Useful for the many experimental people that do not label subgroups uniquely, but otherwise no statistical difference to a normal random effect. Crossed random effects: You can add random effects independently, as in (1 | group1) + (1 | group2). 4.4.2 Task: a mixed model for plantHeight Task Take our old plantHeight dataset that we worked with already a lot. Consider that the relationship height ~ temp may be different for each family. Some families may have larger plants in general (random intercept), but it may also be that the temperature relationship changes per family. Thus, you could include family as random intercept + slope in this relationship. Specify the model and run it! Solution oldModel &lt;- lm(loght ~ temp, data = plantHeight) summary(oldModel) # Stragegy: if you have a grouping factor, add a random intercept # check additionally for random slope randomInterceptModel &lt;- lmer(loght ~ temp + (1|Family), data = plantHeight) summary(randomInterceptModel) # if I count sd = 1 paramter more # if I count the REs, 69 parameters more # solution: depends on the sd estimate -&gt; wide sd, loose nearly 69df, narrow sd = loose nothing except the sd estimate -&gt; flexibility of an random effect model is adoptiv (not fixed) fixedInterceptModel &lt;- lm(loght ~ temp + Family, data = plantHeight) summary(fixedInterceptModel) # loose 69 degrees of freedom -&gt; this model has 69 parameters more to fit than the model loght ~ temp, loose a lot of power randomSlopeModel &lt;- lmer(loght ~ temp + (temp | Family), data = plantHeight) summary(randomSlopeModel) # scaling helps the optimizer plantHeight$sTemp = scale(plantHeight$temp) randomSlopeModel &lt;- lmer(loght ~ sTemp + (sTemp | Family), data = plantHeight) summary(randomSlopeModel) # fixed effect model has no significance any more fixedSlopeInterceptModel &lt;- lm(loght ~ temp * Family, data = plantHeight) summary(fixedSlopeInterceptModel) summary(aov(fixedSlopeInterceptModel)) 4.4.3 Problems With Mixed Models Specifying mixed models is quite simple, however, there is a large list of (partly quite complicate) issues in their practical application. Here, a (incomplete) list: Interpretation of random effects What do the random effects mean? The classical interpretation of the rando intercept is that they are a group-structured error, i.e. they are like residuals, but for an entire group. However, this view breaks down a bit for a random slope model. Another way to view this is that the RE terms absorb variation in parameters. This view works for random intercept and slope models. So, our main model fits the grand mean, and the RE models variation in the parameters, which is assumed to be normally distributed. A third view of RE models is that they are regularized fixed effect models. What does that mean? Note again that, conceptual, every random effect model corresponds to a fixed effect model. For a random intercept model, this would be lmer(y ~ x + (1|group)) =&gt; lm(y ~ x + group) and for the random slope, it would be lmer(y ~ x + (x|group)) =&gt; lm(y ~ x * group) So, you could alternatively always fit the fixed effect model. What’s the difference? First of all, for the fixed effect models, you would get many more estimates and p-values in the standard summary functions. But that’s not really a big difference. More importantly, however, estimates in the random effect model will be closer together, and if you have groups with very little data, they will still be fittable. So, in a random effect model, parameter estimates for groups with few data points are informed by the estimates of the other groups, because they are connected via the normal distribution. This is today often used by people that want to fit effects across groups with varying availability of data. For example, if we have data for common and rare species, and we are interested in the density dependence of the species, we could fit mortality ~ density + (density | species) In such a model, we have the mean density effect across all species, and rare species with few data will be constrained by this effect, while common species with a lot of data can overrule the normal distribution imposed by the random slope and get their own estimate. In this picture, the random effect imposes an adaptive shrinkage, similar to a LASSO or ridge shrinkage estimator, with the shrinkage strength controlled by the standard deviation of the random effect. Degrees of freedom for a random effect The second problem is: How many parameters does a random effect model have? To know how many parameters the model hss is crucial for calculating p-values, AIC and all that. We can estimate roughly how many parameters we should have by looking at the fixed effect version of the models: mod1 = lm(normexam ~ standLRT + sex , data = Exam) mod1$rank # 3 parameters. ## [1] 3 mod2 = lmer(normexam ~ standLRT + sex + (1 | school), data = Exam) # No idea how many parameters. mod3 = lm(normexam ~ standLRT + sex + school, data = Exam) mod3$rank # 67 parameters. ## [1] 67 What we can say is that the mixed model is more complicated than mod1, but less than mod2 (as it has the additional constraint), so the complexity must be somewhere in-between. But now much? In fact, the complexity is controlled by the estimated variance of the random effect. For a high variance, the model is nearly as complex as mod3, for a low variance, it is only as complex as mod1. Because of these issues, lmer by default does not return p-values. However, you can calculate p-values based on approximate degrees of freedom via the lmerTest package, which also corrects ANOVA for random effects, but not AIC. library(lmerTest) m2 = lmer(normexam ~ standLRT + sex + (1 | school), data = Exam, REML = F) summary(m2) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s ## method [lmerModLmerTest] ## Formula: normexam ~ standLRT + sex + (1 | school) ## Data: Exam ## ## AIC BIC logLik deviance df.resid ## 9340.0 9371.6 -4665.0 9330.0 4054 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7117 -0.6326 0.0168 0.6865 3.2744 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school (Intercept) 0.08807 0.2968 ## Residual 0.56226 0.7498 ## Number of obs: 4059, groups: school, 65 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.07646 0.04168 76.66670 1.834 0.0705 . ## standLRT 0.55954 0.01245 4052.83930 44.950 &lt; 2e-16 *** ## sexM -0.17138 0.03276 3032.37966 -5.231 1.8e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) stnLRT ## standLRT -0.013 ## sexM -0.339 0.061 Predictions When we predict, we can predict conditional on the fitted REs, or we can use the grand mean. In some packages, the predict function can be adjusted. In lme4, this is via the option re.form, which is available in the predict and simulate function. Model selection with mixed models For model selection, the degrees of freedom problem means that normal model selection techniques such as AIC or naive LRTs don’t work on the random effect structure, because they don’t count the correct degrees of freedom. However, assuming that by changing the fixed effect structure, the flexibility of the REs doesn’t change a lot (you would see this by looking at the RE sd), we can use standard model selection on the fixed effect structure. All I have said about model selection on standard models applies also here: good for predictions, rarely a good idea if your goal is causal inference. Regarding the random effect structure - my personal recommendation for most cases is the following: add random intercept on all abvious grouping variables check residuals per group (e.g. with the plot function below), add random slope if needed m1 &lt;- lmer(y ~ x + (1|group)) plot(m1, resid(., scaled=TRUE) ~ fitted(.) | group, abline = 0) If you absolutely want to do model selection on the RE structure lmerTest::ranova performs an ANOVA with estimated df, adding entire RE groups if you want to do details model selections on the RE structure, you should implement a simulated LRT based on a parametric bootstrap. See day 5, on the parametric bootstrap. Variance partitioning / ANOVA Also variance partitioning in mixed models is a bit tricky, as (see type I/II/III ANOVA discussion) fixed and random components of the model are in some way “correlated”. Moreover, a key question is (see also interpreatio above): Do you want to count the random effect variance as “explained”, or “residual”. The most common approach is the hierarchical partitioning proposed by by Nakagawa &amp; Schielzeth 2013, Nakagawa et al. (2017), which is implemented in the MuMIn.{R} package. With this, we can run library(MuMIn) r.squaredGLMM(m2) ## R2m R2c ## [1,] 0.3303846 0.4210712 Interpretation R2m: Marginal \\({R}^{2}\\) value associated with fixed effects. R2c: Conditional \\({R}^{2}\\) value associated with fixed effects plus the random effects. 4.5 Case studies 4.5.1 Case Study 1: College Student Performance Over Time Background and data structure The GPA (college grade point average) data is a longitudinal data set (also named panel data, German: “Längsschnittstudie”. A study repeated at several different moments in time, compared to a cross-sectional study (German: “Querschnittstudie”) which has several participants at one time). In this data set, 200 college students and their GPA have been followed 6 consecutive semesters. Look at the GPA data set, which can be found in the EcoData package: library(EcoData) str(gpa) In this data set, there are GPA measures on 6 consecutive occasions, with a job status variable (how many hours worked) for the same 6 occasions. There are two student-level explanatory variables: The sex (1 = male, 2 = female) and the high school gpa. There is also a dichotomous student-level outcome variable, which indicates whether a student has been admitted to the university of their choice. Since not every student applies to a university, this variable has many missing values. Each student and each year of observation have an id. Task Analyze if GPA improves over time (occasion)! Here a few hints to look at: Consider which fixed effect structure you want to fit. For example, you might be interested if males and femals differ in their temporal trend Student is the grouping variable -&gt; RE. Which RE structure do you want to fit? A residual plot may help For your benefit, have a look at the difference in the regression table (confidence intervals, coefficients and p-values) of mixed and corresponding fixed effects model. You can also look at the estimates of the mixed effects model (hint: ?ranef). After having specified the mixed model, have a look at residuals. You can model dispersion problems in mixed models with glmmTMB, same syntax for REs as lme4 Solution library(lme4) library(glmmTMB) library(EcoData) # initial model with a random intercept and fixed effect structure based on # causal assumptions gpa$sOccasion = scale(gpa$occasion) gpa$nJob = as.numeric(gpa$job) fit &lt;- lmer(gpa ~ sOccasion*sex + nJob + (1|student), data = gpa) summary(fit) # plot seems to show a lot of differences still, so add random slope plot(fit, resid(., scaled=TRUE) ~ fitted(.) | student, abline = 0) # slope + intercept model fit &lt;- lmer(gpa ~ sOccasion*sex + nJob + (sOccasion|student), data = gpa) # checking residuals - looks like heteroskedasticity plot(fit) fit &lt;- glmmTMB(gpa ~ sOccasion*sex + nJob + (sOccasion|student), data = gpa, dispformula = ~ sOccasion) summary(fit) # unfortunately, the dispersion in this model cannot be reliably checked, because the functions for this # are not (yet) implemented in glmmTMB plot(residuals(fit, type = &quot;pearson&quot;) ~ predict(fit)) # not implemented library(DHARMa) simulateResiduals(fit, plot = T) # not very reliable because glmmTMB cannot do simulations conditional on the fitted REs # still, the variable dispersion model is highly supported by the data and clearly preferable over a fixed dispersion model 4.5.2 Case Study 2 - Honeybee Data Task We use a dataset on bee colonies infected by the American Foulbrood (AFB) disease. library(EcoData) str(bees) Perform the data analysis, according to the hypothesis discussed in the course. Solution # adding BeesN as a possible confounder library(lme4) fit &lt;- lmer(log(Spobee + 1) ~ Infection + BeesN + (1|Hive), data = bees) summary(fit) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: log(Spobee + 1) ~ Infection + BeesN + (1 | Hive) ## Data: bees ## ## REML criterion at convergence: 260.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.27939 -0.45413 0.09209 0.52159 1.64023 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Hive (Intercept) 4.8463 2.2014 ## Residual 0.6033 0.7767 ## Number of obs: 72, groups: Hive, 24 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 5.583e+00 1.907e+00 2.100e+01 2.927 0.00805 ** ## Infection 2.663e+00 5.528e-01 2.100e+01 4.818 9.22e-05 *** ## BeesN -2.068e-05 2.558e-05 2.100e+01 -0.808 0.42804 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Infctn ## Infection -0.476 ## BeesN -0.966 0.398 ## fit warnings: ## Some predictor variables are on very different scales: consider rescaling # residual plot shows that hives are either infected or not, thus # doesn&#39;t make sense to add a random slope plot(fit, resid(., scaled=TRUE) ~ fitted(.) | Hive, abline = 0) "],["GLMMs.html", "5 GLMMs 5.1 Basics 5.2 Dispersion Problems in GLMs 5.3 Case Studies", " 5 GLMMs 5.1 Basics Generalized linear models (GLMs) in R are fit with the glm() function. The main difference from lm() is that you can specify the ‘family’ parameter, which gives you the option to use different distributions than the normal distribution. The family argument also includes the link function. The link function internally transforms a linear model on the predictors, so that its response corresponds to the range of the outcome distribution. If you don’t specify a link, the default link for each family is chosen. The most important are Log link for Poisson family. Logit link for Bernoulli / Binomial family. Of course, there are many additional distributions that you could consider for your response. Here an overview of the most common choices: Screenshot taken from Wikipedia: https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function. Content licensed under the Creative Commons Attribution-ShareAlike License 3.0. 5.1.1 Binomial data - logistic regression The standard model to fit binomial (0/1 or k/n) data is the logistic regression, which combines the binomial distribution with a logit link function. To get to know this model, let’s have a look at the titanic data set in EcoData: library(EcoData) ## ## Attaching package: &#39;EcoData&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## cement str(titanic) ## &#39;data.frame&#39;: 1309 obs. of 14 variables: ## $ pclass : int 1 1 1 1 1 1 1 1 1 1 ... ## $ survived : int 1 1 0 0 0 1 1 0 1 0 ... ## $ name : chr &quot;Allen, Miss. Elisabeth Walton&quot; &quot;Allison, Master. Hudson Trevor&quot; &quot;Allison, Miss. Helen Loraine&quot; &quot;Allison, Mr. Hudson Joshua Creighton&quot; ... ## $ sex : chr &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;male&quot; ... ## $ age : num 29 0.917 2 30 25 ... ## $ sibsp : int 0 1 1 1 1 0 1 0 2 0 ... ## $ parch : int 0 2 2 2 2 0 0 0 0 0 ... ## $ ticket : chr &quot;24160&quot; &quot;113781&quot; &quot;113781&quot; &quot;113781&quot; ... ## $ fare : num 211 152 152 152 152 ... ## $ cabin : chr &quot;B5&quot; &quot;C22 C26&quot; &quot;C22 C26&quot; &quot;C22 C26&quot; ... ## $ embarked : chr &quot;S&quot; &quot;S&quot; &quot;S&quot; &quot;S&quot; ... ## $ boat : chr &quot;2&quot; &quot;11&quot; &quot;&quot; &quot;&quot; ... ## $ body : int NA NA NA 135 NA NA NA NA NA 22 ... ## $ home.dest: chr &quot;St Louis, MO&quot; &quot;Montreal, PQ / Chesterville, ON&quot; &quot;Montreal, PQ / Chesterville, ON&quot; &quot;Montreal, PQ / Chesterville, ON&quot; ... mosaicplot( ~ survived + sex + pclass, data = titanic) titanic$pclass = as.factor(titanic$pclass) We want to analyze how survival in the titanic accident dependend on other predictors. We could fit an lm, but the residual checks make it very evident that the data with a 0/1 response don’t fit to the assumption of an lm: fit = lm(survived ~ sex * age, data = titanic) summary(fit) ## ## Call: ## lm(formula = survived ~ sex * age, data = titanic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8901 -0.2291 -0.1564 0.2612 0.9744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.637645 0.046165 13.812 &lt; 2e-16 *** ## sexmale -0.321308 0.059757 -5.377 9.35e-08 *** ## age 0.004006 0.001435 2.792 0.00534 ** ## sexmale:age -0.007641 0.001823 -4.192 3.01e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4115 on 1042 degrees of freedom ## (263 observations deleted due to missingness) ## Multiple R-squared: 0.3017, Adjusted R-squared: 0.2997 ## F-statistic: 150 on 3 and 1042 DF, p-value: &lt; 2.2e-16 par(mfrow = c(2, 2)) plot(fit) Thus, what we want to fit is a logistic regression, which assumes a 0/1 response + logit link. In principle, this is distribution is called Bernoulli, but in R both 0/1 and k/n are called “binomial”, as Bernoulli is the special case of binomial where n = 1. m1 = glm(survived ~ sex*age, family = &quot;binomial&quot;, data = titanic) summary(m1) ## ## Call: ## glm(formula = survived ~ sex * age, family = &quot;binomial&quot;, data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0247 -0.7158 -0.5776 0.7707 2.2960 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.493381 0.254188 1.941 0.052257 . ## sexmale -1.154139 0.339337 -3.401 0.000671 *** ## age 0.022516 0.008535 2.638 0.008342 ** ## sexmale:age -0.046276 0.011216 -4.126 3.69e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1414.6 on 1045 degrees of freedom ## Residual deviance: 1083.4 on 1042 degrees of freedom ## (263 observations deleted due to missingness) ## AIC: 1091.4 ## ## Number of Fisher Scoring iterations: 4 Can you interpret the output? What do the regression coefficients mean? In principle, interpretation as before, but if you want transform the coefficients in predictions, you have to apply the link function on the linear predictor. Binomial uses per default the logit link, to calculate the response use: plogis(0.493381 + 0.022516 * 20) # Women, age 20. ## [1] 0.7198466 plogis(0.493381 -1.154139 + 20*(0.022516-0.046276)) # Men, age 20 ## [1] 0.2430632 Alternatively, you can also use the predict function to transform predictions to the response scale newDat = data.frame(sex = as.factor(c(&quot;female&quot;, &quot;male&quot;)), age = c(20,20)) predict(m1, newdata = newDat) # Linear predictor. ## 1 2 ## 0.9436919 -1.1359580 predict(m1, newdata = newDat, type = &quot;response&quot;) # Response scale. ## 1 2 ## 0.7198448 0.2430633 A third alternative is to look at the effect plots, which scale the y axis according to the link scale library(effects) plot(allEffects(m1)) Note: Treatment coding for factors works as before. If you have k/n data, you can either specify the response as cbind(k, n-k), or you can fit the glm with k ~ x, weights = n For interactions, as in our age effect for male / female, effect sizes can in general not be directly be compared, because they are calculated at a different intercept, and through the nonlinear link, this leads to a different effect on the response. One option to solve this are the so-called odds ratios. Or just look at the response scale, e.g. via the effect plots, and interpret there! In our example, however, effect directions changed, so there is no question that there is an interactions. Residual checks How can we check the residuals of a GLM? First of all: Due to an unfortunate programming choice in R (Nerds: Check class(m1)), the standard residual plots still work par(mfrow = c(2, 2)) plot(m1) but they don’t look any better than before, because they still check for normality of the residuals, while we are interested in the question of whether the residuals are binomially distributed. The DHARMa.{R} package solves this problem. Load the DHARMa.{R} package, which should have been installed with EcoData already: library(DHARMa) res = simulateResiduals(m1) Standard plot: plot(res) Out of the help page: The function creates a plot with two panels. The left panel is a uniform Q-Q plot (calling plotQQunif), and the right panel shows residuals against predicted values (calling plotResiduals), with outliers highlighted in red. Very briefly, we would expect that a correctly specified model shows: A straight 1-1 line, as well as not significant of the displayed tests in the Q-Q-plot (left) -&gt; Evidence for a correct overall residual distribution (for more details on the interpretation of this plot, see help). Visual homogeneity of residuals in both vertical and horizontal direction, as well as no significance of quantile tests in the Residual vs. predicted plot (for more details on the interpretation of this plot, see help). Deviations from these expectations can be interpreted similarly to a linear regression. See the vignette for detailed examples. Also residuals against predictors shows no particular problem: par(mfrow = c(1, 2)) plotResiduals(m1, form = model.frame(m1)$age) plotResiduals(m1, form = model.frame(m1)$sex) Residuals against missing predictor show a clear problem: dataUsed = as.numeric(rownames(model.frame(m1))) plotResiduals(m1, form = titanic$pclass[dataUsed]) Thus, I should add passenger class to the model m2 = glm(survived ~ sex*age + pclass, family = &quot;binomial&quot;, data = titanic) summary(m2) ## ## Call: ## glm(formula = survived ~ sex * age + pclass, family = &quot;binomial&quot;, ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3844 -0.6721 -0.4063 0.7041 2.5440 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.790839 0.362822 7.692 1.45e-14 *** ## sexmale -1.029755 0.358593 -2.872 0.00408 ** ## age -0.004084 0.009461 -0.432 0.66598 ## pclass2 -1.424582 0.241513 -5.899 3.67e-09 *** ## pclass3 -2.388178 0.236380 -10.103 &lt; 2e-16 *** ## sexmale:age -0.052891 0.012025 -4.398 1.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1414.62 on 1045 degrees of freedom ## Residual deviance: 961.92 on 1040 degrees of freedom ## (263 observations deleted due to missingness) ## AIC: 973.92 ## ## Number of Fisher Scoring iterations: 5 plotResiduals(m2, form = model.frame(m2)$pclass) Now, residuals look fine. We will talk about DHARMa.{R} more later, see also comments on testing binomial GLMs here. 5.1.2 Poisson regression The second common regression model is the Poisson regression, which is used for count data (1,2,3). The Poisson regression means a Poisson distribution + log link function. library(EcoData) str(birdfeeding) ## &#39;data.frame&#39;: 25 obs. of 2 variables: ## $ feeding : int 3 6 8 4 2 7 6 8 10 3 ... ## $ attractiveness: int 1 1 1 1 1 2 2 2 2 2 ... plot(feeding ~ attractiveness, data = birdfeeding) fit = glm(feeding ~ attractiveness, data = birdfeeding, family = &quot;poisson&quot;) summary(fit) ## ## Call: ## glm(formula = feeding ~ attractiveness, family = &quot;poisson&quot;, data = birdfeeding) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.55377 -0.72834 0.03699 0.59093 1.54584 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.47459 0.19443 7.584 3.34e-14 *** ## attractiveness 0.14794 0.05437 2.721 0.00651 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 25.829 on 24 degrees of freedom ## Residual deviance: 18.320 on 23 degrees of freedom ## AIC: 115.42 ## ## Number of Fisher Scoring iterations: 4 Log link means that calculating predicted value for attractiveness requires exp(linear response). exp(1.47459 + 3 * 0.14794) ## [1] 6.810122 Effect plots, note the log scaling on the y axis plot(allEffects(fit)) Residual checks are OK, but note that most Poisson models in practice tend to be overdispersed (see next chapter). res = simulateResiduals(fit, plot = T) 5.1.3 Example - Elk Data Task You will be given a data set of habitat use of Elks in Canada. Measured is the presence of Elks (0/1), and a number of other predictors. Perform either: A predictive analysis, i.e. a model to predict where Elks can be found. A causal analysis, trying to understand the effect of roads on Elk presence. Solution a b 5.2 Dispersion Problems in GLMs First of all: all other comments (causal structure, checking for misfit of the model) that we discussed for LMs also apply for GLMs in general, and you should check models for those problems. The reason that I concentrate here on dispersion problems is that those are different in GLMs than in normal LMs, so this is an issue that comes on top of the other things. GLMs have more problems with dispersion because standard GLM distributions such as the Poisson or the Binomial (for k/n data) do not have a parameter for adjusting the spread of the observed data around the regression line (dispersion). Thus, unlike the normal distribution, which can have different levels of spread around the regression line, the Poisson distribution always assumes a certain mean corresponds to a fixed variance. This is obviously not always a good assumption. In most cases with count data, we actually find overdispersion (more dispersion than expected). You can, however, also have underdispersion, i.e. less dispersion than expected. Ways to treat this include Quasi-distributions, which are available in glm. Those add a term to the likelihood that corrects the p-values for the dispersion, but they are not distributions .-&gt; Can’t check residuals, no AIC. -&gt; Discouraged. Observation-level random effect (OLRE) - Add a separate random effect per observation. This effectively creates a normal random variate at the level of the linear predictor, increases variance on the responses. A GLM distribution with variable dispersion, for Poisson usually the negative binomial. Because the 3rd option gives us more possibilities to model e.g. heteroskedasticity later, its preferable over an OLRE. I would always recommend the third option. Example: library(glmmTMB) library(lme4) library(DHARMa) m1 = glm(count ~ spp + mined, family = poisson, data = Salamanders) summary(m1) ## ## Call: ## glm(formula = count ~ spp + mined, family = poisson, data = Salamanders) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8155 -1.0024 -0.7241 0.0315 9.9255 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.33879 0.13811 -9.694 &lt; 2e-16 *** ## sppPR -1.38629 0.21517 -6.443 1.17e-10 *** ## sppDM 0.23052 0.12889 1.789 0.0737 . ## sppEC-A -0.77011 0.17105 -4.502 6.73e-06 *** ## sppEC-L 0.62117 0.11931 5.206 1.92e-07 *** ## sppDES-L 0.67916 0.11813 5.749 8.96e-09 *** ## sppDF 0.08004 0.13344 0.600 0.5486 ## minedno 2.03676 0.11092 18.363 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2120.7 on 643 degrees of freedom ## Residual deviance: 1310.3 on 636 degrees of freedom ## AIC: 2049.6 ## ## Number of Fisher Scoring iterations: 6 res = simulateResiduals(m1, plot = T) # Looks overdispersed, additional check. testDispersion(res) ## ## DHARMa nonparametric dispersion test via sd of residuals fitted vs. ## simulated ## ## data: simulationOutput ## dispersion = 3.9152, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided # Add random effect for site. m2 = glmer(count ~ spp + mined + (1|site), family = poisson, data = Salamanders) summary(m2) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: count ~ spp + mined + (1 | site) ## Data: Salamanders ## ## AIC BIC logLik deviance df.resid ## 1962.8 2003.0 -972.4 1944.8 635 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6006 -0.7446 -0.4143 0.0836 11.7241 ## ## Random effects: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.3313 0.5756 ## Number of obs: 644, groups: site, 23 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.62411 0.23853 -6.809 9.84e-12 *** ## sppPR -1.38623 0.21416 -6.473 9.61e-11 *** ## sppDM 0.23048 0.12829 1.797 0.0724 . ## sppEC-A -0.77012 0.17026 -4.523 6.09e-06 *** ## sppEC-L 0.62111 0.11875 5.230 1.69e-07 *** ## sppDES-L 0.67911 0.11758 5.776 7.66e-09 *** ## sppDF 0.08004 0.13282 0.603 0.5468 ## minedno 2.26373 0.27869 8.123 4.56e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) sppPR sppDM spEC-A spEC-L sDES-L sppDF ## sppPR -0.180 ## sppDM -0.300 0.334 ## sppEC-A -0.226 0.252 0.420 ## sppEC-L -0.324 0.361 0.602 0.454 ## sppDES-L -0.327 0.364 0.608 0.458 0.657 ## sppDF -0.290 0.322 0.538 0.406 0.582 0.587 ## minedno -0.733 0.000 0.000 0.000 0.000 0.000 0.000 res = simulateResiduals(m2, plot = T) # Now dispersion seems to be OK, rather another problem with heteroskedasticity, see next. # Just for the sake of completeness, if we would have still overdispersion, # these would be the two options: # Variable dispersion via OLRE. Salamanders$ID = 1:nrow(Salamanders) m3 = glmer(count ~ spp + mined + (1|site) + (1|ID), family = poisson, data = Salamanders) summary(m3) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: count ~ spp + mined + (1 | site) + (1 | ID) ## Data: Salamanders ## ## AIC BIC logLik deviance df.resid ## 1671.5 1716.2 -825.8 1651.5 634 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.122 -0.458 -0.286 0.139 2.736 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 1.0111 1.0055 ## site (Intercept) 0.2459 0.4958 ## Number of obs: 644, groups: ID, 644; site, 23 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.1808 0.2838 -7.684 1.54e-14 *** ## sppPR -1.4120 0.3065 -4.606 4.10e-06 *** ## sppDM 0.3801 0.2354 1.615 0.106389 ## sppEC-A -0.7762 0.2710 -2.864 0.004185 ** ## sppEC-L 0.5243 0.2332 2.248 0.024561 * ## sppDES-L 0.8157 0.2279 3.579 0.000344 *** ## sppDF 0.2856 0.2386 1.197 0.231184 ## minedno 2.3197 0.2723 8.517 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) sppPR sppDM spEC-A spEC-L sDES-L sppDF ## sppPR -0.307 ## sppDM -0.477 0.405 ## sppEC-A -0.364 0.360 0.460 ## sppEC-L -0.474 0.412 0.541 0.467 ## sppDES-L -0.504 0.418 0.557 0.475 0.560 ## sppDF -0.480 0.400 0.532 0.455 0.537 0.553 ## minedno -0.658 -0.023 0.029 -0.015 0.026 0.042 0.037 ## optimizer (Nelder_Mead) convergence code: 0 (OK) ## Model failed to converge with max|grad| = 0.103661 (tol = 0.002, component 1) res = simulateResiduals(m3, plot = T) # Variable dispersion via negative binomial. m4 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, data = Salamanders) summary(m4) ## Family: nbinom2 ( log ) ## Formula: count ~ spp + mined + (1 | site) ## Data: Salamanders ## ## AIC BIC logLik deviance df.resid ## 1672.4 1717.1 -826.2 1652.4 634 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.2945 0.5426 ## Number of obs: 644, groups: site, 23 ## ## Dispersion parameter for nbinom2 family (): 0.942 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6832 0.2742 -6.140 8.28e-10 *** ## sppPR -1.3197 0.2875 -4.591 4.42e-06 *** ## sppDM 0.3686 0.2235 1.649 0.099056 . ## sppEC-A -0.7098 0.2530 -2.806 0.005017 ** ## sppEC-L 0.5714 0.2191 2.608 0.009105 ** ## sppDES-L 0.7929 0.2166 3.660 0.000252 *** ## sppDF 0.3120 0.2329 1.340 0.180337 ## minedno 2.2633 0.2838 7.975 1.53e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 res = simulateResiduals(m4, plot = T) 5.2.1 Heteroskedasticity in GLMMs GLM(M)s can be heteroskedastic as well, i.e. dispersion depends on some predictors. In glmmTMB.{R}, you can make the dispersion of the negative Binomial dependent on a formula via the dispformula.{R} argument, in the same way as in nlme.{R} for the linear model. Variance problems would show up when plotting residuals against predicted and predictors. On the previous page, we saw some variance problems in the Salamander model. We could add a variable dispersion model via m3 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom1, dispformula = ~ spp + mined , data = Salamanders) summary(m3) ## Family: nbinom1 ( log ) ## Formula: count ~ spp + mined + (1 | site) ## Dispersion: ~spp + mined ## Data: Salamanders ## ## AIC BIC logLik deviance df.resid ## 1654.4 1730.3 -810.2 1620.4 627 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.2283 0.4778 ## Number of obs: 644, groups: site, 23 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.5288 0.2799 -5.462 4.70e-08 *** ## sppPR -1.3304 0.3480 -3.822 0.000132 *** ## sppDM 0.2695 0.2004 1.345 0.178561 ## sppEC-A -0.7525 0.2772 -2.714 0.006641 ** ## sppEC-L 0.6228 0.2109 2.952 0.003155 ** ## sppDES-L 0.7113 0.1976 3.600 0.000318 *** ## sppDF 0.1470 0.2171 0.677 0.498259 ## minedno 2.1348 0.2825 7.557 4.14e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Dispersion model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2834 0.6414 -0.442 0.6586 ## sppPR 0.3160 0.7501 0.421 0.6735 ## sppDM 0.1979 0.5712 0.346 0.7289 ## sppEC-A 0.3592 0.6477 0.554 0.5792 ## sppEC-L 1.0830 0.5215 2.077 0.0378 * ## sppDES-L 0.7951 0.5370 1.481 0.1387 ## sppDF 0.3769 0.6109 0.617 0.5373 ## minedno 0.5583 0.4187 1.334 0.1823 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 res = simulateResiduals(m3, plot = T) par(mfrow = c(1, 2)) plotResiduals(res, Salamanders$spp) plotResiduals(res, Salamanders$mined) 5.2.2 Zero-inflation Another common problem in count data (Poisson / negative binomial), but also other GLMs (e.g. beta) is that the observed data has more zeros than expected by the fitted distribution. To deal with this zero-inflation, we have to add an additional model component that controls how many zeros are produced. The default way to do this is assuming two separate processes which act after one another: A binomial model for 0 or not, if is not zero, a number from Poisson or negative binomial. Note that the result of 2. can again be zero, so there are two explanations for a zero in the data. Zero-inflated GLMMs can, for example, be fit with the glmmTMB.{R} package, using ziformula = ~ 0. How to check for zero-inflation Important: Do not check for zero-inflation in the response. DHARMa.{R} has a function for testing zero-inflation: m4 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, data = Salamanders) summary(m4) ## Family: nbinom2 ( log ) ## Formula: count ~ spp + mined + (1 | site) ## Data: Salamanders ## ## AIC BIC logLik deviance df.resid ## 1672.4 1717.1 -826.2 1652.4 634 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.2945 0.5426 ## Number of obs: 644, groups: site, 23 ## ## Dispersion parameter for nbinom2 family (): 0.942 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6832 0.2742 -6.140 8.28e-10 *** ## sppPR -1.3197 0.2875 -4.591 4.42e-06 *** ## sppDM 0.3686 0.2235 1.649 0.099056 . ## sppEC-A -0.7098 0.2530 -2.806 0.005017 ** ## sppEC-L 0.5714 0.2191 2.608 0.009105 ** ## sppDES-L 0.7929 0.2166 3.660 0.000252 *** ## sppDF 0.3120 0.2329 1.340 0.180337 ## minedno 2.2633 0.2838 7.975 1.53e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 res = simulateResiduals(m4, plot = T) testZeroInflation(res) ## ## DHARMa zero-inflation test via comparison to expected zeros with ## simulation under H0 = fitted model ## ## data: simulationOutput ## ratioObsSim = 1.0172, p-value = 0.744 ## alternative hypothesis: two.sided This shows no sign of zero-inflation. Problem with this test: When there is really zero-inflation, variable dispersion models such as the negative Binomial often simply increase the dispersion to account for the zeros, leading to no apparent zero-inflation in the residuals, but rather underdispersion. Thus, for zero-inflation, model selection, or simply fitting a ZIP model is often more reliable than residual checks. You can compare a zero-inflation model via AIC or likelihood ratio test to your base model, or simply check if the ZIP term in glmmTMB is significant. m5 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, ziformula = ~1, data = Salamanders) summary(m5) ## Family: nbinom2 ( log ) ## Formula: count ~ spp + mined + (1 | site) ## Zero inflation: ~1 ## Data: Salamanders ## ## AIC BIC logLik deviance df.resid ## 1674.4 1723.5 -826.2 1652.4 633 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.2944 0.5426 ## Number of obs: 644, groups: site, 23 ## ## Dispersion parameter for nbinom2 family (): 0.942 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6832 0.2742 -6.140 8.28e-10 *** ## sppPR -1.3197 0.2875 -4.591 4.42e-06 *** ## sppDM 0.3686 0.2235 1.649 0.099047 . ## sppEC-A -0.7098 0.2530 -2.806 0.005016 ** ## sppEC-L 0.5714 0.2191 2.608 0.009105 ** ## sppDES-L 0.7929 0.2166 3.660 0.000252 *** ## sppDF 0.3120 0.2329 1.340 0.180329 ## minedno 2.2633 0.2838 7.975 1.53e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -16.41 4039.11 -0.004 0.997 In this case, we have no evidence for zero-inflation. To see an example where you can find zero-inflation, do the Owl case study below. 5.3 Case Studies Strategy for analysis: Define formula via scientific questions + confounders. Define type of GLM (lm, logistic, Poisson). Blocks in data -&gt; Random effects, start with random intercept. Fit this base model, then do residual checks for Wrong functional form -&gt; Change fitted function. Wrong distribution-&gt; Transformation or GLM adjustment. (Over)dispersion -&gt; Variable dispersion GLM. Heteroskedasticity -&gt; Model dispersion. Zero-inflation -&gt; Add ZIP term. … And adjust the model accordingly. 5.3.1 Hurricanes Task In https://www.pnas.org/content/111/24/8782, Jung et al. claim that “Female hurricanes are deadlier than male hurricanes”. Specifically, they analyze the number of hurricane fatalities, and claim that there is an effect of the femininity of the name on the number of fatalities, correcting for several possible confounders. They interpret the result as causal (including mediators), claiming that giving only male names to hurricanes would considerably reduce death toll. The data is available in DHARMa. library(DHARMa) library(mgcv) str(hurricanes) ## tibble [92 × 14] (S3: tbl_df/tbl/data.frame) ## $ Year : num [1:92] 1950 1950 1952 1953 1953 ... ## $ Name : chr [1:92] &quot;Easy&quot; &quot;King&quot; &quot;Able&quot; &quot;Barbara&quot; ... ## $ MasFem : num [1:92] 6.78 1.39 3.83 9.83 8.33 ... ## $ MinPressure_before : num [1:92] 958 955 985 987 985 960 954 938 962 987 ... ## $ Minpressure_Updated_2014: num [1:92] 960 955 985 987 985 960 954 938 962 987 ... ## $ Gender_MF : num [1:92] 1 0 0 1 1 1 1 1 1 1 ... ## $ Category : num [1:92] 3 3 1 1 1 3 3 4 3 1 ... ## $ alldeaths : num [1:92] 2 4 3 1 0 60 20 20 0 200 ... ## $ NDAM : num [1:92] 1590 5350 150 58 15 ... ## $ Elapsed_Yrs : num [1:92] 63 63 61 60 60 59 59 59 58 58 ... ## $ Source : chr [1:92] &quot;MWR&quot; &quot;MWR&quot; &quot;MWR&quot; &quot;MWR&quot; ... ## $ ZMasFem : num [1:92] -0.000935 -1.670758 -0.913313 0.945871 0.481075 ... ## $ ZMinPressure_A : num [1:92] -0.356 -0.511 1.038 1.141 1.038 ... ## $ ZNDAM : num [1:92] -0.439 -0.148 -0.55 -0.558 -0.561 ... Some plots: plot(hurricanes$MasFem, hurricanes$NDAM, cex = 0.5, pch = 5) points(hurricanes$MasFem, hurricanes$NDAM, cex = hurricanes$alldeaths/20, pch = 4, col= &quot;red&quot;) The original model from the paper fits a negative binomial, using mgcv.{R}. originalModelGAM = gam(alldeaths ~ MasFem * (Minpressure_Updated_2014 + NDAM), data = hurricanes, family = nb, na.action = &quot;na.fail&quot;) summary(originalModelGAM) ## ## Family: Negative Binomial(0.736) ## Link function: log ## ## Formula: ## alldeaths ~ MasFem * (Minpressure_Updated_2014 + NDAM) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.014e+01 2.003e+01 3.502 0.000462 *** ## MasFem -5.986e+00 2.529e+00 -2.367 0.017927 * ## Minpressure_Updated_2014 -7.008e-02 2.060e-02 -3.402 0.000669 *** ## NDAM -3.845e-05 2.945e-05 -1.305 0.191735 ## MasFem:Minpressure_Updated_2014 6.124e-03 2.603e-03 2.352 0.018656 * ## MasFem:NDAM 1.593e-05 3.756e-06 4.242 2.21e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = -3.61e+03 Deviance explained = 57.4% ## -REML = 357.56 Scale est. = 1 n = 92 Tasks: Confirm that you get the same results as in the paper. Have a look at the ?hurricanes to see a residual analysis of the model in the paper Forget what they did. Go back to start, do a causal analysis like we did, and do your own model, diagnosing all residual problems that we discussed. Do you think there is an effect of femininity? Solution 5.3.2 Researchers Degrees of Freedom — Skin Color and Red Cards In 2018 Silberzahn et al. published a “meta analysis” in Advances in Methods and Practices in Psychological Science, where they had provided 29 teams with the same data set to answer one research question: “[W]hether soccer players with dark skin tone are more likely than those with light skin tone to receive red cards from referees”. Spoiler: They found that the “[a]nalytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units”, highlighting that different approaches in data analysis can yield significant variation in the results. You can find the paper “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results” at: https://journals.sagepub.com/doi/10.1177/2515245917747646. Task Do a re-analysis of the data as if you were the 30th team to contribute the results to the meta analysis. Download the data file “CrowdstormingDataJuly1st.csv” here: https://osf.io/fv8c3/. Variable explanations are provided in the README: https://osf.io/9yh4x/. Analyze the data. Given the research question, the selected variables are: Response variable: ‘redCards’ (+‘yellowReds’?). Multiple variables, potentially accounting for confounding, offsetting, grouping, … are included in the data. primary predictors: ‘rater1’, ‘rater2’ These variables reflect ratings of “two independent raters blind to the research question who, based on their profile photo, categorized players on a 5-point scale ranging from (1) very light skin to (5) very dark skin. Make sure that ‘rater1’ and ‘rater2’ are rescaled to the range 0 … 1 as described in the paper (“This variable was rescaled to be bounded by 0 (very light skin) and 1 (very dark skin) prior to the final analysis, to ensure consistency of effect sizes across the teams of analysts. The raw ratings were rescaled to 0, .25, .50, .75, and 1 to create this new scale.”) Research the concept of odd ratios and convert your effect estimate into this format. Are your results within the range of estimates from the 29 teams in Silberzahn et al. (2018)? Have a look at the other modelling teams. Do you understand the models they fit? Solution 5.3.3 Ants The paper available here uses a binomial GLMM to analyze the directional decision taken by ants in a Y-maze. Tasks: download the data in the paper re-implement the model, based on the description in the paper check model assumptions, residuals, and all that. Do you agree with the analysis? 5.3.4 Owls Task Look at the Owl data set in the glmmTMB.{R} package. The initial hypothesis is library(glmmTMB) m1 = glm(SiblingNegotiation ~ FoodTreatment*SexParent + offset(log(BroodSize)), data = Owls , family = poisson) res = simulateResiduals(m1) plot(res) The offset is a special command that can be used in all regression models. It means that we include an effect with effect size 1. The offset has a special importance in models with a log link function, because with these models, we have y = exp(x …), so if you do y = exp(x + log(BroodSize) ) and use exp rules, this is y = exp(x) * exp(log(BroodSize)) = y = exp(x) * BroodSize, so this makes the response proportional to BroodSize. This trick is often used in log link GLMs to make the response proportional to Area, Sampling effort, etc. Now, try to improve the model with everything we have discussed so far. Possible solution m1 = glmmTMB::glmmTMB(SiblingNegotiation ~ FoodTreatment * SexParent + (1|Nest) + offset(log(BroodSize)), data = Owls , family = nbinom1, dispformula = ~ FoodTreatment + SexParent, ziformula = ~ FoodTreatment + SexParent) summary(m1) ## Family: nbinom1 ( log ) ## Formula: ## SiblingNegotiation ~ FoodTreatment * SexParent + (1 | Nest) + ## offset(log(BroodSize)) ## Zero inflation: ~FoodTreatment + SexParent ## Dispersion: ~FoodTreatment + SexParent ## Data: Owls ## ## AIC BIC logLik deviance df.resid ## 3354.6 3402.9 -1666.3 3332.6 588 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## Nest (Intercept) 0.0876 0.296 ## Number of obs: 599, groups: Nest, 27 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.80028 0.09736 8.220 &lt; 2e-16 *** ## FoodTreatmentSatiated -0.46893 0.16760 -2.798 0.00514 ** ## SexParentMale -0.09127 0.09247 -0.987 0.32363 ## FoodTreatmentSatiated:SexParentMale 0.13087 0.19028 0.688 0.49158 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.9132 0.3269 -5.853 4.84e-09 *** ## FoodTreatmentSatiated 1.0564 0.4072 2.594 0.00948 ** ## SexParentMale -0.4688 0.3659 -1.281 0.20012 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Dispersion model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.2122 0.2214 5.475 4.37e-08 *** ## FoodTreatmentSatiated 0.7978 0.2732 2.920 0.0035 ** ## SexParentMale -0.1540 0.2399 -0.642 0.5209 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 res = simulateResiduals(m1, plot = T) testDispersion(m1) ## ## DHARMa nonparametric dispersion test via sd of residuals fitted vs. ## simulated ## ## data: simulationOutput ## dispersion = 0.78311, p-value = 0.104 ## alternative hypothesis: two.sided testZeroInflation(m1) ## ## DHARMa zero-inflation test via comparison to expected zeros with ## simulation under H0 = fitted model ## ## data: simulationOutput ## ratioObsSim = 1.0465, p-value = 0.608 ## alternative hypothesis: two.sided "],["correlation.html", "6 Correlation structures 6.1 General Idea 6.2 Temporal Correlation Structures 6.3 Spatial Correlation Structures 6.4 Phylogenetic Structures (PGLS) 6.5 Case studies", " 6 Correlation structures This chapter explains how to model correlation structures in the residuals. 6.1 General Idea Except for the random effects, we have so far assumed that observations are independent. However, there are a number of other common correlation structures that we may want to consider. Here a visualization from Roberts et al., 2016 (reproduced as OA, copyright: the authors). The figure shows random effects, and a number of other correlation structures. In random effects, residuals are structured in groups. All of the other three correlation structures discussed here are different. They are distance-based correlations between data points. Distance is expressed, e.g., by: Spatial distance. Temporal distance. Phylogenetic distance. For either of these structures, there can be two phenomena that lead to correlations: There can be a trend in the given space (e.g. time, space), which we have to remove first. After accounting for the trend, there can be a so-called autocorrelation between data points. The idea of the so-called conditional autoregressive (CAR) structures is, that we make parametric assumptions for how the correlation between data points falls off with distance. Then, we fit the model with this structure. Similar as for the variance modelling, we can add this structures either in nlme::gls, see https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html, or in glmmTMB, see https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html. The following pages provide examples and further comments on how to do this. 6.2 Temporal Correlation Structures In principle, spatial and temporal correlation are quite similar, there are 2 options we can have: There is a spatial trend in time / space, which creates a correlation in space / time. There truly is a spatial correlation, after accounting for the trend. Unfortunately, the distinction between a larger trend and a correlation is quite fluid. Nevertheless, one should always first check for and remove the trend, typically by including time/space as a predictor, potentially in a flexible way (GAMs come in handy). After this is done, we can fit a model with a temporally/spatially correlated error. As our first example, I look at the hurricane study from yesterday, which is, after all, temporal data. This data set is located in DHARMa. library(glmmTMB) library(DHARMa) originalModelGAM = glmmTMB(alldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) + scale(NDAM)), data = hurricanes, family = nbinom2) # Residual checks with DHARMa. res = simulateResiduals(originalModelGAM) ## Warning in TMB::openmp(parallel): OpenMP not supported. ## Warning in TMB::openmp(n = n_orig): OpenMP not supported. ## Warning in TMB::openmp(parallel): OpenMP not supported. ## Warning in TMB::openmp(n = n_orig): OpenMP not supported. plot(res) # No significant deviation in the general plot, but try this, which was highlighted by # https://www.theguardian.com/science/grrlscientist/2014/jun/04/hurricane-gender-name-bias-sexism-statistics plotResiduals(res, hurricanes$NDAM) # We also find temporal autocorrelation. res2 = recalculateResiduals(res, group = hurricanes$Year) testTemporalAutocorrelation(res2, time = unique(hurricanes$Year)) ## ## Durbin-Watson test ## ## data: simulationOutput$scaledResiduals ~ 1 ## DW = 2.5518, p-value = 0.04758 ## alternative hypothesis: true autocorrelation is not 0 A second example from Pinheiro and Bates, pp. 255-258. The data originates from Vonesh and Carter (1992), who describe data measured on high-flux hemodialyzers to assess their in vivo ultrafiltration characteristics. The ultrafiltration rates (in mL/hr) of 20 high-flux dialyzers were measured at seven different transmembrane pressures (in dmHg). The in vitro evaluation of the dialyzers used bovine blood at flow rates of either 200~dl/min or 300~dl/min. The data, are also analyzed in Littell, Milliken, Stroup and Wolfinger (1996). See ?Dialyzer for explanation of the variables (data comes with the package nlme.{R}). The data highlights the flexibility of gls for structured ( 1| subject) temporal data. Unfortunately, nlme.{R} does not interface with DHARMa.{R}. library(nlme) fm1Dial.gls = gls(rate ~(pressure + I(pressure^2) + I(pressure^3) + I(pressure^4))*QB, data = Dialyzer) plot(fm1Dial.gls) fm2Dial.gls = update(fm1Dial.gls, weights = varPower(form = ~ pressure)) plot(fm2Dial.gls) fm3Dial.gls = update(fm2Dial.gls, corr = corAR1(0.771, form = ~ 1 | Subject)) summary(fm3Dial.gls) ## Generalized least squares fit by REML ## Model: rate ~ (pressure + I(pressure^2) + I(pressure^3) + I(pressure^4)) * QB ## Data: Dialyzer ## AIC BIC logLik ## 642.6746 679.9526 -308.3373 ## ## Correlation Structure: AR(1) ## Formula: ~1 | Subject ## Parameter estimate(s): ## Phi ## 0.7526038 ## Variance function: ## Structure: Power of variance covariate ## Formula: ~pressure ## Parameter estimates: ## power ## 0.5182386 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) -16.81845 1.050536 -16.009405 0.0000 ## pressure 92.33424 5.266862 17.531167 0.0000 ## I(pressure^2) -49.26516 6.995059 -7.042851 0.0000 ## I(pressure^3) 11.39968 3.454779 3.299683 0.0012 ## I(pressure^4) -1.01964 0.558637 -1.825226 0.0703 ## QB300 -1.59419 1.598447 -0.997336 0.3205 ## pressure:QB300 1.70543 7.757062 0.219855 0.8263 ## I(pressure^2):QB300 2.12680 10.147281 0.209593 0.8343 ## I(pressure^3):QB300 0.47971 4.968707 0.096547 0.9232 ## I(pressure^4):QB300 -0.22064 0.799379 -0.276019 0.7830 ## ## Correlation: ## (Intr) pressr I(p^2) I(p^3) I(p^4) QB300 p:QB30 I(^2): ## pressure -0.891 ## I(pressure^2) 0.837 -0.959 ## I(pressure^3) -0.773 0.895 -0.981 ## I(pressure^4) 0.718 -0.838 0.946 -0.990 ## QB300 -0.657 0.585 -0.550 0.508 -0.472 ## pressure:QB300 0.605 -0.679 0.651 -0.608 0.569 -0.900 ## I(pressure^2):QB300 -0.577 0.661 -0.689 0.676 -0.652 0.845 -0.960 ## I(pressure^3):QB300 0.538 -0.622 0.682 -0.695 0.688 -0.780 0.898 -0.982 ## I(pressure^4):QB300 -0.502 0.586 -0.661 0.692 -0.699 0.724 -0.840 0.947 ## I(^3): ## pressure ## I(pressure^2) ## I(pressure^3) ## I(pressure^4) ## QB300 ## pressure:QB300 ## I(pressure^2):QB300 ## I(pressure^3):QB300 ## I(pressure^4):QB300 -0.990 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.44570115 -0.67381573 0.07715872 0.68039816 2.21585297 ## ## Residual standard error: 3.046316 ## Degrees of freedom: 140 total; 130 residual 6.3 Spatial Correlation Structures We will use a data set with the thickness of coal seams, that we try to predict with a spatial (soil) predictor. Read in data library(EcoData) library(DHARMa) library(gstat) plot(thick ~ soil, data = thickness) fit = lm(thick ~ soil, data = thickness) summary(fit) ## ## Call: ## lm(formula = thick ~ soil, data = thickness) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0414 -1.1975 0.0876 1.4836 4.9584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.9420 3.1570 10.118 1.54e-15 *** ## soil 2.2552 0.8656 2.605 0.0111 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.278 on 73 degrees of freedom ## Multiple R-squared: 0.08508, Adjusted R-squared: 0.07254 ## F-statistic: 6.788 on 1 and 73 DF, p-value: 0.01111 # Quantile residuals are not actually needed in this case but # DHARMa includes a test for spatial autocorrelation which # will save us coding time res = simulateResiduals(fit) testSpatialAutocorrelation(res, x = thickness$north, y = thickness$east) ## ## DHARMa Moran&#39;s I test for distance-based autocorrelation ## ## data: res ## observed = 0.210870, expected = -0.013514, sd = 0.021940, p-value &lt; ## 2.2e-16 ## alternative hypothesis: Distance-based autocorrelation # Looking also at the directional variogram tann.dir.vgm = variogram(residuals(fit) ~ 1, loc =~ east + north, data = thickness, alpha = c(0, 45, 90, 135)) plot(tann.dir.vgm) Remove trend via a GAM: library(mgcv) library(modEvA) fit1 = gam(thick ~ soil + te(east, north) , data = spdata) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## thick ~ soil + te(east, north) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.68933 0.26498 149.780 &lt;2e-16 *** ## soil 0.12363 0.07275 1.699 0.0952 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## te(east,north) 21.09 22.77 721.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.996 Deviance explained = 99.7% ## GCV = 0.033201 Scale est. = 0.022981 n = 75 plot(fit1, pages = 0, lwd = 2) col = colorRamp(c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;))(range01(residuals(fit1))) plot(spdata$east, spdata$north, col = rgb(col, maxColorValue = 255) ) Almost the same, but simpler: fit = lm(thick ~ soil + north + I(north^2), data = spdata) Alternatively, fit an autoregressive model. Of course, both options can be combined. fit2 = gls(thick ~ soil , correlation = corExp(form =~ east + north) , data = spdata) summary(fit2) ## Generalized least squares fit by REML ## Model: thick ~ soil ## Data: spdata ## AIC BIC logLik ## 164.3474 173.5092 -78.17368 ## ## Correlation Structure: Exponential spatial correlation ## Formula: ~east + north ## Parameter estimate(s): ## range ## 719.4121 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 42.81488 5.314541 8.056176 0.0000 ## soil 0.02662 0.199737 0.133289 0.8943 ## ## Correlation: ## (Intr) ## soil -0.12 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.5811122 -0.7276873 -0.5028102 -0.2092991 0.3217326 ## ## Residual standard error: 5.573087 ## Degrees of freedom: 75 total; 73 residual fit1 = gls(thick ~ soil + north + I(north^2), data = spdata) anova(fit1, fit2) ## Model df AIC BIC logLik Test L.Ratio p-value ## fit1 1 5 278.7468 290.0602 -134.37340 ## fit2 2 4 164.3474 173.5092 -78.17368 1 vs 2 112.3994 &lt;.0001 6.3.1 Exercise Task Use the dataset EcoData::plantcounts. Our scientific question is if richness ~ agrarea. Help on the dataset, as well as a few initial plots, is in the help of ?plantcounts. This is count data, so start with a Poisson or Neg Binom GLM. The quadrats are not all equally sized, so you should include an offest to account for area. Then, check for spatial autocorrelation. If you find autocorrelation that cannot be removed with a gam, the problem is that the gls function that we have used so far only extends lm, and not glm models. In this case, you can either read up in https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html how to specify a spatial covariance in glmmTMB, or just log transform your counts + 1, and fit a gls. Solution library(EcoData) 6.4 Phylogenetic Structures (PGLS) This is mostly taken from https://lukejharmon.github.io/ilhabela/instruction/2015/07/03/PGLS/. The two datasets associated with this example are in the EcoData package. Perform analysis: library(EcoData) library(ape) library(geiger) library(nlme) library(phytools) library(DHARMa) To plot the phylogenetic tree, use plot(anolisTree) Regress species traits # Check whether names are matching in both files. name.check(anolisTree, anolisData) ## $tree_not_data ## [1] &quot;ahli&quot; &quot;alayoni&quot; &quot;alfaroi&quot; &quot;aliniger&quot; ## [5] &quot;allisoni&quot; &quot;allogus&quot; &quot;altitudinalis&quot; &quot;alumina&quot; ## [9] &quot;alutaceus&quot; &quot;angusticeps&quot; &quot;argenteolus&quot; &quot;argillaceus&quot; ## [13] &quot;armouri&quot; &quot;bahorucoensis&quot; &quot;baleatus&quot; &quot;baracoae&quot; ## [17] &quot;barahonae&quot; &quot;barbatus&quot; &quot;barbouri&quot; &quot;bartschi&quot; ## [21] &quot;bremeri&quot; &quot;breslini&quot; &quot;brevirostris&quot; &quot;caudalis&quot; ## [25] &quot;centralis&quot; &quot;chamaeleonides&quot; &quot;chlorocyanus&quot; &quot;christophei&quot; ## [29] &quot;clivicola&quot; &quot;coelestinus&quot; &quot;confusus&quot; &quot;cooki&quot; ## [33] &quot;cristatellus&quot; &quot;cupeyalensis&quot; &quot;cuvieri&quot; &quot;cyanopleurus&quot; ## [37] &quot;cybotes&quot; &quot;darlingtoni&quot; &quot;distichus&quot; &quot;dolichocephalus&quot; ## [41] &quot;equestris&quot; &quot;etheridgei&quot; &quot;eugenegrahami&quot; &quot;evermanni&quot; ## [45] &quot;fowleri&quot; &quot;garmani&quot; &quot;grahami&quot; &quot;guafe&quot; ## [49] &quot;guamuhaya&quot; &quot;guazuma&quot; &quot;gundlachi&quot; &quot;haetianus&quot; ## [53] &quot;hendersoni&quot; &quot;homolechis&quot; &quot;imias&quot; &quot;inexpectatus&quot; ## [57] &quot;insolitus&quot; &quot;isolepis&quot; &quot;jubar&quot; &quot;krugi&quot; ## [61] &quot;lineatopus&quot; &quot;longitibialis&quot; &quot;loysiana&quot; &quot;lucius&quot; ## [65] &quot;luteogularis&quot; &quot;macilentus&quot; &quot;marcanoi&quot; &quot;marron&quot; ## [69] &quot;mestrei&quot; &quot;monticola&quot; &quot;noblei&quot; &quot;occultus&quot; ## [73] &quot;olssoni&quot; &quot;opalinus&quot; &quot;ophiolepis&quot; &quot;oporinus&quot; ## [77] &quot;paternus&quot; &quot;placidus&quot; &quot;poncensis&quot; &quot;porcatus&quot; ## [81] &quot;porcus&quot; &quot;pulchellus&quot; &quot;pumilis&quot; &quot;quadriocellifer&quot; ## [85] &quot;reconditus&quot; &quot;ricordii&quot; &quot;rubribarbus&quot; &quot;sagrei&quot; ## [89] &quot;semilineatus&quot; &quot;sheplani&quot; &quot;shrevei&quot; &quot;singularis&quot; ## [93] &quot;smallwoodi&quot; &quot;strahmi&quot; &quot;stratulus&quot; &quot;valencienni&quot; ## [97] &quot;vanidicus&quot; &quot;vermiculatus&quot; &quot;websteri&quot; &quot;whitemani&quot; ## ## $data_not_tree ## [1] &quot;1&quot; &quot;10&quot; &quot;100&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; ## [13] &quot;2&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;3&quot; ## [25] &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;4&quot; &quot;40&quot; ## [37] &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;5&quot; &quot;50&quot; &quot;51&quot; ## [49] &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;6&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; ## [61] &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;7&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; ## [73] &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;8&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; ## [85] &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;9&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; ## [97] &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; # Plot traits. plot(anolisData[, c(&quot;awesomeness&quot;, &quot;hostility&quot;)]) plot(hostility ~ awesomeness, data = anolisData) fit = lm(hostility ~ awesomeness, data = anolisData) summary(fit) ## ## Call: ## lm(formula = hostility ~ awesomeness, data = anolisData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7035 -0.3065 -0.0416 0.2440 0.7884 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.10843 0.03953 2.743 0.00724 ** ## awesomeness -0.88116 0.03658 -24.091 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3807 on 98 degrees of freedom ## Multiple R-squared: 0.8555, Adjusted R-squared: 0.8541 ## F-statistic: 580.4 on 1 and 98 DF, p-value: &lt; 2.2e-16 abline(fit) Check for phylogenetic signal in residuals. # Calculate weight matrix for phylogenetic distance. w = 1/cophenetic(anolisTree) diag(w) = 0 Moran.I(residuals(fit), w) ## $observed ## [1] 0.05067625 ## ## $expected ## [1] -0.01010101 ## ## $sd ## [1] 0.00970256 ## ## $p.value ## [1] 3.751199e-10 Conclusion: signal in the residuals, a normal lm will not work. You can also check with DHARMa, using this works also for GLMMs res = simulateResiduals(fit) testSpatialAutocorrelation(res, distMat = cophenetic(anolisTree)) ## ## DHARMa Moran&#39;s I test for distance-based autocorrelation ## ## data: res ## observed = 0.0509093, expected = -0.0101010, sd = 0.0097304, p-value = ## 3.609e-10 ## alternative hypothesis: Distance-based autocorrelation An old-school method to deal with the problem are the so-called Phylogenetically Independent Contrasts (PICs) (Felsenstein, J. (1985) “Phylogenies and the comparative method”. American Naturalist, 125, 1–15.). The idea here is to transform your data in a way that an lm is still appropriate. For completeness, I show the method here. # Extract columns. host = anolisData[, &quot;hostility&quot;] awe = anolisData[, &quot;awesomeness&quot;] # Give them names. names(host) = names(awe) = rownames(anolisData) # Calculate PICs. hPic = pic(host, anolisTree) ## Warning in pic(host, anolisTree): the names of argument &#39;x&#39; and the tip labels ## of the tree did not match: the former were ignored in the analysis. aPic = pic(awe, anolisTree) ## Warning in pic(awe, anolisTree): the names of argument &#39;x&#39; and the tip labels of ## the tree did not match: the former were ignored in the analysis. # Make a model. picModel = lm(hPic ~ aPic - 1) summary(picModel) # Yes, significant. ## ## Call: ## lm(formula = hPic ~ aPic - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.30230 -0.23485 0.06003 0.34772 0.92222 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## aPic -0.91964 0.03887 -23.66 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4263 on 98 degrees of freedom ## Multiple R-squared: 0.851, Adjusted R-squared: 0.8495 ## F-statistic: 559.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 # plot results. plot(hPic ~ aPic) abline(a = 0, b = coef(picModel)) Now, new school, with a PGLS pglsModel = gls(hostility ~ awesomeness, correlation = corBrownian(phy = anolisTree, form =~ species), data = anolisData, method = &quot;ML&quot;) summary(pglsModel) ## Generalized least squares fit by maximum likelihood ## Model: hostility ~ awesomeness ## Data: anolisData ## AIC BIC logLik ## 42.26092 50.07643 -18.13046 ## ## Correlation Structure: corBrownian ## Formula: ~species ## Parameter estimate(s): ## numeric(0) ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 0.1158895 0.12500397 0.927087 0.3562 ## awesomeness -0.9196414 0.03886501 -23.662451 0.0000 ## ## Correlation: ## (Intr) ## awesomeness -0.065 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.49512017 -0.75193433 -0.06672209 0.56527753 2.04613817 ## ## Residual standard error: 0.4220369 ## Degrees of freedom: 100 total; 98 residual coef(pglsModel) ## (Intercept) awesomeness ## 0.1158895 -0.9196414 plot(hostility ~ awesomeness, data = anolisData) abline(pglsModel, col = &quot;red&quot;) OK, same result, but PGLS is WAY more flexible than PICs. For example, we can include a discrete predictor: pglsModel2 = gls(hostility ~ ecomorph, correlation = corBrownian(phy = anolisTree, form =~ species), data = anolisData, method = &quot;ML&quot;) summary(pglsModel2) ## Generalized least squares fit by maximum likelihood ## Model: hostility ~ ecomorph ## Data: anolisData ## AIC BIC logLik ## 235.1126 255.954 -109.5563 ## ## Correlation Structure: corBrownian ## Formula: ~species ## Parameter estimate(s): ## numeric(0) ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 0.2280018 0.3630767 0.6279713 0.5316 ## ecomorphGB -0.2737370 0.2128984 -1.2857635 0.2017 ## ecomorphT -0.2773801 0.3872137 -0.7163490 0.4756 ## ecomorphTC -0.5457771 0.2449466 -2.2281475 0.0283 ## ecomorphTG -0.2645627 0.2084928 -1.2689297 0.2076 ## ecomorphTW -0.5388436 0.2370223 -2.2733878 0.0253 ## ecomorphU -0.3013944 0.2264264 -1.3310922 0.1864 ## ## Correlation: ## (Intr) ecmrGB ecmrpT ecmrTC ecmrTG ecmrTW ## ecomorphGB -0.385 ## ecomorphT -0.276 0.360 ## ecomorphTC -0.369 0.626 0.349 ## ecomorphTG -0.426 0.638 0.431 0.608 ## ecomorphTW -0.372 0.626 0.377 0.588 0.641 ## ecomorphU -0.395 0.597 0.394 0.587 0.647 0.666 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.57909973 -0.62394508 0.03716963 0.49997446 2.33859983 ## ## Residual standard error: 1.05295 ## Degrees of freedom: 100 total; 93 residual anova(pglsModel2) ## Denom. DF: 93 ## numDF F-value p-value ## (Intercept) 1 0.0555807 0.8141 ## ecomorph 6 1.2170027 0.3046 # We can even include multiple predictors: pglsModel3 = gls(hostility ~ ecomorph * awesomeness, correlation = corBrownian(phy = anolisTree, form =~ species), data = anolisData, method = &quot;ML&quot;) summary(pglsModel3) ## Generalized least squares fit by maximum likelihood ## Model: hostility ~ ecomorph * awesomeness ## Data: anolisData ## AIC BIC logLik ## 53.36917 92.44673 -11.68459 ## ## Correlation Structure: corBrownian ## Formula: ~species ## Parameter estimate(s): ## numeric(0) ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 0.2740102 0.14336154 1.911323 0.0593 ## ecomorphGB -0.2079698 0.08757937 -2.374644 0.0198 ## ecomorphT -0.1751884 0.15478802 -1.131795 0.2609 ## ecomorphTC -0.2030466 0.10752002 -1.888454 0.0623 ## ecomorphTG -0.1260964 0.08339737 -1.511994 0.1342 ## ecomorphTW -0.1600076 0.09700188 -1.649531 0.1027 ## ecomorphU -0.1244498 0.09457082 -1.315943 0.1917 ## awesomeness -1.0131496 0.08971063 -11.293529 0.0000 ## ecomorphGB:awesomeness 0.0750120 0.08289316 0.904924 0.3680 ## ecomorphT:awesomeness 0.1373797 0.11770513 1.167152 0.2464 ## ecomorphTC:awesomeness 0.1161086 0.11490811 1.010447 0.3151 ## ecomorphTG:awesomeness 0.1666831 0.09824670 1.696577 0.0934 ## ecomorphTW:awesomeness 0.0120495 0.11532810 0.104480 0.9170 ## ecomorphU:awesomeness 0.0283477 0.10510376 0.269711 0.7880 ## ## Correlation: ## (Intr) ecmrGB ecmrpT ecmrTC ecmrTG ecmrTW ecmrpU awsmns ## ecomorphGB -0.398 ## ecomorphT -0.289 0.372 ## ecomorphTC -0.361 0.598 0.357 ## ecomorphTG -0.435 0.647 0.447 0.579 ## ecomorphTW -0.377 0.644 0.391 0.579 0.657 ## ecomorphU -0.403 0.589 0.424 0.546 0.658 0.666 ## awesomeness -0.104 0.123 0.045 0.078 0.046 0.005 0.108 ## ecomorphGB:awesomeness 0.129 -0.280 -0.095 -0.171 -0.151 -0.191 -0.184 -0.682 ## ecomorphT:awesomeness 0.082 -0.085 -0.074 -0.071 -0.036 -0.011 -0.111 -0.716 ## ecomorphTC:awesomeness 0.102 -0.120 -0.092 -0.359 -0.079 -0.091 -0.136 -0.695 ## ecomorphTG:awesomeness 0.090 -0.073 -0.023 -0.058 -0.056 -0.036 -0.140 -0.811 ## ecomorphTW:awesomeness 0.051 -0.124 0.029 -0.054 -0.023 -0.052 -0.006 -0.666 ## ecomorphU:awesomeness 0.101 -0.129 -0.129 -0.143 -0.133 -0.122 -0.283 -0.672 ## ecmGB: ecmrT: ecmTC: ecmTG: ecmTW: ## ecomorphGB ## ecomorphT ## ecomorphTC ## ecomorphTG ## ecomorphTW ## ecomorphU ## awesomeness ## ecomorphGB:awesomeness ## ecomorphT:awesomeness 0.516 ## ecomorphTC:awesomeness 0.519 0.530 ## ecomorphTG:awesomeness 0.611 0.684 0.609 ## ecomorphTW:awesomeness 0.535 0.536 0.482 0.569 ## ecomorphU:awesomeness 0.515 0.535 0.644 0.626 0.480 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.6656909 -0.7164061 -0.1305515 0.6718348 1.7699106 ## ## Residual standard error: 0.3956912 ## Degrees of freedom: 100 total; 86 residual anova(pglsModel3) ## Denom. DF: 86 ## numDF F-value p-value ## (Intercept) 1 0.3640 0.5479 ## ecomorph 6 7.9691 &lt;.0001 ## awesomeness 1 517.8319 &lt;.0001 ## ecomorph:awesomeness 6 0.8576 0.5295 We can also assume that the error structure follows an Ornstein-Uhlenbeck model rather than Brownian motion. When trying this, however, I noted that the model does not converge due to a scaling problem. We can do a quick fix by making the branch lengths longer. This will not affect the analysis other than rescaling a nuisance parameter. tempTree = anolisTree tempTree$edge.length = tempTree$edge.length * 100 pglsModelLambda = gls(hostility ~ awesomeness, correlation = corPagel(1, phy = tempTree, fixed = FALSE, form =~ species), data = anolisData, method = &quot;ML&quot;) summary(pglsModelLambda) ## Generalized least squares fit by maximum likelihood ## Model: hostility ~ awesomeness ## Data: anolisData ## AIC BIC logLik ## 43.64714 54.06782 -17.82357 ## ## Correlation Structure: corPagel ## Formula: ~species ## Parameter estimate(s): ## lambda ## 1.01521 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 0.1170472 0.12862370 0.909997 0.3651 ## awesomeness -0.9248858 0.03870928 -23.893129 0.0000 ## ## Correlation: ## (Intr) ## awesomeness -0.062 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.46625592 -0.74557818 -0.06456682 0.54645141 2.02371257 ## ## Residual standard error: 0.4317018 ## Degrees of freedom: 100 total; 98 residual pglsModelOU = gls(hostility ~ awesomeness, correlation = corMartins(1, phy = tempTree, form =~ species), data = anolisData) summary(pglsModelOU) ## Generalized least squares fit by REML ## Model: hostility ~ awesomeness ## Data: anolisData ## AIC BIC logLik ## 50.7625 61.10237 -21.38125 ## ## Correlation Structure: corMartins ## Formula: ~species ## Parameter estimate(s): ## alpha ## 0.003194918 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 0.1179388 0.4300640 0.274236 0.7845 ## awesomeness -0.9148437 0.0384949 -23.765320 0.0000 ## ## Correlation: ## (Intr) ## awesomeness -0.02 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.11558554 -0.54574106 -0.05696661 0.40461428 1.48285459 ## ## Residual standard error: 0.5740297 ## Degrees of freedom: 100 total; 98 residual Other example: http://schmitzlab.info/pgls.html. For fitting PGLS with various models, you should also consider the caper package. 6.4.1 Exercise Download the following two datasets http://www.phytools.org/Cordoba2017/data/BarbetTree.nex http://www.phytools.org/Cordoba2017/data/Barbetdata.csv These data are from a study by Corboda et al., 2017, which examined the influence of environmental factors on the evolution of song in an group of Asian bird species called “barbets.” Check if there is a relationship between altitude at which a species is found and the length of the note in its song, which uses the variables Lnote~Lnalt 6.5 Case studies 6.5.1 Snouter Task Fit one of the responses in the snouter datset against the predictors rain + djungle (see ?snouter). Check for spatial autocorrelation and proceed to fitting a spatial model if needed. See the data set’s help for details on the variables. Solution library(EcoData) str(snouter) ## &#39;data.frame&#39;: 1108 obs. of 34 variables: ## $ X : int 14 15 16 17 18 19 13 14 15 16 ... ## $ Y : int 1 1 1 1 1 1 2 2 2 2 ... ## $ rain : num 237 268 268 268 239 ... ## $ djungle : int 15 18 6 2 4 5 11 16 3 17 ... ## $ snouter1.1 : num 100.5 103.4 93.8 86.5 72.3 ... ## $ snouter1.2 : num 63.5 62.1 62.5 69.9 71 67.4 62.7 63.2 61.9 73.2 ... ## $ snouter1.3 : num 76.6 79.1 80.2 84.3 79.8 80.5 76.9 80.3 74.2 77.6 ... ## $ snouter1.4 : num 69.4 63 66.8 69.3 66.8 72.9 77.4 77.4 66.2 58.5 ... ## $ snouter1.5 : num 66 61.7 52.1 57.2 69.2 62.9 76.8 69.1 63.6 58.3 ... ## $ snouter1.6 : num 71 72.6 65.7 70.4 72.8 76.1 72.2 67.7 70.1 62.1 ... ## $ snouter1.7 : num 65.6 64.9 68.3 68.8 86.6 91.2 61.7 63.2 67.3 76.1 ... ## $ snouter1.8 : num 86.3 69.8 69.7 68.9 84.3 86.3 83.9 72.6 69.8 73.3 ... ## $ snouter1.9 : num 73.5 67 71.6 82.2 84.8 78.8 69.3 65.5 68.3 70.1 ... ## $ snouter1.10: num 64.3 68.6 67.7 69.4 90 87.4 65.8 69.7 70.6 76.6 ... ## $ snouter2.1 : int 1 1 1 1 1 0 1 1 1 1 ... ## $ snouter2.2 : int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter2.3 : int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter2.4 : int 1 1 1 1 1 1 1 1 1 0 ... ## $ snouter2.5 : int 1 1 0 0 1 1 1 1 1 0 ... ## $ snouter2.6 : int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter2.7 : int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter2.8 : int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter2.9 : int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter2.10: int 1 1 1 1 1 1 1 1 1 1 ... ## $ snouter3.1 : int 25 26 22 19 14 7 24 22 21 19 ... ## $ snouter3.2 : int 10 9 10 12 13 12 10 9 9 13 ... ## $ snouter3.3 : int 15 16 17 18 17 17 15 15 13 15 ... ## $ snouter3.4 : int 13 10 11 12 11 14 15 14 10 7 ... ## $ snouter3.5 : int 11 9 5 7 12 10 15 11 9 7 ... ## $ snouter3.6 : int 13 14 11 13 14 15 13 11 12 9 ... ## $ snouter3.7 : int 11 11 12 12 19 22 9 9 11 14 ... ## $ snouter3.8 : int 19 12 12 12 18 20 18 13 11 13 ... ## $ snouter3.9 : int 14 11 13 17 19 17 12 10 11 12 ... ## $ snouter3.10: int 10 12 12 12 21 20 11 11 12 14 ... 6.5.2 Covariance structures in glmmTMB gls only allows normally distributed responses. For GLMMs, you can use glmmTMB, which has (experimental) support for spatial, temporal or phylogenetic covariance structures on the REs. If you want to specific residual autocorrelation, you can create and observation-level RE and specify the covariance structure there. Take one of the examples that we had before (e.g. plantcount) and try to fit a spatial covariance with glmmTMB, using the tutorial here https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html Alternative packages for spatial models are MASS::glmmPQL, BRMS, or INLA. "],["advanced_topics.html", "7 Summary and advanced topics 7.1 Reminder: Modelling Strategy 7.2 Thoughts About the Analysis Pipeline 7.3 Nonparametric estimators 7.4 Structural Equation Models (SEMs) 7.5 Intro Bayes", " 7 Summary and advanced topics 7.1 Reminder: Modelling Strategy Things to note: For an lm, the link function is the identity function. Fixed effects \\(\\operatorname{f}(x)\\) can be either a polynomial \\(\\left( a \\cdot x = b \\right)\\) = linear regression, a nonlinear function = nonlinear regression, or a smooth spline = generalized additive model (GAM). Random effects assume normal distribution for groups. Random effects can also act on fixed effects (random slope). For an lm with correlation structure, C is integrated in Dist. For all other GLMMs, there is another distribution, plus the additional multivariate normal on the linear predictor. Strategy for analysis: Define formula via scientific questions + confounders. Define type of GLM (lm, logistic, Poisson). Blocks in data -&gt; Random effects, start with random intercept. Fit this base model, then do residual checks for Wrong functional form -&gt; Change fitted function. Wrong distribution-&gt; Transformation or GLM adjustment. (Over)dispersion -&gt; Variable dispersion GLM. Heteroskedasticity -&gt; Model dispersion. Zero-inflation -&gt; Add ZIP term. Correlation -&gt; Add correlation structure. And adjust the model accordingly. Packages: baseR.{R}: lm.{R}, glm.{R}. lme4.{R}: mixed models, lmer.{R}, glmer.{R}. mgcv.{R}: GAM. nlme.{R}: Variance and correlations structure modelling for linear (mixed) models, using gls.{R} + lme.{R}. glmmTMB.{R}: Generalized linear mixed models with variance / correlation modelling and zip term. 7.2 Thoughts About the Analysis Pipeline In statistics, we rarely use a simple analysis. We often use an entire pipeline, consisting, for example, of the protocol that I sketched in chapter 5.3. What we should constantly ask ourselves: Is our pipeline good? By “good”, we typically mean: If 1000 analyses are run in that way: What is the typical error of the estimate? What is the Type I error (false positives)? Are the confidence intervals correctly calculated? … The way to check this is to run simulations. For example, the following function creates data that follows the assumptions of a linear regression with slope 0.5, then fits a linear regression, and returns the estimate getEstimate = function(n = 100){ x = runif(n) y = 0.5 * x + rnorm(n) fit = lm(y ~ x) x = summary(fit) return(x$coefficients[2, 1]) # Get fitted x weight (should be ~0.5). } The replicate function allows us to execute this 1000 times: set.seed(543210) out = replicate(1000, getEstimate()) Plotting the result, we can check whether the linear regression is an unbiased estimator for the slope. hist(out, breaks = 50) abline(v = 0.5, col = &quot;red&quot;) “Unbiased” means that, while each single estimate will have some error, the mean of many estimates will spread around the true value. Explicitly calculating these values Bias mean(out) - 0.5 # Should be ~0. ## [1] -0.001826401 Variance / standard deviation of the estimator sd(out) ## [1] 0.3587717 To check p-values, we could run: set.seed(12345) getEstimate = function(n = 100){ # Mind: Function has changed! x = runif(n) y = rnorm(n) # No dependence of x! Identical: y = 0 * x + rnorm(100). fit = lm(y ~ x) x = summary(fit) return(x$coefficients[2, 4]) # P-value for H0: Weight of x = 0. } out = replicate(2000, getEstimate()) hist(out) # Expected: Uniformly distributed p-values. -&gt; Check. mean(out &lt; 0.05) # Expected: ~0.05. But this is NO p-value... Check H0/H1! ## [1] 0.0515 # Explanation of syntax: Logical vectors are interpreted as vectors of 0s and 1s. To check the properties of other, possibly more complicated pipelines, statisticians will typically use the same technique. I recommend doing this! For example, you could modify the function above to have a non-normal error. How much difference does that make? Simulating often beats recommendations in the books! 7.3 Nonparametric estimators 7.3.1 The bootstrap Standard (non-parametric) bootstrap The bootstrap is a method to generate approximate confidence intervals based on resampling the data. Imagine you have some kind of weird data distribution: set.seed(123) data = ifelse(rbinom(100, 1, 0.5) == 1, rexp(100, 4) , rnorm(100, -2)) hist(data) We want to calculate the mean and it’s uncertainty. The mean is simple, but what is the uncertainty of the mean? The standard error can’t be used, because this is not a normal distribution. If we don’t know the distribution, we can’t use a parametric method to calculate the confidence interval. The solution is the bootstrap. The idea is the following: We re-sample from the data to generate an estimation of the uncertainty of the mean. Let’s first do this by hand: set.seed(123) performBootstrap = function(){ resampledData = sample(data, size = length(data), replace = T) return(mean(resampledData)) } bootstrappedMean = replicate(500, performBootstrap()) hist(bootstrappedMean, breaks = 50) abline(v = mean(data), col = &quot;red&quot;) Roughly, this distribution is the confidence interval for the mean for this particular distribution. In detail, there are a few tricks to correct confidence intervals for the bootstrap, which are implemented in the boot.{R} package. Here is how you would do a boostrap with the boot package. The trick here is to implement the function f().{R}, which must take the data as well as a selection of data points “k” (for example c(1,3,4,5,8,9), or 1:20, etc.) as input, and calculate the desired statistics. library(boot) f = function(d, k){ mean(d[k]) } out = boot(data, f, 500) plot(out) boot.ci(out) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 500 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = out) ## ## Intervals : ## Level Normal Basic ## 95% (-1.2730, -0.7144 ) (-1.2755, -0.7177 ) ## ## Level Percentile BCa ## 95% (-1.2427, -0.6849 ) (-1.2699, -0.7177 ) ## Calculations and Intervals on Original Scale ## Some BCa intervals may be unstable Task Calculate a bootstrapped confidence interval for the mean of this exponential distribution. Compare it to the naive standard error: set.seed(1234) data = rexp(500) Solution Jacknife An alternative to the bootstrap is the jacknife. From Wikipedia: In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a data set and calculating the estimate and then finding the average of these calculations. Given a sample of size N, the jackknife estimate is found by aggregating the estimates of each N-1-sized sub-sample. The jackknife technique was developed by Maurice Quenouille (1949, 1956). John Tukey (1958) expanded on the technique and proposed the name “jackknife” since, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool. The jackknife is a linear approximation of the bootstrap. library(bootstrap) theta = function(x){ mean(x) } results = jackknife(data, theta) results$jack.se ## [1] 0.04727612 results$jack.bias ## [1] 0 Parametric bootstrap We call it a parametric bootstrap if we don’t re-sample the data to generate new data, but simulate from the fitted model. Simple example with a linear model: set.seed(123) x = runif(100, -2, 2) y = rnorm(100, 1 + 2*x, 1) dat = data.frame(x = x, y = y) m = lm(y ~ x) summary(m) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.23797 -0.61323 -0.01973 0.59633 2.21723 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.94612 0.09693 9.761 4e-16 *** ## x 1.97754 0.08546 23.141 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9693 on 98 degrees of freedom ## Multiple R-squared: 0.8453, Adjusted R-squared: 0.8437 ## F-statistic: 535.5 on 1 and 98 DF, p-value: &lt; 2.2e-16 We are interested in getting the confidence intervals for the coefficients of the model: resampledParameters = function(){ newData = dat newData$y = unlist(simulate(m)) mNew = lm(y ~ x, newData) return(coef(mNew)[1]) } bootstrappedIntercept = replicate(500, resampledParameters()) hist(bootstrappedIntercept, breaks = 50) abline(v = coef(m)[1], col = &quot;red&quot;) The same with the boot.{R} package. We need a statistics: foo = function(out){ m = lm(y ~ x, out) return(coef(m)) } and a function to create new data rgen = function(dat, mle){ out = dat out$y = unlist(simulate(mle)) return(out) } b2 = boot(dat, foo, R = 1000, sim = &quot;parametric&quot;, ran.gen = rgen, mle = m) boot.ci(b2, type = &quot;perc&quot;, index = 1) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = b2, type = &quot;perc&quot;, index = 1) ## ## Intervals : ## Level Percentile ## 95% ( 0.7534, 1.1287 ) ## Calculations and Intervals on Original Scale Application: Simulated likelihood ratio test The parametric bootstrap can be used to generate simulated likelihood ratio tests for mixed models. This allows us to test for the significance of variance components without specifying degrees of freedom. We could program this ourselves, but here is a package: library(pbkrtest) data(beets, package = &quot;pbkrtest&quot;) head(beets) ## harvest block sow yield sugpct ## 1 harv1 block1 sow3 128.0 17.1 ## 2 harv1 block1 sow4 118.0 16.9 ## 3 harv1 block1 sow5 95.0 16.6 ## 4 harv1 block1 sow2 131.0 17.0 ## 5 harv1 block1 sow1 136.5 17.0 ## 6 harv2 block2 sow3 136.5 17.0 ## Linear mixed effects model: sug = lmer(sugpct ~ block + sow + harvest + (1 | block:harvest), data = beets, REML = FALSE) sug.h = update(sug, .~. -harvest) sug.s = update(sug, .~. -sow) anova(sug, sug.h) ## Data: beets ## Models: ## sug.h: sugpct ~ block + sow + (1 | block:harvest) ## sug: sugpct ~ block + sow + harvest + (1 | block:harvest) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## sug.h 9 -69.084 -56.473 43.542 -87.084 ## sug 10 -79.998 -65.986 49.999 -99.998 12.914 1 0.0003261 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 PBmodcomp(sug, sug.h, nsim = 50) ## Bootstrap test; time: 0.45 sec; samples: 50; extremes: 5; ## large : sugpct ~ block + sow + harvest + (1 | block:harvest) ## sugpct ~ block + sow + (1 | block:harvest) ## stat df p.value ## LRT 12.914 1 0.0003261 *** ## PBtest 12.914 0.1176471 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(sug, sug.s) ## Data: beets ## Models: ## sug.s: sugpct ~ block + harvest + (1 | block:harvest) ## sug: sugpct ~ block + sow + harvest + (1 | block:harvest) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## sug.s 6 -2.795 5.612 7.398 -14.795 ## sug 10 -79.998 -65.986 49.999 -99.998 85.203 4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 PBmodcomp(sug, sug.s, nsim = 50) ## Bootstrap test; time: 0.38 sec; samples: 50; extremes: 0; ## large : sugpct ~ block + sow + harvest + (1 | block:harvest) ## sugpct ~ block + harvest + (1 | block:harvest) ## stat df p.value ## LRT 85.203 4 &lt; 2e-16 *** ## PBtest 85.203 0.01961 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A similar approach is taken in RLRsim: library(RLRsim) library(lme4) set.seed(1234) g = rep(1:10, e = 10) x = rnorm(100) y = 0.1 * x + rnorm(100) m = lmer(y ~ x + (1|g), REML = FALSE) m0 = lm(y ~ 1) obs.LRT = 2*(logLik(m) - logLik(m0)) X = getME(m, &quot;X&quot;) Z = t(as.matrix(getME(m, &quot;Zt&quot;))) sim.LRT = LRTSim(X, Z, 1, diag(10)) pval = mean(sim.LRT &gt; obs.LRT) 7.3.2 Cross-validation Cross-validation is the non-parametric alternative to AIC. Note that AIC is asymptotically equal to leave-one-out cross-validation. For most advanced models, you will have to program the cross-validation by hand, but here an example for glm.{R}, using the cv.glm function: library(boot) # Leave-one-out and 6-fold cross-validation prediction error for the mammals data set. data(mammals, package=&quot;MASS&quot;) mammals.glm = glm(log(brain) ~ log(body), data = mammals) (cv.err = cv.glm(mammals, mammals.glm)$delta) ## [1] 0.4918650 0.4916571 (cv.err.6 = cv.glm(mammals, mammals.glm, K = 6)$delta) ## [1] 0.4787539 0.4776334 # As this is a linear model we could calculate the leave-one-out # cross-validation estimate without any extra model-fitting. muhat = fitted(mammals.glm) mammals.diag = glm.diag(mammals.glm) (cv.err = mean((mammals.glm$y - muhat)^2/(1 - mammals.diag$h)^2)) ## [1] 0.491865 # Leave-one-out and 11-fold cross-validation prediction error for # the nodal data set. Since the response is a binary variable an # appropriate cost function is cost = function(r, pi = 0){ mean(abs(r - pi) &gt; 0.5) } nodal.glm = glm(r ~ stage+xray+acid, binomial, data = nodal) (cv.err = cv.glm(nodal, nodal.glm, cost, K = nrow(nodal))$delta) ## [1] 0.1886792 0.1886792 (cv.11.err = cv.glm(nodal, nodal.glm, cost, K = 11)$delta) ## [1] 0.2264151 0.2228551 Note that cross-validation requires independence of data points. For non-independent data, it is possible to block the cross-validation, see Roberts, David R., et al. “Cross‐validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure.” Ecography 40.8 (2017): 913-929., methods implemented in package blockCV, see https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html. 7.3.3 Null Models Parametric hypothesis tests usually make a fixed assumption about H0. A non-parametric method to get around this that is used for complicated situations are randomization null models. The idea of these is to shuffle around the data, and thus generate a null distribution set.seed(1337) # Permutation t-test. # A hand-coded randomization test for comparing two groups with arbitrary distribution. groupA = rnorm(50) groupB = rlnorm(50) dat = data.frame(value = c(groupA, groupB), group = factor(rep(c(&quot;A&quot;, &quot;B&quot;), each = 50))) plot(value ~ group, data = dat) # Point here is that we can&#39;t do a t-test, because groups are not normal. We could do hist(dat$value, breaks = 40) reference = mean(groupA) - mean(groupB) nSim = 5000 nullDistribution = rep(NA, nSim) for(i in 1:nSim){ sel = dat$value[sample.int(100, size = 100)] nullDistribution[i] = mean(sel[1:50]) - mean(sel[51:100]) } hist(nullDistribution, xlim = c(-2,2)) abline(v = reference, col = &quot;red&quot;) ecdf(nullDistribution)(reference) ## [1] 0 Null models are used abundant, e.g., in packages: library(vegan).{R} library(bipartide).{R} 7.4 Structural Equation Models (SEMs) Structural equation models (SEMs) are models that are designed to estimate entire causal diagrams. For GLMs responses, you will currently have to estimate the DAG (directed acyclic graph) piece-wise, e.g. with https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html. library(piecewiseSEM) mod = psem( lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley), lm(firesev ~ elev + age + cover, data = keeley), lm(cover ~ age + elev + hetero + abiotic, data = keeley) ) summary(mod) ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## ## Structural Equation Model of mod ## ## Call: ## rich ~ distance + elev + abiotic + age + hetero + firesev + cover ## firesev ~ elev + age + cover ## cover ~ age + elev + hetero + abiotic ## ## AIC BIC ## 46.543 96.539 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## cover ~ distance + ... coef 84 0.4201 0.6755 ## firesev ~ distance + ... coef 85 -0.8264 0.4109 ## firesev ~ abiotic + ... coef 85 -1.1799 0.2413 ## firesev ~ hetero + ... coef 85 -0.5755 0.5665 ## ## Global goodness-of-fit: ## ## Fisher&#39;s C = 6.543 with P-value = 0.587 and on 8 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## rich distance 0.6157 0.1855 82 3.3195 0.0013 0.3599 ** ## rich elev -0.0092 0.0059 82 -1.5663 0.1211 -0.1569 ## rich abiotic 0.4881 0.1641 82 2.9741 0.0039 0.2482 ** ## rich age 0.0241 0.1097 82 0.2199 0.8265 0.0201 ## rich hetero 44.4135 10.8093 82 4.1088 0.0001 0.3376 *** ## rich firesev -1.0181 0.8031 82 -1.2677 0.2085 -0.1114 ## rich cover 12.3998 4.2206 82 2.9379 0.0043 0.2604 ** ## firesev elev -0.0006 0.0006 86 -0.9298 0.3551 -0.0874 ## firesev age 0.0473 0.0129 86 3.6722 0.0004 0.3597 *** ## firesev cover -1.5214 0.5204 86 -2.9236 0.0044 -0.2921 ** ## cover age -0.0101 0.0024 85 -4.1757 0.0001 -0.3991 *** ## cover elev 0.0004 0.0001 85 2.9688 0.0039 0.2999 ** ## cover hetero -0.7875 0.2719 85 -2.8960 0.0048 -0.2850 ** ## cover abiotic 0.0021 0.0042 85 0.4855 0.6286 0.0498 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## rich none 0.57 ## firesev none 0.30 ## cover none 0.26 plot(mod) For linear SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is lavaan.{R}: library(lavaan) mod = &quot; rich ~ distance + elev + abiotic + age + hetero + firesev + cover firesev ~ elev + age + cover cover ~ age + elev + abiotic &quot; fit = sem(mod, data = keeley) summary(fit) ## lavaan 0.6-11 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 10.437 ## Degrees of freedom 5 ## P-value (Chi-square) 0.064 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## rich ~ ## distance 0.616 0.177 3.485 0.000 ## elev -0.009 0.006 -1.644 0.100 ## abiotic 0.488 0.156 3.134 0.002 ## age 0.024 0.105 0.229 0.819 ## hetero 44.414 9.831 4.517 0.000 ## firesev -1.018 0.759 -1.341 0.180 ## cover 12.400 3.841 3.228 0.001 ## firesev ~ ## elev -0.001 0.001 -0.951 0.342 ## age 0.047 0.013 3.757 0.000 ## cover -1.521 0.509 -2.991 0.003 ## cover ~ ## age -0.009 0.002 -3.875 0.000 ## elev 0.000 0.000 2.520 0.012 ## abiotic -0.000 0.004 -0.115 0.909 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 97.844 14.586 6.708 0.000 ## .firesev 1.887 0.281 6.708 0.000 ## .cover 0.081 0.012 6.708 0.000 Plot options … not so nice as before. library(lavaanPlot) ## Warning: package &#39;lavaanPlot&#39; was built under R version 4.1.1 lavaanPlot(model = fit) library(semPlot) ## Warning: package &#39;semPlot&#39; was built under R version 4.1.1 semPaths(fit) 7.5 Intro Bayes Intro Bayes will be done via a lecture Code for the lecture here. To fit Bayesian models, for full flexibility, most people use Stan. Stan is a completely new modelling specification language, therefore we won’t do this here. However, there is the brms package. brms allows you to specify regression models in the same syntax as lme4 / glmmTMB, but translates then to Stan code and fits them. Here a comparison: Non-Bayesian (GLMM) with lme4: library(lme4) mod0 = glmer(real ~ corpus + (1 | sound) + (1 | id), data = df, family = &quot;binomial&quot;) Bayesian with brms: library(brms) mod1 = brm(real ~ corpus + (1 | sound) + (1 | id), data = df, family = &quot;bernoulli&quot;, ...) Extended syntax: Non-Bayesian (GLMM) with lme4: library(lme4) mod0 = glmer(real ~ corpus + (1 | sound) + (1 | id), data = df, family = &quot;binomial&quot;) # Running time: 6 s. Bayesian with brms: library(brms) mod1 = brm(real ~ corpus + (1 | sound) + (1 | id), data = df, family = &quot;bernoulli&quot;, prior = set_prior(&quot;normal(0, 3)&quot;), iter = 1000, chains = 4, cores = 4) # Running time: 40 s compilation + 50 s sampling = 1.5 min. Task Take any of our simpler models, and run them with brms! Solution library(brms) Bayesian model comparison: In Bayesian stats, there is no p-value. So, how do we know if something has an effect? There are two options: Just look at the effect size and its uncertainties. Compare the simpler with the more complex model, and calculate which has a higher posterior probability The latter is called posterior weights, and they are based on the so-called Bayes factor. For simple tests, e.g. t-test or lm, the Bayes factor is implemented in the BayesFactor package in R. Look at the examples here. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
