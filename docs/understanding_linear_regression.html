<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Understanding Linear Regression | Advanced Regression Models with R</title>
  <meta name="description" content="Advanced Regression with R" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Understanding Linear Regression | Advanced Regression Models with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Advanced Regression with R" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Understanding Linear Regression | Advanced Regression Models with R" />
  
  <meta name="twitter:description" content="Advanced Regression with R" />
  

<meta name="author" content="Florian Hartig" />


<meta name="date" content="2022-06-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="reminder.html"/>
<link rel="next" href="heteroskedasticity.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.9/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="reminder.html"><a href="reminder.html"><i class="fa fa-check"></i><b>2</b> Reminder: R Basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="reminder.html"><a href="reminder.html#your-r-system"><i class="fa fa-check"></i><b>2.1</b> Your R System</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="reminder.html"><a href="reminder.html#installing-libraries"><i class="fa fa-check"></i><b>2.1.1</b> Installing Libraries</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="reminder.html"><a href="reminder.html#representing-data-in-r"><i class="fa fa-check"></i><b>2.2</b> Representing Data in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="reminder.html"><a href="reminder.html#exploring-data-structures"><i class="fa fa-check"></i><b>2.2.1</b> Exploring Data Structures</a></li>
<li class="chapter" data-level="2.2.2" data-path="reminder.html"><a href="reminder.html#dynamic-typing"><i class="fa fa-check"></i><b>2.2.2</b> Dynamic Typing</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="reminder.html"><a href="reminder.html#data-selection-slicing-and-subsetting"><i class="fa fa-check"></i><b>2.3</b> Data Selection, Slicing and Subsetting</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="reminder.html"><a href="reminder.html#subsetting-and-slicing-for-single-data-types"><i class="fa fa-check"></i><b>2.3.1</b> Subsetting and Slicing for Single Data Types</a></li>
<li class="chapter" data-level="2.3.2" data-path="reminder.html"><a href="reminder.html#logic-and-slicing"><i class="fa fa-check"></i><b>2.3.2</b> Logic and Slicing</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="reminder.html"><a href="reminder.html#applying-functions-and-aggregates-across-a-data-set"><i class="fa fa-check"></i><b>2.4</b> Applying Functions and Aggregates Across a Data Set</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="reminder.html"><a href="reminder.html#functions"><i class="fa fa-check"></i><b>2.4.1</b> Functions</a></li>
<li class="chapter" data-level="2.4.2" data-path="reminder.html"><a href="reminder.html#the-apply-function"><i class="fa fa-check"></i><b>2.4.2</b> The apply() Function</a></li>
<li class="chapter" data-level="2.4.3" data-path="reminder.html"><a href="reminder.html#the-aggregate-function"><i class="fa fa-check"></i><b>2.4.3</b> The aggregate() Function</a></li>
<li class="chapter" data-level="2.4.4" data-path="reminder.html"><a href="reminder.html#for-loops"><i class="fa fa-check"></i><b>2.4.4</b> For loops</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reminder.html"><a href="reminder.html#plotting"><i class="fa fa-check"></i><b>2.5</b> Plotting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html"><i class="fa fa-check"></i><b>3</b> Understanding Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#fitting-and-interpreting-the-regression"><i class="fa fa-check"></i><b>3.1.1</b> Fitting and Interpreting the Regression</a></li>
<li class="chapter" data-level="3.1.2" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#centering-and-scaling-of-predictors"><i class="fa fa-check"></i><b>3.1.2</b> Centering and Scaling of Predictors</a></li>
<li class="chapter" data-level="3.1.3" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#residual-checks"><i class="fa fa-check"></i><b>3.1.3</b> Residual Checks</a></li>
<li class="chapter" data-level="3.1.4" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#categorical-predictors"><i class="fa fa-check"></i><b>3.1.4</b> Categorical Predictors</a></li>
<li class="chapter" data-level="3.1.5" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#plantTrait1"><i class="fa fa-check"></i><b>3.1.5</b> Exercise: Global Plant Trait Analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#multiple-regression"><i class="fa fa-check"></i><b>3.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#understanding-the-effect-of-collinearity"><i class="fa fa-check"></i><b>3.2.1</b> Understanding the Effect of Collinearity</a></li>
<li class="chapter" data-level="3.2.2" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#scaling-variables-in-the-multiple-regression"><i class="fa fa-check"></i><b>3.2.2</b> Scaling Variables in the Multiple Regression</a></li>
<li class="chapter" data-level="3.2.3" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#anova-for-multiple-regression"><i class="fa fa-check"></i><b>3.2.3</b> ANOVA for Multiple Regression</a></li>
<li class="chapter" data-level="3.2.4" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#interactions"><i class="fa fa-check"></i><b>3.2.4</b> Interactions</a></li>
<li class="chapter" data-level="3.2.5" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#plantTrait2"><i class="fa fa-check"></i><b>3.2.5</b> Exercise: Global Plant Trait Analysis #2</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#model-choice-and-causal-inference"><i class="fa fa-check"></i><b>3.3</b> Model Choice and Causal Inference</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>3.3.1</b> The Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.3.2" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#causal-inference"><i class="fa fa-check"></i><b>3.3.2</b> Causal Inference</a></li>
<li class="chapter" data-level="3.3.3" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#model-selection-methods"><i class="fa fa-check"></i><b>3.3.3</b> Model Selection Methods</a></li>
<li class="chapter" data-level="3.3.4" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#pHacking"><i class="fa fa-check"></i><b>3.3.4</b> P-hacking</a></li>
<li class="chapter" data-level="3.3.5" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#problems-of-stepwise-model-selection"><i class="fa fa-check"></i><b>3.3.5</b> Problems of Stepwise Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#case-studies"><i class="fa fa-check"></i><b>3.4</b> Case studies</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#exercise-global-plant-trait-analysis-3"><i class="fa fa-check"></i><b>3.4.1</b> Exercise: Global Plant Trait Analysis #3</a></li>
<li class="chapter" data-level="3.4.2" data-path="understanding_linear_regression.html"><a href="understanding_linear_regression.html#case-study-life-satisfaction"><i class="fa fa-check"></i><b>3.4.2</b> Case study: Life satisfaction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html"><i class="fa fa-check"></i><b>4</b> Heteroskedasticity and Grouped Data (Random Effects)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#adjusting-the-functional-form"><i class="fa fa-check"></i><b>4.1</b> Adjusting the Functional Form</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#changing-the-regression-formular"><i class="fa fa-check"></i><b>4.1.1</b> Changing the regression formular</a></li>
<li class="chapter" data-level="4.1.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>4.1.2</b> Generalized additive models (GAMs)</a></li>
<li class="chapter" data-level="4.1.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#exercise-functional-form"><i class="fa fa-check"></i><b>4.1.3</b> Exercise functional form</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#modelling-variance-terms"><i class="fa fa-check"></i><b>4.2</b> Modelling Variance Terms</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#transformation"><i class="fa fa-check"></i><b>4.2.1</b> Transformation</a></li>
<li class="chapter" data-level="4.2.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#model-the-variance"><i class="fa fa-check"></i><b>4.2.2</b> Model the variance</a></li>
<li class="chapter" data-level="4.2.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#exercise-variance-modelling"><i class="fa fa-check"></i><b>4.2.3</b> Exercise variance modelling</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#non-normality-and-outliers"><i class="fa fa-check"></i><b>4.3</b> Non-normality and Outliers</a></li>
<li class="chapter" data-level="4.4" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#random-and-mixed-effects---motivation"><i class="fa fa-check"></i><b>4.4</b> Random and Mixed Effects - Motivation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#fitting-random-effects-models"><i class="fa fa-check"></i><b>4.4.1</b> Fitting Random Effects Models</a></li>
<li class="chapter" data-level="4.4.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#problems-with-mixed-models"><i class="fa fa-check"></i><b>4.4.2</b> Problems With Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#case-studies-1"><i class="fa fa-check"></i><b>4.5</b> Case studies</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#case-study-1-college-student-performance-over-time"><i class="fa fa-check"></i><b>4.5.1</b> Case Study 1: College Student Performance Over Time</a></li>
<li class="chapter" data-level="4.5.2" data-path="heteroskedasticity.html"><a href="heteroskedasticity.html#case-study-2---honeybee-data"><i class="fa fa-check"></i><b>4.5.2</b> Case Study 2 - Honeybee Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="GLMMs.html"><a href="GLMMs.html"><i class="fa fa-check"></i><b>5</b> GLMMs</a>
<ul>
<li class="chapter" data-level="5.1" data-path="GLMMs.html"><a href="GLMMs.html#basics"><i class="fa fa-check"></i><b>5.1</b> Basics</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="GLMMs.html"><a href="GLMMs.html#a-binomial-example"><i class="fa fa-check"></i><b>5.1.1</b> A Binomial Example</a></li>
<li class="chapter" data-level="5.1.2" data-path="GLMMs.html"><a href="GLMMs.html#a-poisson-example"><i class="fa fa-check"></i><b>5.1.2</b> A Poisson Example</a></li>
<li class="chapter" data-level="5.1.3" data-path="GLMMs.html"><a href="GLMMs.html#example---elk-data"><i class="fa fa-check"></i><b>5.1.3</b> Example - Elk Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="GLMMs.html"><a href="GLMMs.html#dispersion-problems-in-glms"><i class="fa fa-check"></i><b>5.2</b> Dispersion Problems in GLMs</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="GLMMs.html"><a href="GLMMs.html#heteroskedasticity-in-glmms"><i class="fa fa-check"></i><b>5.2.1</b> Heteroskedasticity in GLMMs</a></li>
<li class="chapter" data-level="5.2.2" data-path="GLMMs.html"><a href="GLMMs.html#zero-inflation"><i class="fa fa-check"></i><b>5.2.2</b> Zero-inflation</a></li>
<li class="chapter" data-level="5.2.3" data-path="GLMMs.html"><a href="GLMMs.html#p-hacking-link-collection"><i class="fa fa-check"></i><b>5.2.3</b> P-hacking Link Collection</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="GLMMs.html"><a href="GLMMs.html#protocol"><i class="fa fa-check"></i><b>5.3</b> Case Studies</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="GLMMs.html"><a href="GLMMs.html#owls"><i class="fa fa-check"></i><b>5.3.1</b> Owls</a></li>
<li class="chapter" data-level="5.3.2" data-path="GLMMs.html"><a href="GLMMs.html#hurricanes"><i class="fa fa-check"></i><b>5.3.2</b> Hurricanes</a></li>
<li class="chapter" data-level="5.3.3" data-path="GLMMs.html"><a href="GLMMs.html#researchers-degrees-of-freedom-skin-color-and-red-cards"><i class="fa fa-check"></i><b>5.3.3</b> Researchers Degrees of Freedom — Skin Color and Red Cards</a></li>
<li class="chapter" data-level="5.3.4" data-path="GLMMs.html"><a href="GLMMs.html#marmots"><i class="fa fa-check"></i><b>5.3.4</b> Marmots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>6</b> Correlation structures</a>
<ul>
<li class="chapter" data-level="6.1" data-path="correlation.html"><a href="correlation.html#general-idea"><i class="fa fa-check"></i><b>6.1</b> General Idea</a></li>
<li class="chapter" data-level="6.2" data-path="correlation.html"><a href="correlation.html#temporal-and-spatial-correlation-structures"><i class="fa fa-check"></i><b>6.2</b> Temporal and Spatial Correlation Structures</a></li>
<li class="chapter" data-level="6.3" data-path="correlation.html"><a href="correlation.html#phylogenetic-structures-pgls"><i class="fa fa-check"></i><b>6.3</b> Phylogenetic Structures (PGLS)</a></li>
<li class="chapter" data-level="6.4" data-path="correlation.html"><a href="correlation.html#exercices"><i class="fa fa-check"></i><b>6.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="advanced_topics.html"><a href="advanced_topics.html"><i class="fa fa-check"></i><b>7</b> Summary and advanced topics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advanced_topics.html"><a href="advanced_topics.html#reminder-modelling-strategy"><i class="fa fa-check"></i><b>7.1</b> Reminder: Modelling Strategy</a></li>
<li class="chapter" data-level="7.2" data-path="advanced_topics.html"><a href="advanced_topics.html#thoughts-about-the-analysis-pipeline"><i class="fa fa-check"></i><b>7.2</b> Thoughts About the Analysis Pipeline</a></li>
<li class="chapter" data-level="7.3" data-path="advanced_topics.html"><a href="advanced_topics.html#nonparametric-estimators"><i class="fa fa-check"></i><b>7.3</b> Nonparametric estimators</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="advanced_topics.html"><a href="advanced_topics.html#the-bootstrap"><i class="fa fa-check"></i><b>7.3.1</b> The bootstrap</a></li>
<li class="chapter" data-level="7.3.2" data-path="advanced_topics.html"><a href="advanced_topics.html#cross-validation"><i class="fa fa-check"></i><b>7.3.2</b> Cross-validation</a></li>
<li class="chapter" data-level="7.3.3" data-path="advanced_topics.html"><a href="advanced_topics.html#null-models"><i class="fa fa-check"></i><b>7.3.3</b> Null Models</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="advanced_topics.html"><a href="advanced_topics.html#structural-equation-models-sems"><i class="fa fa-check"></i><b>7.4</b> Structural Equation Models (SEMs)</a></li>
<li class="chapter" data-level="7.5" data-path="advanced_topics.html"><a href="advanced_topics.html#intro-bayes"><i class="fa fa-check"></i><b>7.5</b> Intro Bayes</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Regression Models with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="understanding_linear_regression" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Understanding Linear Regression<a href="understanding_linear_regression.html#understanding_linear_regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- Put this here (right after the first markdown headline) and only here for each document! -->
<script src="./scripts/multipleChoice.js"></script>
<p>This chapter is a reminder about the basic regression model functions in R.</p>
<p>Here a warm-up exercise: Fit the regression:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="understanding_linear_regression.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality))</span></code></pre></div>
<p>And answer / discuss with your partner the following questions: What is the effect of Wind on Ozone? How important is Wind to explain Ozone? Next, run the following regressions:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="understanding_linear_regression.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp, <span class="at">data =</span> airquality))</span>
<span id="cb81-2"><a href="understanding_linear_regression.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">*</span> Temp, <span class="at">data =</span> airquality))</span></code></pre></div>
<p>Why does the effect of Wind on Ozone change so much as we change the formula? What is the “true” or correct estimate of the effect of Wind on Ozone? At the end of this chapter, you should be able to answer all these questions!</p>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Simple Linear Regression<a href="understanding_linear_regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>OK, after our warm-up, let’s start with the basics. We will again used the data set airquality, which is built-in in R. If you don’t know the data set, have a look at the description via</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="understanding_linear_regression.html#cb82-1" aria-hidden="true" tabindex="-1"></a>?airquality</span></code></pre></div>
<p>and at the variables via</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="understanding_linear_regression.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(airquality)</span></code></pre></div>
<p>To get started, let’s say we want to examine the relationship between Ozone and Wind. Let’s visualize this first:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="understanding_linear_regression.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk4-1.png" width="672" /></p>
<p>OK, I would say there is some dependency there. To quantify this numerically, you could also run</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="understanding_linear_regression.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(airquality<span class="sc">$</span>Ozone, airquality<span class="sc">$</span>Wind, <span class="at">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<p>to get the (Pearson) correlation, which is negative: -0.6015465.</p>
<p>What we want to do now is fitting regression models through the data with the <code class="sourceCode r"><span class="fu">lm</span>()</code> function of R. The function name lm is short for “linear model”. However, remember from the basic course: This model is not called linear because we necessarily fit a linear function. It’s called linear because we express the response (in our case Wind) as a polynomial of the predictor(s). That means, the predictors have linear coefficients but they might themselves be for example quadratic or sinus terms. So <span class="math inline">\(y = \operatorname{f}(x) + \mathcal{N}(0, \sigma)\)</span>, where <span class="math inline">\(\operatorname{f}\)</span> is a polynomial, e.g. <span class="math inline">\({a}_{0} + {a}_{1} \cdot x + {a}_{2} \cdot {x}^{2}\)</span>, and <span class="math inline">\(\mathcal{N}(0, \sigma)\)</span> means that we assume the data scattering as a normal (Gaussian) distribution with unknown standard deviation <span class="math inline">\(\sigma\)</span> around <span class="math inline">\(\operatorname{f}(x)\)</span>. The model is called linear because when estimating the unknown parameters (we call them “<em>effects</em>”) of the polynomial, we will see that they are all affecting the predictions linearly, and can thus be solved as a system of linear equations.</p>
<div id="fitting-and-interpreting-the-regression" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Fitting and Interpreting the Regression<a href="understanding_linear_regression.html#fitting-and-interpreting-the-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For fitting a line through this data, we have 3 options:</p>
<ol style="list-style-type: decimal">
<li>Fit a horizontal line (intercept only).</li>
<li>Fit only the slope, but assume the line goes through the origin (0, 0).</li>
<li>Fit slope and intercept.</li>
</ol>
<p>Option 3 is the most common case, but we will discuss all 3 options here.</p>
<p><strong>Intercept Only Model</strong></p>
<p>The following code fits an intercept only model, meaning that we assume the line is perfectly flat, and we only adjust it’s height (the intercept).</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="understanding_linear_regression.html#cb86-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> airquality)</span></code></pre></div>
<p>We can visualize the result via</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="understanding_linear_regression.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb87-2"><a href="understanding_linear_regression.html#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk7-1.png" width="672" /></p>
<p>and get a summary of the fitted regression coefficients via</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="understanding_linear_regression.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ 1, data = airquality)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -41.13 -24.13 -10.63  21.12 125.87 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.129      3.063   13.76   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 32.99 on 115 degrees of freedom
##   (37 observations deleted due to missingness)</code></pre>
<p>We will talk more about this summary later, but for the moment, let’s look only at the coefficients.</p>
<p>This tells us that</p>
<ul>
<li>We estimate the mean Ozone (our line) to be at <span class="math inline">\(42.12 \pm 3.1\)</span> units.</li>
<li>The value is significantly different from zero (the t-test always tests <span class="math inline">\({H}_{0}\)</span>: “The estimate is zero”).</li>
</ul>
<p>By the way, the value for the intercept is identical to <code>mean(airquality$Ozone, na.rm = T)</code>. This is no accident, as the mean is the maximum likelihood estimation for the mean of the normal distribution.</p>
<p><strong>Slope Only Model</strong></p>
<p>Although rarely sensible, you can also fit a model with just a slope. This only makes sense if you are sure that the line must go through the origin (0, 0) for physical or biological reasons.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="understanding_linear_regression.html#cb90-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> <span class="dv">0</span>, <span class="at">data =</span> airquality)</span>
<span id="cb90-2"><a href="understanding_linear_regression.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb90-3"><a href="understanding_linear_regression.html#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="understanding_linear_regression.html#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative for removing the linear term:</span></span>
<span id="cb90-5"><a href="understanding_linear_regression.html#cb90-5" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> airquality)</span>
<span id="cb90-6"><a href="understanding_linear_regression.html#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<p>In the results, you can see that we estimate a positive slope, in contradiction to our visual assessment that the data seems negatively correlated. This is because we are forcing the regression line to go through the origin (0, 0).</p>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Wind - 1, data = airquality)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -55.11 -19.34  -2.45  35.71 157.32 
## 
## Coefficients:
##      Estimate Std. Error t value Pr(&gt;|t|)    
## Wind   3.1398     0.3742   8.391  1.4e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 42.25 on 115 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.3798, Adjusted R-squared:  0.3744 
## F-statistic: 70.41 on 1 and 115 DF,  p-value: 1.404e-13</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="understanding_linear_regression.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb92-2"><a href="understanding_linear_regression.html#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk11-1.png" width="672" /></p>
<p><strong>Slope and Intercept</strong></p>
<p>The most common case will be a model with slope and intercept which is probably corresponds most with our visual assessment.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="understanding_linear_regression.html#cb93-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb93-2"><a href="understanding_linear_regression.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb93-3"><a href="understanding_linear_regression.html#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk12-1.png" width="672" /></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="understanding_linear_regression.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Wind, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -51.572 -18.854  -4.868  15.234  90.000 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  96.8729     7.2387   13.38  &lt; 2e-16 ***
## Wind         -5.5509     0.6904   -8.04 9.27e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 26.47 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.3619, Adjusted R-squared:  0.3563 
## F-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13</code></pre>
<p>This time, we want to look in full at the regression table. Recall that:</p>
<ul>
<li>“<strong>Call</strong>” repeats the regression formula.</li>
<li>“<strong>Residuals</strong>” gives you an indication about how far the observed data scatters around the fitted regression line / function.</li>
<li>The <em>regression table</em> (starting with “<strong>Coefficients</strong>”) provides the estimated parameters, one row for each fitted parameter. The first column is the estimate, the second (standard error) is the 0.63 confidence interval (for 0.95 confidence interval multiply with 1.96), and the fourth column is the p-value for a two-sided test with <span class="math inline">\({H}_{0}\)</span>: “Estimate is zero”. The t-value is used for calculation of the p-value and can usually be ignored.</li>
<li>The last section of the summary provides information about the <em>model fit</em>.
<ul>
<li>Residual error = Standard deviation of the residuals,</li>
<li>114 df = Degrees of freedom = Observed - fitted parameters.</li>
<li>R-squared <span class="math inline">\(\left({R}^{2}\right)\)</span> = How much of the signal, respective variance is explained by the model, calculated by
<span class="math inline">\(\displaystyle 1 - \frac{\text{residual variance}}{\text{total variance}}\)</span>.</li>
<li>Adjusted R-squared = Adjusted for model complexity.</li>
<li>F-test = Test against intercept only model, i.e. is the fitted model significantly better than the intercept only model (most relevant for models with &gt; 1 predictor).</li>
</ul></li>
</ul>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Discussion</span></strong><br/>
<p>What is the meaning of “An effect is not significant”?</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<p>You should NOT say that the effect is zero, or that the null hypothesis has been accepted. Official language is “there is no significant evidence for an effect(p = XXX)”. If we would like to assess what that means, some people do a post-hoc power analysis (which effect size could have been estimated), but better is typically just to discuss the confidence interval, i.e. look at the confidence interval and say: if there is an effect, we are relatively certain that it is smaller than X, given the confidence interval of XYZ.</p>
    </p>
  </details>
  <br/>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Discussion</span></strong><br/>
<p>Is an effect with three *** more significant / certain than an effect with one *?</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<p>Many people view it that way, and some even write “highly significant” for *** . It is probably true that we should have a slightly higher confidence in a very small p-value, but strictly speaking, however, there is only <em>significant</em>, or <em>not significant</em>. Interpreting the p-value as a measure of certainty is a slight misinterpretation. Again, if we want to say how certain we are about the effect, it is better to look again at the confidence interval, i.e. the standard error and use this to discuss the precision of the estimate (small confidence interval / standard error = high precision / certainty).</p>
    </p>
  </details>
  <br/>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Fit simple (univariate) linear regression models for the other two numeric variables (Temp and Solar.R) and interpret the results with your partner.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="understanding_linear_regression.html#cb96-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb96-2"><a href="understanding_linear_regression.html#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="understanding_linear_regression.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb98-2"><a href="understanding_linear_regression.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_0-1.png" width="672" /></p>
<p>Temperature seems to have a positive effect of Ozone and this effect is significant. The intercept (value for Ozone at Temp = 0) is negative and also significant. This model explains nearly 50% of the variance of the given data. This holds even for the complexity adjusted <span class="math inline">\({R}^{2}\)</span> measure. 37 observations have missing data and are omitted. Compared to the model with only an intercept, this model is significantly different.</p>
  <br/><hr/><br/>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="understanding_linear_regression.html#cb99-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Solar.R, <span class="at">data =</span> airquality)</span>
<span id="cb99-2"><a href="understanding_linear_regression.html#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Solar.R, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.292 -21.361  -8.864  16.373 119.136 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18.59873    6.74790   2.756 0.006856 ** 
## Solar.R      0.12717    0.03278   3.880 0.000179 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.33 on 109 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.1213, Adjusted R-squared:  0.1133 
## F-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="understanding_linear_regression.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Solar.R, <span class="at">data =</span> airquality)</span>
<span id="cb101-2"><a href="understanding_linear_regression.html#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_1-1.png" width="672" /></p>
<p>Solar.R seems to have a positive effect of Ozone and this effect is significant. The intercept (value for Ozone at Solar.R = 0) is positive and also significant. This model explains slightly more than 10% of the variance of the given data. This holds even for the complexity adjusted <span class="math inline">\({R}^{2}\)</span> measure. 42 observations have missing data and are omitted. Thus this model has not the power of the previous one. Compared to the model with only an intercept, this model is significantly different.</p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="centering-and-scaling-of-predictors" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Centering and Scaling of Predictors<a href="understanding_linear_regression.html#centering-and-scaling-of-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the last model</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="understanding_linear_regression.html#cb102-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb102-2"><a href="understanding_linear_regression.html#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<p>we saw an intercept of 96 for the Wind parameter. Per definition, the intercept is the predicted value for <span class="math inline">\(y\)</span> (Ozone) at <span class="math inline">\(x\)</span> (Wind) = 0. It’s fine to report this, as long as we are interested in this value. However, there are certain situations where the value at predictor = 0 is not particularly interesting. Let’s look at the regression for Temp, for example:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="understanding_linear_regression.html#cb103-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb103-2"><a href="understanding_linear_regression.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here, the intercept is -146, which doesn’t make much sense for an ozone concentration, which should be positive. We can see the reason when we plot the results:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="understanding_linear_regression.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">110</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">200</span>, <span class="dv">170</span>))</span>
<span id="cb105-2"><a href="understanding_linear_regression.html#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span>
<span id="cb105-3"><a href="understanding_linear_regression.html#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb105-4"><a href="understanding_linear_regression.html#cb105-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk15-1.png" width="672" /></p>
<p>That shows us that the value 0 is far outside of the set of our observed values for Temp, which is measured in Fahrenheit. Thus, we are extrapolating the Ozone far beyond the observed data. What we can do to avoid this is to simply re-define the x-Axis, by subtracting the mean, which is called <strong>centering</strong>:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="understanding_linear_regression.html#cb106-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>cTemp <span class="ot">=</span> airquality<span class="sc">$</span>Temp <span class="sc">-</span> <span class="fu">mean</span>(airquality<span class="sc">$</span>Temp)</span></code></pre></div>
<p>Alternatively, you can center with the build-in R command scale</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="understanding_linear_regression.html#cb107-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>cTemp <span class="ot">=</span> <span class="fu">scale</span>(airquality<span class="sc">$</span>Temp, <span class="at">center =</span> T, <span class="at">scale =</span> F)</span></code></pre></div>
<p>Fitting the model with the centered variable</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="understanding_linear_regression.html#cb108-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> cTemp, <span class="at">data =</span> airquality)</span>
<span id="cb108-2"><a href="understanding_linear_regression.html#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ cTemp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  42.1576     2.2018   19.15   &lt;2e-16 ***
## cTemp         2.4287     0.2331   10.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>produces a more interpretable value for the intercept. We can see this also visual if we plot the results, i.e. the Ozone concentration at the mean observed temperature.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="understanding_linear_regression.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> cTemp, <span class="at">data =</span> airquality)</span>
<span id="cb110-2"><a href="understanding_linear_regression.html#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span>
<span id="cb110-3"><a href="understanding_linear_regression.html#cb110-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk19-1.png" width="672" /></p>
<p>When we center, the intercept of the centered variable can be interpreted as the Ozone concentrate at the mean temperature. This value will also typically be very similar to the grand mean <code>mean(airquality$Ozone)</code>.</p>
<p>Another very common transformation is to divide the x axis by its standard deviation. This is called <strong>scaling</strong>.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="understanding_linear_regression.html#cb111-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>sTemp <span class="ot">=</span> airquality<span class="sc">$</span>Temp <span class="sc">/</span> <span class="fu">sd</span>(airquality<span class="sc">$</span>Temp)</span></code></pre></div>
<p>Fitting the model with the scaled variable mainly changes the estimate of the regression slope</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="understanding_linear_regression.html#cb112-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> sTemp, <span class="at">data =</span> airquality)</span>
<span id="cb112-2"><a href="understanding_linear_regression.html#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ sTemp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.995     18.287  -8.038 9.37e-13 ***
## sTemp         22.988      2.207  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>which is now around 23 (before it was 2.4). The difference in interpretation is the following: for the unscaled variable, we estimate the effect of 1 unit change of temperature on Ozone. For the scaled variable, we estimate the effect of a temperature change of 1 sd of the temperature values, so we can interpret this as an Ozone effect scaled to typical temperature differences in the data.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Have a look at the results below, where we apply linear transformations on a variable (linear = <em>either</em> subtract / add something to the variable, <em>or</em> multiply / divide the variable by a certain value). How does the transformation change the regression’s estimates?</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<p>Original model</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="understanding_linear_regression.html#cb114-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb114-2"><a href="understanding_linear_regression.html#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="understanding_linear_regression.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality, <span class="at">main =</span> <span class="st">&quot;Standard&quot;</span>)</span>
<span id="cb116-2"><a href="understanding_linear_regression.html#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_2-1.png" width="672" /></p>
<p>Additive transformation change the intercept value, all p-values, CIs stay the same (except for the intercept, as the test changes)</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="understanding_linear_regression.html#cb117-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>TempAdd <span class="ot">=</span> airquality<span class="sc">$</span>Temp <span class="sc">+</span> <span class="dv">10</span></span>
<span id="cb117-2"><a href="understanding_linear_regression.html#cb117-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> TempAdd, <span class="at">data =</span> airquality)</span>
<span id="cb117-3"><a href="understanding_linear_regression.html#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ TempAdd, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -171.2825    20.6034  -8.313 2.22e-13 ***
## TempAdd        2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="understanding_linear_regression.html#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> TempAdd, <span class="at">data =</span> airquality, <span class="at">main =</span> <span class="st">&quot;Addition + 10&quot;</span>)</span>
<span id="cb119-2"><a href="understanding_linear_regression.html#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_3-1.png" width="672" /></p>
<p>Multiplicative transformations change the slope value, p-values and relative CIs for intercept and slope stay the same.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="understanding_linear_regression.html#cb120-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>TempMult <span class="ot">=</span> airquality<span class="sc">$</span>Temp <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb120-2"><a href="understanding_linear_regression.html#cb120-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> TempMult, <span class="at">data =</span> airquality)</span>
<span id="cb120-3"><a href="understanding_linear_regression.html#cb120-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ TempMult, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.99549   18.28717  -8.038 9.37e-13 ***
## TempMult       0.24287    0.02331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="understanding_linear_regression.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> TempMult, <span class="at">data =</span> airquality, <span class="at">main =</span> <span class="st">&quot;Multiplication * 10&quot;</span>)</span>
<span id="cb122-2"><a href="understanding_linear_regression.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_4-1.png" width="672" /></p>
<p>Combinations of both have both effects together</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="understanding_linear_regression.html#cb123-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>TempMix <span class="ot">=</span> airquality<span class="sc">$</span>Temp <span class="sc">*</span> <span class="fl">0.1</span> <span class="sc">-</span> <span class="dv">10</span></span>
<span id="cb123-2"><a href="understanding_linear_regression.html#cb123-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> TempMix, <span class="at">data =</span> airquality)</span>
<span id="cb123-3"><a href="understanding_linear_regression.html#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ TempMix, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   95.875      5.609   17.09   &lt;2e-16 ***
## TempMix       24.287      2.331   10.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="understanding_linear_regression.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> TempMix, <span class="at">data =</span> airquality, <span class="at">main =</span> <span class="st">&quot;Mixed&quot;</span>)</span>
<span id="cb125-2"><a href="understanding_linear_regression.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_5-1.png" width="672" /></p>
    </p>
  </details>
  <br/>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Pro Task</span></strong><br/>
<p>Look at the centered and uncentered regression models</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="understanding_linear_regression.html#cb126-1" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb126-2"><a href="understanding_linear_regression.html#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="understanding_linear_regression.html#cb128-1" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> cTemp, <span class="at">data =</span> airquality)</span>
<span id="cb128-2"><a href="understanding_linear_regression.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ cTemp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  42.1576     2.2018   19.15   &lt;2e-16 ***
## cTemp         2.4287     0.2331   10.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Why do the confidence intervals (Std. Error) on the intercept in the two models (centered and uncentered) differ? To get an idea, look at the effect plots (library effects) for the model. You can also run compare <code class="sourceCode r"><span class="fu">vcov</span>(fit)</code> (calculates variance-covariance matrix) for both models.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="understanding_linear_regression.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb130-2"><a href="understanding_linear_regression.html#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="understanding_linear_regression.html#cb130-3" aria-hidden="true" tabindex="-1"></a>preList <span class="ot">=</span> <span class="fu">list</span>(<span class="at">Temp =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">110</span>, <span class="dv">1</span>))</span>
<span id="cb130-4"><a href="understanding_linear_regression.html#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">effect</span>(<span class="st">&quot;Temp&quot;</span>, fit1,  <span class="at">xlevels =</span> preList), <span class="at">main =</span> <span class="st">&quot;Standard&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_7-1.png" width="672" /></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="understanding_linear_regression.html#cb131-1" aria-hidden="true" tabindex="-1"></a>preList <span class="ot">=</span> <span class="fu">list</span>(<span class="at">cTemp =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">110</span>, <span class="dv">1</span>))</span>
<span id="cb131-2"><a href="understanding_linear_regression.html#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">effect</span>(<span class="st">&quot;cTemp&quot;</span>, fit2,  <span class="at">xlevels =</span> preList), <span class="at">main =</span> <span class="st">&quot;Centered&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_7-2.png" width="672" /></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="understanding_linear_regression.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(fit1)</span></code></pre></div>
<pre><code>##             (Intercept)        Temp
## (Intercept)  334.420718 -4.23230774
## Temp          -4.232308  0.05435046</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="understanding_linear_regression.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(fit2)</span></code></pre></div>
<pre><code>##             (Intercept)       cTemp
## (Intercept) 4.848002921 0.000633905
## cTemp       0.000633905 0.054350459</code></pre>
    </p>
  </details>
  <br/>
<p>Solution: both centered and uncentered inherently fit the same model, but uncertainty of the intercept for the uncentered model is higher, because this is wide outside the data area, thus we are extrapolating.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
</div>
<div id="residual-checks" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Residual Checks<a href="understanding_linear_regression.html#residual-checks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we fitted a regression model, but we didn’t check if the model assumptions fit to the data. Actually, in quite a few examples above we actually saw quite bad fits. For example, let’s take the slope only model <code class="sourceCode r"><span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> airquality)</code>, where we assumed that the regression line should go through (0, 0). Maybe we have good reasons to think that this should be the case biologically, but our data seem to suggest a different behavior.</p>
<p>Wht about the slope and intercept model? Also here, if we plot the predicitons, it seems the model systematically underpredicts Ozone for low Wind, and overpredicts for high Wind.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="understanding_linear_regression.html#cb136-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> cTemp, <span class="at">data =</span> airquality)</span>
<span id="cb136-2"><a href="understanding_linear_regression.html#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone <span class="sc">~</span> cTemp, <span class="at">data =</span> airquality)</span>
<span id="cb136-3"><a href="understanding_linear_regression.html#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk21-1.png" width="672" /></p>
<p>We can see this a bit better if we use the <code>effects</code>.{R} package, which we will use from now on for doing result plots for regression models.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="understanding_linear_regression.html#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb137-2"><a href="understanding_linear_regression.html#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit, <span class="at">partial.residuals =</span> T))</span></code></pre></div>
<pre><code>## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the
## predictor cTemp is a one-column matrix that was converted to a vector</code></pre>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk22-1.png" width="672" /></p>
<p>Here, the blue line is the fitted model (with confidence interval in light blue), purple circles are the data, and the purple line is a nonparametric fit to the data. What we see highlighted here is that the data seems to follow a completely different curve than the fitted model.</p>
<p>The conclusion here would be: The model we are fitting does not fit to the data, we should not interpret its outputs, but rather say that we reject it, it’s the wrong model, we have to search for a more appropriate description of the data.</p>
<p>Let’s look at the same plot for the following model:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="understanding_linear_regression.html#cb139-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb139-2"><a href="understanding_linear_regression.html#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit, <span class="at">partial.residuals =</span> T))</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk23-1.png" width="672" /></p>
<p>This looks already better, but there seems to be still a bit of a pattern regarding the scattering of the observed data around the regression line. We can get the difference between model and observations via <code class="sourceCode r"><span class="fu">residuals</span>(fit)</code>, and we could plot them against the model predictions (which can be obtained via the <code class="sourceCode r">predict</code> function) via</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="understanding_linear_regression.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">residuals</span>(fit) <span class="sc">~</span> <span class="fu">predict</span>(fit))</span>
<span id="cb140-2"><a href="understanding_linear_regression.html#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk24-1.png" width="672" /></p>
<p>Remember: The model assumes that the data scatters with a homogenous normal distribution around the regression predictions (which is the 0 line here). What seems to happen, however, is that the scatter increases towards higher predictions, and there also seems to be a tendency towards underprediction at the high and low end.</p>
<p>To better analyse these residuals (and potential problems), R offers a function for residual plots. It produces 4 plots. I think it’s most convenient plotting them all into one figure, via</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="understanding_linear_regression.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span></code></pre></div>
<p>which produces a figure with 2 x 2 = 4 panels.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="understanding_linear_regression.html#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb142-2"><a href="understanding_linear_regression.html#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk26-1.png" width="672" /></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>Residuals vs Fitted: Shows misfits and wrong functional form. Scattering should be uniformly distributed.</li>
<li>Normal Q-Q: Checks if residuals follow an overall normal distribution. Bullets should lie on the line in the middle of the plot and may scatter a little bit at the ends.</li>
<li>Scale - Location: Checks for heteroskedasticity. Does the variance change with predictions/changing values? Scattering should be uniformly distributed.</li>
<li>Residuals vs Leverage: How much impact do outliers have on the regression? Data points with high leverage should not have high residuals and vice versa. Bad points lie in the upper right or in the lower right corner. This is measured via the Cook’s distance. Distances higher than 0.5 indicate candidates for relevant outliers or strange effects.</li>
</ul>
<p><strong>Important</strong>: Residuals are always getting better for more complex models. They should therefore NOT solely be used for model selection. Select your model structure in a different way, residual checks are just for doing a final check to see if the fitted model makes sense.</p>
<p><strong>Generally</strong>: If you want to do model selection, control for model complexity. The more complex the model, the higher the cost related to the increase of accuracy.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Modify the formula to get (as far as possible) an acceptable fit to the data. Consider the following options:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="understanding_linear_regression.html#cb143-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality) <span class="co"># Intercept + slope.</span></span>
<span id="cb143-2"><a href="understanding_linear_regression.html#cb143-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> airquality) <span class="co"># Only intercept.</span></span>
<span id="cb143-3"><a href="understanding_linear_regression.html#cb143-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">-</span> <span class="dv">1</span> , <span class="at">data =</span> airquality) <span class="co"># Only slope.</span></span>
<span id="cb143-4"><a href="understanding_linear_regression.html#cb143-4" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> <span class="fu">log</span>(Wind), <span class="at">data =</span> airquality) <span class="co"># Predictor variables can be transformed.</span></span>
<span id="cb143-5"><a href="understanding_linear_regression.html#cb143-5" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone<span class="sc">^</span><span class="fl">0.5</span> <span class="sc">~</span> Wind, <span class="at">data =</span> airquality) <span class="co"># Output variables can also be transformed.</span></span>
<span id="cb143-6"><a href="understanding_linear_regression.html#cb143-6" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> airquality) <span class="co"># Mathematical functions with I() command.</span></span>
<span id="cb143-7"><a href="understanding_linear_regression.html#cb143-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-8"><a href="understanding_linear_regression.html#cb143-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb143-9"><a href="understanding_linear_regression.html#cb143-9" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb143-10"><a href="understanding_linear_regression.html#cb143-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculates optimal transformation for Ozone^lambda to achieve residuals as normally distributed as possible.</span></span>
<span id="cb143-11"><a href="understanding_linear_regression.html#cb143-11" aria-hidden="true" tabindex="-1"></a><span class="fu">boxcox</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_10-1.png" width="672" /></p>
<p>Annotation: In the picture above, you can see, that the 95% confidence interval of the best <span class="math inline">\(\lambda\)</span> lies approximately in <span class="math inline">\([0.15, 0.5]\)</span>.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<p>Possible solution, adding a quadratic predictor and chosing a power of 0.35 transformation based on the boxcox function:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="understanding_linear_regression.html#cb144-1" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">=</span> <span class="fu">lm</span>(Ozone<span class="sc">^</span><span class="fl">0.35</span> <span class="sc">~</span> Wind <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> airquality)</span>
<span id="cb144-2"><a href="understanding_linear_regression.html#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit1, <span class="at">partial.residuals =</span> T), <span class="at">selection =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_11-1.png" width="672" /></p>
<p>You could get even better fit by adding more and more predictors, as we will discuss on the section on model selection, this model probably overfits:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="understanding_linear_regression.html#cb145-1" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">=</span> <span class="fu">lm</span>(Ozone<span class="sc">^</span><span class="fl">0.35</span> <span class="sc">~</span> Wind <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb145-2"><a href="understanding_linear_regression.html#cb145-2" aria-hidden="true" tabindex="-1"></a>           <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">I</span>(Wind<span class="sc">^</span><span class="dv">8</span>), <span class="at">data =</span> airquality)</span>
<span id="cb145-3"><a href="understanding_linear_regression.html#cb145-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit2, <span class="at">partial.residuals =</span> T), <span class="at">selection =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_16-1.png" width="672" /></p>
<p>We can see this by looking at common model selection indicators (again, more in the section on model selection). AIC comparison (lower = better)</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="understanding_linear_regression.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(fit1)</span></code></pre></div>
<pre><code>## [1] 270.2059</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="understanding_linear_regression.html#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(fit2) </span></code></pre></div>
<pre><code>## [1] 274.7512</code></pre>
<p>Likelihood ratio test (is there evidence for the more complex model?)</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="understanding_linear_regression.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fit1, fit2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Ozone^0.35 ~ Wind + I(Wind^2)
## Model 2: Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) + 
##     I(Wind^6) + I(Wind^7) + I(Wind^8)
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1    113 65.112                           
## 2    107 61.059  6    4.0528 1.1837 0.3205</code></pre>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="categorical-predictors" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Categorical Predictors<a href="understanding_linear_regression.html#categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code class="sourceCode r"><span class="fu">lm</span>()</code> function can handle both numerical and categorical variables. To understand what happens if the predictor is categorical, we’ll use another data set here, <code class="sourceCode r">PlantGrowth</code> (type <code class="sourceCode r">?PlantGrowth</code> or F1 help if you want details). We visualize the data via:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="understanding_linear_regression.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(weight <span class="sc">~</span> group, <span class="at">data =</span> PlantGrowth)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk27-1.png" width="672" /></p>
<p><strong><em>A basic lm()</em></strong></p>
<p>Let’s fit an <code class="sourceCode r"><span class="fu">lm</span>()</code> now with the categorical explanatory variable group. They syntax is the same as before:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="understanding_linear_regression.html#cb153-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(weight <span class="sc">~</span> group, <span class="at">data =</span> PlantGrowth)</span>
<span id="cb153-2"><a href="understanding_linear_regression.html#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = weight ~ group, data = PlantGrowth)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.0710 -0.4180 -0.0060  0.2627  1.3690 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***
## grouptrt1    -0.3710     0.2788  -1.331   0.1944    
## grouptrt2     0.4940     0.2788   1.772   0.0877 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6234 on 27 degrees of freedom
## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 
## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591</code></pre>
<p>But the interpretation of the results often leads to confusion. Let’s look at the results of <code class="sourceCode r"><span class="fu">summary</span>(fit)</code>.</p>
<p>Where did the group ctrl go? The answer is there is a short, and longer answer to this. Let’s first give the short one: ctrl is the intercept, and the other predictors depict the difference between ctrl and the respective levels. So, we could say that ctrl is a kind of “reference”, encoded by the intercept, and we test for a difference of the other levels against this reference.</p>
<p><strong><em>Re-ordering the levels</em></strong></p>
<p>If you want to change which factor level is the reference, you can use:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="understanding_linear_regression.html#cb155-1" aria-hidden="true" tabindex="-1"></a>PlantGrowth<span class="sc">$</span>group2 <span class="ot">=</span> <span class="fu">relevel</span>(PlantGrowth<span class="sc">$</span>group, <span class="st">&quot;trt1&quot;</span>)</span></code></pre></div>
<p>Now, we plot</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="understanding_linear_regression.html#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(weight <span class="sc">~</span> group2, <span class="at">data =</span> PlantGrowth)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk30-1.png" width="672" /></p>
<p>We see that trt1 is the first level (you can also see this if checking <code class="sourceCode r"><span class="fu">levels</span>()</code> or <code class="sourceCode r"><span class="fu">str</span>()</code> for the factor). Let’s fit the model:</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="understanding_linear_regression.html#cb157-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(weight <span class="sc">~</span> group2, <span class="at">data =</span> PlantGrowth)</span>
<span id="cb157-2"><a href="understanding_linear_regression.html#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = weight ~ group2, data = PlantGrowth)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.0710 -0.4180 -0.0060  0.2627  1.3690 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.6610     0.1971  23.644  &lt; 2e-16 ***
## group2ctrl    0.3710     0.2788   1.331  0.19439    
## group2trt2    0.8650     0.2788   3.103  0.00446 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6234 on 27 degrees of freedom
## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 
## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591</code></pre>
<p>Weird, now suddenly we have a significant difference between the groups. Wasn’t the group difference not significant before? What’s the difference?</p>
<p>The answer is that we are still fitting the identical regression model, and if you would do a <code class="sourceCode r"><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit))</code> for the first and second model, it would look the same. However, as the p-values in the regression table always compare against the reference, we now do a comparison (ctr1 vs ctr2) that we didn’t do before, and this comparison is significant.</p>
<p>So, if the ordering influences what levels are compared (technically, we call this <strong>contrasts</strong>, see below), how can we deal with the problem that the order influences which factors are compared. There are three answers for this:</p>
<p>First, in many cases, the scientific question / experimental design determines which factor level should be first. In this case, the original reference was ctrl. This clearly stands for control. So, we have a special treatment here (control), and we are probably interested in the contrast between control and the treatments, but not between the different treatments. In this case, we are probably fine.</p>
<p><strong><em>ANOVA (Analysis of Variance)</em></strong></p>
<p>Second, there is a another test that is commonly performed in this case, the <strong>ANOVA</strong>. We can run this via</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="understanding_linear_regression.html#cb159-1" aria-hidden="true" tabindex="-1"></a>anov <span class="ot">=</span> <span class="fu">aov</span>(fit)</span>
<span id="cb159-2"><a href="understanding_linear_regression.html#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(anov)</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## group2       2  3.766  1.8832   4.846 0.0159 *
## Residuals   27 10.492  0.3886                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>And the result is</p>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## group2       2  3.766  1.8832   4.846 0.0159 *
## Residuals   27 10.492  0.3886                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>To interpret this, recall that in a nutshell, the ANOVA starts with a base model (in this case intercept only) and adds the variable group. It then measures:</p>
<ul>
<li>How much the model improves in terms of <span class="math inline">\({R}^{2}\)</span> (this is in the column Sum Sq).</li>
<li>If this increase of model fit is significant.</li>
</ul>
<p>In this case, we can conclude that the variable group (3 levels) significantly improves model fit, i.e. the group seems to have an overall effect, even though the individual contrasts in the original model where not significant.</p>
<p><strong><em>Post-Hoc Tests</em></strong></p>
<p>Third, if there is no clear reference level, and the ANOVA confirms that the factor has an effect, we may want to compute p-values for all possible combinations of factor levels. This is done via the so-called post-hoc tests:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="understanding_linear_regression.html#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="fu">TukeyHSD</span>(anov)</span></code></pre></div>
<p>The result is:</p>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = fit)
## 
## $group2
##            diff        lwr      upr     p adj
## ctrl-trt1 0.371 -0.3202161 1.062216 0.3908711
## trt2-trt1 0.865  0.1737839 1.556216 0.0120064
## trt2-ctrl 0.494 -0.1972161 1.185216 0.1979960</code></pre>
<p>This highlights, as before, a significant difference between trt1 and trt2. It is common to visualize the results of the post-hoc tests with the so-called <strong>Compact Letter Display</strong> (cld). This doesn’t work with the base <code class="sourceCode r">TukeyHSD</code> function, so we will use the <code>multcomp</code>.{R} pacakge:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="understanding_linear_regression.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(multcomp)</span>
<span id="cb164-2"><a href="understanding_linear_regression.html#cb164-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-3"><a href="understanding_linear_regression.html#cb164-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(weight <span class="sc">~</span> group, <span class="at">data =</span> PlantGrowth)</span>
<span id="cb164-4"><a href="understanding_linear_regression.html#cb164-4" aria-hidden="true" tabindex="-1"></a>tuk <span class="ot">=</span> <span class="fu">glht</span>(fit, <span class="at">linfct =</span> <span class="fu">mcp</span>(<span class="at">group =</span> <span class="st">&quot;Tukey&quot;</span>))</span>
<span id="cb164-5"><a href="understanding_linear_regression.html#cb164-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tuk)          <span class="co"># Standard display.</span></span></code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = weight ~ group, data = PlantGrowth)
## 
## Linear Hypotheses:
##                  Estimate Std. Error t value Pr(&gt;|t|)  
## trt1 - ctrl == 0  -0.3710     0.2788  -1.331    0.391  
## trt2 - ctrl == 0   0.4940     0.2788   1.772    0.198  
## trt2 - trt1 == 0   0.8650     0.2788   3.103    0.012 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="understanding_linear_regression.html#cb166-1" aria-hidden="true" tabindex="-1"></a>tuk.cld <span class="ot">=</span> <span class="fu">cld</span>(tuk)    <span class="co"># Letter-based display.</span></span>
<span id="cb166-2"><a href="understanding_linear_regression.html#cb166-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tuk.cld)</span></code></pre></div>
<p>The cld gives a new letter for each group of factor levels that are statistically undistinguishable. You can see the output via <code class="sourceCode r">tuk.cld</code>, here I only show the plot:</p>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk38-1.png" width="672" /></p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task: Categorical analysis for the airquality data set</span></strong><br/>
<p>The airquality data set contains a categorical predictor “month”, which, however, is wrongly coded as a numeric value. We can correct this by doing</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="understanding_linear_regression.html#cb167-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>fMonth <span class="ot">=</span> <span class="fu">factor</span>(airquality<span class="sc">$</span>Month)</span></code></pre></div>
<p>Execute this code and fit a regression for fMonth!</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
<p><strong><em>Advanced topic: Changing the contrasts</em></strong></p>
<p>Before, I said that there is a long and short answer to the interpretation of the regression coefficients. Now here is the long answer: If you have a categorical predictor with &gt; 2 levels, there are several ways to set up the model to fit those levels. Maybe the easiest idea would be to fit a mean per level. You can actually tell R to do this via</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="understanding_linear_regression.html#cb168-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(weight <span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> group, <span class="at">data =</span> PlantGrowth)</span></code></pre></div>
<p>If we look at the output, we see that now we simply get the mean of each group (level):</p>
<pre><code>## 
## Call:
## lm(formula = weight ~ 0 + group, data = PlantGrowth)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.0710 -0.4180 -0.0060  0.2627  1.3690 
## 
## Coefficients:
##           Estimate Std. Error t value Pr(&gt;|t|)    
## groupctrl   5.0320     0.1971   25.53   &lt;2e-16 ***
## grouptrt1   4.6610     0.1971   23.64   &lt;2e-16 ***
## grouptrt2   5.5260     0.1971   28.03   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6234 on 27 degrees of freedom
## Multiple R-squared:  0.9867, Adjusted R-squared:  0.9852 
## F-statistic: 665.5 on 3 and 27 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Why does R not do that by default? Because now, we see the comparison of each group against zero in the p-values. In some cases, this can be interesting, but in most cases where we have a control and treatment and are interested in the difference between treatment and control, this is not informative. Therefore, R uses the so-called treatment contrasts, which is what we had before.</p>
<p>There are actually a number of further options for specifying contrasts. You can tell R by hand how the levels should be compared or use some of the pre-defined contrasts. Here is an example:</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="understanding_linear_regression.html#cb170-1" aria-hidden="true" tabindex="-1"></a>PlantGrowth<span class="sc">$</span>group3 <span class="ot">=</span> PlantGrowth<span class="sc">$</span>group</span>
<span id="cb170-2"><a href="understanding_linear_regression.html#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(PlantGrowth<span class="sc">$</span>group3) <span class="ot">=</span> contr.helmert</span>
<span id="cb170-3"><a href="understanding_linear_regression.html#cb170-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(weight <span class="sc">~</span> group3, <span class="at">data =</span> PlantGrowth)</span>
<span id="cb170-4"><a href="understanding_linear_regression.html#cb170-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = weight ~ group3, data = PlantGrowth)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.0710 -0.4180 -0.0060  0.2627  1.3690 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.07300    0.11381  44.573  &lt; 2e-16 ***
## group31     -0.18550    0.13939  -1.331  0.19439    
## group32      0.22650    0.08048   2.814  0.00901 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6234 on 27 degrees of freedom
## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 
## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591</code></pre>
<p>What we are using here is <strong>Helmert contrasts</strong>, which contrast the second level with the first, the third with the average of the first two, and so on. Which contrasts make most sense depends on the question. For more details, see here:<br />
<a href="https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x" target="_blank" rel="noopener">https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x</a>.</p>
</div>
<div id="plantTrait1" class="section level3 hasAnchor" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Exercise: Global Plant Trait Analysis<a href="understanding_linear_regression.html#plantTrait1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Look at the plantHeight dataset in Ecodata. Let’s assume we want to analyze whether height of plant species from around the world depends on temperature at the location of occurrence. Note that “loght” = log(height).</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="understanding_linear_regression.html#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EcoData)</span>
<span id="cb172-2"><a href="understanding_linear_regression.html#cb172-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-3"><a href="understanding_linear_regression.html#cb172-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">lm</span>(loght <span class="sc">~</span> temp, <span class="at">data =</span> plantHeight)</span>
<span id="cb172-4"><a href="understanding_linear_regression.html#cb172-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loght ~ temp, data = plantHeight)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.97903 -0.42804 -0.00918  0.43200  1.79893 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.225665   0.103776  -2.175    0.031 *  
## temp         0.042414   0.005593   7.583 1.87e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6848 on 176 degrees of freedom
## Multiple R-squared:  0.2463, Adjusted R-squared:  0.242 
## F-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12</code></pre>
<p>The model suggests a significant global trend of plant height increasing with temperature.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Tasks</span></strong><br/>
<ol style="list-style-type: decimal">
<li>Perform residual checks and modify the model if you think it is necessary. Does the effect still hold?</li>
<li>A concern regarding this analysis is that species are not fully independent. E.g., the plant family of Ericaceae, comprising many tiny dwarf shrubs, could have evolved in colder regions by chance. Is the signal still there if we look at families, rather than species? For that, try fitting the regression for the mean per family. Hint: you could use the <code class="sourceCode r"><span class="fu">aggregate</span>()</code> function to get means per family.</li>
<li>The data set also includes a categorical variable “growthform”. Test if growthform has an effect on the plant height.</li>
</ol>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<p><strong><em>1.</em></strong></p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="understanding_linear_regression.html#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb174-2"><a href="understanding_linear_regression.html#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_19-1.png" width="672" /></p>
<p>Looks OK!</p>
<p><strong><em>2.</em></strong></p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="understanding_linear_regression.html#cb175-1" aria-hidden="true" tabindex="-1"></a>aggDat <span class="ot">=</span> <span class="fu">aggregate</span>(. <span class="sc">~</span> Family, </span>
<span id="cb175-2"><a href="understanding_linear_regression.html#cb175-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> plantHeight[, <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">14</span>)], <span class="at">FUN =</span> mean)</span>
<span id="cb175-3"><a href="understanding_linear_regression.html#cb175-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-4"><a href="understanding_linear_regression.html#cb175-4" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">=</span> <span class="fu">lm</span>(loght <span class="sc">~</span> temp, <span class="at">data =</span> aggDat)</span>
<span id="cb175-5"><a href="understanding_linear_regression.html#cb175-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loght ~ temp, data = aggDat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.16556 -0.38220  0.02092  0.26734  1.38896 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.27817    0.14910  -1.866   0.0665 .  
## temp         0.04884    0.00781   6.254 3.35e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5195 on 66 degrees of freedom
## Multiple R-squared:  0.3721, Adjusted R-squared:  0.3626 
## F-statistic: 39.12 on 1 and 66 DF,  p-value: 3.349e-08</code></pre>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="understanding_linear_regression.html#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(model2))</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## temp         1  10.56   10.56   39.12 3.35e-08 ***
## Residuals   66  17.81    0.27                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Yes, there is still an effect</p>
<p><strong><em>3.</em></strong></p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="understanding_linear_regression.html#cb179-1" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">=</span> <span class="fu">lm</span>(loght <span class="sc">+</span>  temp <span class="sc">~</span> growthform, <span class="at">data =</span> plantHeight)</span>
<span id="cb179-2"><a href="understanding_linear_regression.html#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loght + temp ~ growthform, data = plantHeight)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -25.973  -4.362   1.440   5.811  16.561 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)          13.75527    8.62265   1.595    0.113
## growthformHerb       -2.71491    8.72008  -0.311    0.756
## growthformHerb/Shrub  3.92082   12.19427   0.322    0.748
## growthformShrub       0.02093    8.71019   0.002    0.998
## growthformShrub/Tree 11.46166    8.97474   1.277    0.203
## growthformTree        6.88269    8.69304   0.792    0.430
## 
## Residual standard error: 8.623 on 162 degrees of freedom
##   (10 observations deleted due to missingness)
## Multiple R-squared:  0.232,  Adjusted R-squared:  0.2083 
## F-statistic: 9.787 on 5 and 162 DF,  p-value: 3.451e-08</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="understanding_linear_regression.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(model3))</span></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## growthform    5   3638   727.7   9.787 3.45e-08 ***
## Residuals   162  12045    74.4                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 10 observations deleted due to missingness</code></pre>
<p>There is also an effect of growth form. Note that the comparisons are against the growth form fern (intercept), which has only one observation, so it may make sense to re-order the factor in the regression so that you compare, e.g., against herbs (will yield more significant comparisons).</p>
    </p>
  </details>
  <br/><hr/>
</div>
</div>
<div id="multiple-regression" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Multiple Regression<a href="understanding_linear_regression.html#multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multiple (linear) regression means that we consider more than 1 predictor in the same model. The syntax is very easy: Just add your predictors (numerical or categorical) to your regression formula, as in the following example for the airquality dataset. To be able to also add a factor, I created a new variable fMonth to have month as a factor (categorical):</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="understanding_linear_regression.html#cb183-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>fMonth <span class="ot">=</span> <span class="fu">factor</span>(airquality<span class="sc">$</span>Month)</span>
<span id="cb183-2"><a href="understanding_linear_regression.html#cb183-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp <span class="sc">+</span> Wind <span class="sc">+</span> Solar.R <span class="sc">+</span> fMonth, <span class="at">data =</span> airquality)</span></code></pre></div>
<p>The resulting regression table looks already a bit intimidating, but in principle everything is interpreted as before:</p>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.344 -13.495  -3.165  10.399  92.689 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -74.23481   26.10184  -2.844  0.00537 ** 
## Temp          1.87511    0.34073   5.503 2.74e-07 ***
## Wind         -3.10872    0.66009  -4.710 7.78e-06 ***
## Solar.R       0.05222    0.02367   2.206  0.02957 *  
## fMonth6     -14.75895    9.12269  -1.618  0.10876    
## fMonth7      -8.74861    7.82906  -1.117  0.26640    
## fMonth8      -4.19654    8.14693  -0.515  0.60758    
## fMonth9     -15.96728    6.65561  -2.399  0.01823 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.72 on 103 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6369, Adjusted R-squared:  0.6122 
## F-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Luckily, we also have the effect plots to make sense of this:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="understanding_linear_regression.html#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit, <span class="at">partial.residuals =</span> T) )</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk49-1.png" width="672" /></p>
<p><strong><em>Multiple regression != A lot of univariate regressions</em></strong></p>
<p>A common misunderstanding is that the above regression simply amounts to 4 independent univariate regressions. Let’s look at the model</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="understanding_linear_regression.html#cb186-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind , <span class="at">data =</span> airquality)</span>
<span id="cb186-2"><a href="understanding_linear_regression.html#cb186-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Wind, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -51.572 -18.854  -4.868  15.234  90.000 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  96.8729     7.2387   13.38  &lt; 2e-16 ***
## Wind         -5.5509     0.6904   -8.04 9.27e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 26.47 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.3619, Adjusted R-squared:  0.3563 
## F-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13</code></pre>
<p>The estimated effect is - 5.55, while in the multiple regression, we had -3.1. What’s going on?</p>
<p>The reason is that Wind and Temp are correlated (the technical term is <strong>collinear</strong>). You can see this by running</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="understanding_linear_regression.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Wind <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk51-1.png" width="672" /></p>
<p>This means that if we take Temp out of the model, Wind will absorb a part of the effect of Temp, or, to put it differently: If we include Temp in the model, the model will fit the effect of Wind <strong>after</strong> removing the effect that can be explained by Temp, and vice versa.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Try out different combinations of predictors and observe how the estimates change. Try to find the predictor combination for which the effect of Wind on Temp is maximal.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
<p>So, which effect is the correct one, the univariate or the multivariate model? We will speak about the rules when to put variables in and out of the regression later, in the chapter on model choice. For the moment, however, note that if two variables correlate, including or removing one will change the estimate for the other. Remember: If there is collinearity, including one variable changes the effect size for other variables!</p>
<div id="understanding-the-effect-of-collinearity" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Understanding the Effect of Collinearity<a href="understanding_linear_regression.html#understanding-the-effect-of-collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can understand the problem of one variable influencing the effect of the other in more detail if we simulate some data. Let’s create 2 positively collinear predictors:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="understanding_linear_regression.html#cb189-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb189-2"><a href="understanding_linear_regression.html#cb189-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> x1 <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span><span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span></code></pre></div>
<p>We can check whether this has worked, through visual inspection as well as by calculating the correlation coefficient:</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="understanding_linear_regression.html#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, x2)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk53-1.png" width="672" /></p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="understanding_linear_regression.html#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x1, x2)</span></code></pre></div>
<pre><code>## [1] 0.9823957</code></pre>
<p>The first case I want to look at, is when effect1 and effect2 have equal sign. Let’s create such a situation, by simulating a normal response <span class="math inline">\(y\)</span>, where the intercept is 0, and both predictors have effect = 1:</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="understanding_linear_regression.html#cb193-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">0</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span></code></pre></div>
<p>In this case, univariate models have too high effect sizes, because in conjunction, 1) positive correlation between predictors and 2) equal effect direction can lead to predictors absorbing each other’s effect if one is taken out:</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="understanding_linear_regression.html#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1))</span></code></pre></div>
<pre><code>## (Intercept)          x1 
##   0.1223649   2.0490812</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="understanding_linear_regression.html#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x2))</span></code></pre></div>
<pre><code>## (Intercept)          x2 
##  -0.1093031   1.9838618</code></pre>
<p>You can also see this visually:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="understanding_linear_regression.html#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb198-2"><a href="understanding_linear_regression.html#cb198-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, y, <span class="at">main =</span> <span class="st">&quot;x1 effect&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb198-3"><a href="understanding_linear_regression.html#cb198-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1))</span>
<span id="cb198-4"><a href="understanding_linear_regression.html#cb198-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-5"><a href="understanding_linear_regression.html#cb198-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a line with intercept 0 and slope 1,</span></span>
<span id="cb198-6"><a href="understanding_linear_regression.html#cb198-6" aria-hidden="true" tabindex="-1"></a><span class="co"># just like we simulated the true dependency of y on x1:</span></span>
<span id="cb198-7"><a href="understanding_linear_regression.html#cb198-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb198-8"><a href="understanding_linear_regression.html#cb198-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-9"><a href="understanding_linear_regression.html#cb198-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;fitted&quot;</span>, <span class="st">&quot;true&quot;</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb198-10"><a href="understanding_linear_regression.html#cb198-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x2, y, <span class="at">main =</span> <span class="st">&quot;x2 effect&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb198-11"><a href="understanding_linear_regression.html#cb198-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x2))</span>
<span id="cb198-12"><a href="understanding_linear_regression.html#cb198-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb198-13"><a href="understanding_linear_regression.html#cb198-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;fitted&quot;</span>, <span class="st">&quot;true&quot;</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk56-1.png" width="672" /></p>
<p>The multivariate model, on the other hand, gets the right estimates (with a bit of error):</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="understanding_linear_regression.html#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x1 <span class="sc">+</span> x2))</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##  0.04269288  1.33989604  0.70264737</code></pre>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Check what happens if the 2 effects have opposite sign.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="understanding_linear_regression.html#cb201-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb201-2"><a href="understanding_linear_regression.html#cb201-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="sc">-</span>x1 <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span><span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb201-3"><a href="understanding_linear_regression.html#cb201-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">0</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb201-4"><a href="understanding_linear_regression.html#cb201-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb201-5"><a href="understanding_linear_regression.html#cb201-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(x1, x2)</span></code></pre></div>
<pre><code>## [1] -0.9797563</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="understanding_linear_regression.html#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1))</span></code></pre></div>
<pre><code>## (Intercept)          x1 
## -0.21750989  0.03526607</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="understanding_linear_regression.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x2))</span></code></pre></div>
<pre><code>##   (Intercept)            x2 
## -0.2111353836  0.0008128962</code></pre>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="understanding_linear_regression.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb207-2"><a href="understanding_linear_regression.html#cb207-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, y, <span class="at">main =</span> <span class="st">&quot;x1 effect&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb207-3"><a href="understanding_linear_regression.html#cb207-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1))</span>
<span id="cb207-4"><a href="understanding_linear_regression.html#cb207-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb207-5"><a href="understanding_linear_regression.html#cb207-5" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;fitted&quot;</span>, <span class="st">&quot;true&quot;</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb207-6"><a href="understanding_linear_regression.html#cb207-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x2, y, <span class="at">main =</span> <span class="st">&quot;x2 effect&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb207-7"><a href="understanding_linear_regression.html#cb207-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x2))</span>
<span id="cb207-8"><a href="understanding_linear_regression.html#cb207-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb207-9"><a href="understanding_linear_regression.html#cb207-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;fitted&quot;</span>, <span class="st">&quot;true&quot;</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_task_24-1.png" width="672" /></p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="understanding_linear_regression.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x1 <span class="sc">+</span> x2))</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##  -0.2164637   0.8995268   0.8951650</code></pre>
<p><em>Both effects cancel out</em>.</p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="scaling-variables-in-the-multiple-regression" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Scaling Variables in the Multiple Regression<a href="understanding_linear_regression.html#scaling-variables-in-the-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before, we had already computed the regression table for a regression with 4 predictors:</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="understanding_linear_regression.html#cb210-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>fMonth <span class="ot">=</span> <span class="fu">factor</span>(airquality<span class="sc">$</span>Month)</span>
<span id="cb210-2"><a href="understanding_linear_regression.html#cb210-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp <span class="sc">+</span> Wind <span class="sc">+</span> Solar.R <span class="sc">+</span> fMonth, <span class="at">data =</span> airquality)</span>
<span id="cb210-3"><a href="understanding_linear_regression.html#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.344 -13.495  -3.165  10.399  92.689 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -74.23481   26.10184  -2.844  0.00537 ** 
## Temp          1.87511    0.34073   5.503 2.74e-07 ***
## Wind         -3.10872    0.66009  -4.710 7.78e-06 ***
## Solar.R       0.05222    0.02367   2.206  0.02957 *  
## fMonth6     -14.75895    9.12269  -1.618  0.10876    
## fMonth7      -8.74861    7.82906  -1.117  0.26640    
## fMonth8      -4.19654    8.14693  -0.515  0.60758    
## fMonth9     -15.96728    6.65561  -2.399  0.01823 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.72 on 103 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6369, Adjusted R-squared:  0.6122 
## F-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>So, which of the predictors is the strongest (= most effect on the response)? Superficially, it looks as if Month has the highest values. But that does mean that Month is the most important?</p>
<p>No, and the reason is that we have to remember the effect on the response <span class="math inline">\(y = \text{regression estimate} * \text{predictor}\)</span>, i.e if we have a predictor with a large range (difference between min/max values), it may have a strong effect even though the estimate is small. So, we cannot compare the effect sizes directly.</p>
<p>A small trick that is therefore often applied is to divide all numeric predictors by their standard deviation to bring them all on the same range, which will then be roughly between -2, 2. You can do this by hand, or use the <code class="sourceCode r"><span class="fu">scale</span>()</code> function in R:</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="understanding_linear_regression.html#cb212-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>sTemp <span class="ot">=</span> <span class="fu">scale</span>(airquality<span class="sc">$</span>Temp) <span class="co"># also performs centering</span></span>
<span id="cb212-2"><a href="understanding_linear_regression.html#cb212-2" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>sTemp <span class="ot">=</span> airquality<span class="sc">$</span>Temp <span class="sc">/</span> <span class="fu">sd</span>(airquality<span class="sc">$</span>Temp) <span class="co"># only scaling.</span></span></code></pre></div>
<p>We do the same for the other numeric variables and run the regression:</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="understanding_linear_regression.html#cb213-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>sWind <span class="ot">=</span> <span class="fu">scale</span>(airquality<span class="sc">$</span>Wind)  </span>
<span id="cb213-2"><a href="understanding_linear_regression.html#cb213-2" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>sSolar.R <span class="ot">=</span> <span class="fu">scale</span>(airquality<span class="sc">$</span>Solar.R)</span>
<span id="cb213-3"><a href="understanding_linear_regression.html#cb213-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> sTemp <span class="sc">+</span> sWind <span class="sc">+</span> sSolar.R <span class="sc">+</span> fMonth, <span class="at">data =</span> airquality)</span>
<span id="cb213-4"><a href="understanding_linear_regression.html#cb213-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.344 -13.495  -3.165  10.399  92.689 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -95.481     23.505  -4.062 9.51e-05 ***
## sTemp         17.748      3.225   5.503 2.74e-07 ***
## sWind        -10.952      2.325  -4.710 7.78e-06 ***
## sSolar.R       4.703      2.131   2.206   0.0296 *  
## fMonth6      -14.759      9.123  -1.618   0.1088    
## fMonth7       -8.749      7.829  -1.117   0.2664    
## fMonth8       -4.197      8.147  -0.515   0.6076    
## fMonth9      -15.967      6.656  -2.399   0.0182 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.72 on 103 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6369, Adjusted R-squared:  0.6122 
## F-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can compare the effect sizes directly, which suggests that Temp is actually the most important predictor.</p>
<p>Note: In the code above, I used <code class="sourceCode r"><span class="fu">scale</span>(...)</code>. By default, the scale function will scale and center. As discussed before, centering is nearly always useful as it improves the interpretability of the intercept, so I would suggest to use this as a default when scaling.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Tasks</span></strong><br/>
<p>Discuss: Under which circumstances should you center / scale, and how should you discuss the estimated coefficients in a paper?</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
<p>Scaling = estimate of relative imporatance. Original units: interpretable as effect per unit change.</p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="anova-for-multiple-regression" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> ANOVA for Multiple Regression<a href="understanding_linear_regression.html#anova-for-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another option to see which variable is more important is variance partitioning, aka ANOVA.</p>
<p>In an ANOVA, we add variable by variable to the model, and see how much the fit to the data (expressed by residual sum of squares) improves. We can do this via</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="understanding_linear_regression.html#cb215-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb215-2"><a href="understanding_linear_regression.html#cb215-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(fit))</span></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Wind          1  45284   45284   94.81  &lt; 2e-16 ***
## Temp          1  25886   25886   54.20 3.15e-11 ***
## Residuals   113  53973     478                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 37 observations deleted due to missingness</code></pre>
<p>So, why has Wind the larger effect, again? Didn’t we just say that Temp has a larger effect? Is there something wrong with our ANOVA?</p>
<p>The problem with the <code class="sourceCode r">aov</code> function is that it performs a so-called type I ANOVA. The type I ANOVA adds variables in the order in which they are in the model formula. If I specify another formula, the result is different:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="understanding_linear_regression.html#cb217-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp <span class="sc">+</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb217-2"><a href="understanding_linear_regression.html#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(fit))</span></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Temp          1  61033   61033  127.78  &lt; 2e-16 ***
## Wind          1  10137   10137   21.22 1.08e-05 ***
## Residuals   113  53973     478                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 37 observations deleted due to missingness</code></pre>
<p>The difference is due to the collinearity of the variables. Because Temp and Wind are collinear, the variable that is added first to the model will absorb variation from the other, and thus seems to explain more of the response.</p>
<p>There are other types of ANOVA that avoid this problem. The so-called type II ANOVA shows for each variable only the part that is uniquely attributable to the respective variable</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="understanding_linear_regression.html#cb219-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">Anova</span>(fit, <span class="at">type =</span> <span class="st">&quot;II&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: Ozone
##           Sum Sq  Df F value    Pr(&gt;F)    
## Temp       25886   1  54.196 3.149e-11 ***
## Wind       10137   1  21.223 1.080e-05 ***
## Residuals  53973 113                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There is also type III, which is as type II, but avoids a similar problem for interactions (see next subchapter). This is probably the most conservative setting:</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="understanding_linear_regression.html#cb221-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">Anova</span>(fit, <span class="at">type =</span> <span class="st">&quot;III&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: Ozone
##             Sum Sq  Df F value    Pr(&gt;F)    
## (Intercept)   4335   1  9.0763  0.003196 ** 
## Temp         25886   1 54.1960 3.149e-11 ***
## Wind         10137   1 21.2230 1.080e-05 ***
## Residuals    53973 113                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Here is an overview of the situation for 2 predictors A and B and their interaction. The upper left figure corresponds to the case where we have no collinearity between either of those variables. The figure on the top right (and similarly types I - III) are the three possible types of ANOVA for variables with collinearity. The “overlap” between the circles depicts the shared part, i.e. the variability that can be expressed by either variable (due to collinearity). Note that the shares in Type II, III do not add up to 1, as there is a kind of “dark variation” that we cannot securely add to either variable.</p>
<p><img src="images/ANOVA.jpg" width="150%" height="150%" /></p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Try out the difference between type I, II, III ANOVA for the airquality data set, either for the simple Wind + Temp model, or for more complicated models. If you want to see the effects of Type III Anova, you need to add an interaction (see next section).</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="interactions" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Interactions<a href="understanding_linear_regression.html#interactions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we have multiple variables, we can have the situation that the value of one variable influences the effect of the other(s). Technically, this is called in <strong>interaction</strong>. In situations where the causal direction is known, this is also called a <strong>moderator</strong>. An example: Imagine we observe that the effect of aspirin differs depending on the weight of the subject. Technically, we have an interaction between aspirin and weight. Physiologically, we know the causal direction is “weight -&gt; effect of aspirin”, so we can say weight is a moderator for the effect of aspirin.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="understanding_linear_regression.html#cb223-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp <span class="sc">*</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb223-2"><a href="understanding_linear_regression.html#cb223-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit))</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk68-1.png" width="672" /></p>
<p>We will have a look at the summary later, but for the moment, let’s just look at the output visually. In the effect plots, we see the effect of Temperature on Ozone for different values of Wind. We also see that the slope changes. For low Wind, we have a strong effect of Temperature. For high Wind, the effect is basically gone.</p>
<p>Let’s look at the interaction syntax in more detail. The “*” operator in an <code>lm()</code>.{R} is a shorthand for main effects + interactions. You can write equivalently:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="understanding_linear_regression.html#cb224-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp <span class="sc">+</span> Wind<span class="sc">:</span>Temp, <span class="at">data =</span> airquality)</span></code></pre></div>
<p>What is fit here is literally a third predictor that is specified as Wind * Temp (normal multiplication). The above syntax would allow you to also have interactions without main effects, e.g.:</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="understanding_linear_regression.html#cb225-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Wind<span class="sc">:</span>Temp, <span class="at">data =</span> airquality)</span></code></pre></div>
<p>Although this is generally <strong>never</strong> advisable, as the main effect influences the interaction, unless you are sure that the main effect must be zero.</p>
<p>There is another important syntax in R:</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="understanding_linear_regression.html#cb226-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> (Wind <span class="sc">+</span> Temp <span class="sc">+</span> Solar.R)<span class="sc">^</span><span class="dv">2</span> , <span class="at">data =</span> airquality)</span>
<span id="cb226-2"><a href="understanding_linear_regression.html#cb226-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ (Wind + Temp + Solar.R)^2, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.685 -11.727  -2.169   7.360  91.244 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  -1.408e+02  6.419e+01  -2.193  0.03056 * 
## Wind          1.055e+01  4.290e+00   2.460  0.01555 * 
## Temp          2.322e+00  8.330e-01   2.788  0.00631 **
## Solar.R      -2.260e-01  2.107e-01  -1.073  0.28591   
## Wind:Temp    -1.613e-01  5.896e-02  -2.735  0.00733 **
## Wind:Solar.R -7.231e-03  6.688e-03  -1.081  0.28212   
## Temp:Solar.R  5.061e-03  2.445e-03   2.070  0.04089 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 19.17 on 104 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6863, Adjusted R-squared:  0.6682 
## F-statistic: 37.93 on 6 and 104 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="understanding_linear_regression.html#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit), <span class="at">selection =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk71-1.png" width="672" /></p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="understanding_linear_regression.html#cb229-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit), <span class="at">selection =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk71-2.png" width="672" /></p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="understanding_linear_regression.html#cb230-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit), <span class="at">selection =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk71-3.png" width="672" /></p>
<p>This creates all main effect and second order (aka two-way) interactions between variables. You can also use <code class="sourceCode r"><span class="sc">^</span><span class="dv">3</span></code> to create all possible 2-way and 3-way interactions between the variables in the parentheses. By the way: The <code class="sourceCode r">()<span class="sc">^</span><span class="dv">2</span></code> syntax for interactions is the reason why we have to write <code class="sourceCode r"><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)</code> if we want to write a quadratic effect in an lm.</p>
<p><strong><em>Categorical variables</em></strong></p>
<p>When you include an interaction with a categorical variable, that means a separate effect will be fit for each level of the categorical variable, as in</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="understanding_linear_regression.html#cb231-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">*</span> fMonth, <span class="at">data =</span> airquality)</span>
<span id="cb231-2"><a href="understanding_linear_regression.html#cb231-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Wind * fMonth, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -54.528 -12.562  -2.246  10.691  77.750 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    50.748     15.748   3.223  0.00169 ** 
## Wind           -2.368      1.316  -1.799  0.07484 .  
## fMonth6       -41.793     31.148  -1.342  0.18253    
## fMonth7        68.296     20.995   3.253  0.00153 ** 
## fMonth8        82.211     20.314   4.047 9.88e-05 ***
## fMonth9        23.439     20.663   1.134  0.25919    
## Wind:fMonth6    4.051      2.490   1.627  0.10680    
## Wind:fMonth7   -4.663      2.026  -2.302  0.02329 *  
## Wind:fMonth8   -6.154      1.923  -3.201  0.00181 ** 
## Wind:fMonth9   -1.874      1.820  -1.029  0.30569    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.12 on 106 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.5473, Adjusted R-squared:  0.5089 
## F-statistic: 14.24 on 9 and 106 DF,  p-value: 7.879e-15</code></pre>
<p>The interpretation is like for a single categorical predictor, i.e. we see the effect of Wind as the effect for the first Month 5, and the Wind:fMonth6 effect, for example, tests for a difference in the Wind effect between month 5 (reference) and month 6. As before, you could change this behavior by changing contrasts.</p>
<p><strong><em>Interactions and centering</em></strong></p>
<p>A super important topic when working with numeric interactions is centering.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Compare the estimates for Wind / Temp for the following models</p>
<ul>
<li>Ozone ~ Wind</li>
<li>Ozone ~ Temp</li>
<li>Ozone ~ Wind + Temp</li>
<li>Ozone ~ Wind * Temp</li>
</ul>
<p>How do you explain the differences in the estimates for the main effects of Wind and Temp? What do you think corresponds most closely to the “true” effect of Wind and Temp? Maybe you know the answer already. If not, consider the following simulation, where we create data with known effect sizes:</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="understanding_linear_regression.html#cb233-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create predictor variables.</span></span>
<span id="cb233-2"><a href="understanding_linear_regression.html#cb233-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb233-3"><a href="understanding_linear_regression.html#cb233-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">100</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb233-4"><a href="understanding_linear_regression.html#cb233-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-5"><a href="understanding_linear_regression.html#cb233-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create response for lm, all effects are 1.</span></span>
<span id="cb233-6"><a href="understanding_linear_regression.html#cb233-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x1<span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb233-7"><a href="understanding_linear_regression.html#cb233-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-8"><a href="understanding_linear_regression.html#cb233-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model, but shift the mean of the predictor.</span></span>
<span id="cb233-9"><a href="understanding_linear_regression.html#cb233-9" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">*</span> <span class="fu">I</span>(x2 <span class="sc">+</span> <span class="dv">5</span>))</span>
<span id="cb233-10"><a href="understanding_linear_regression.html#cb233-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 * I(x2 + 5))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.82652 -0.20877  0.00984  0.20251  0.87495 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -5.01327    0.28118 -17.829  &lt; 2e-16 ***
## x1           -4.20433    0.49065  -8.569 1.75e-13 ***
## I(x2 + 5)     1.00200    0.05555  18.037  &lt; 2e-16 ***
## x1:I(x2 + 5)  1.03698    0.09623  10.776  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2894 on 96 degrees of freedom
## Multiple R-squared:  0.9179, Adjusted R-squared:  0.9154 
## F-statistic: 357.9 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="understanding_linear_regression.html#cb235-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(fit))</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk73-1.png" width="672" /></p>
<p>Play around with the shift in x2, and observe how the effects change. Try how the estimates change when centering the variables via the <code class="sourceCode r"><span class="fu">scale</span>()</code> command. If you understand what’s going on, you will realize that you should <strong>always center</strong> your variables, whenever you use any interactions.<br />
Excellent explanations of the issues also in the attached paper<br />
<a href="https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x" target="_blank" rel="noopener">https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x</a>.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="plantTrait2" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Exercise: Global Plant Trait Analysis #2<a href="understanding_linear_regression.html#plantTrait2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Revisit exercise <a href="understanding_linear_regression.html#plantTrait1">3.1.5</a>, and test</p>
<ol style="list-style-type: decimal">
<li>If temp or NPP (net primary productivity) is a more important predictor.</li>
<li>If growth forms (variable growthform) differ in their temperature effects.</li>
<li>If the effect of temp remains significant if we include latitude and an interaction of latitude with temp. If not, why? Plot temp ~ lat.</li>
</ol>
<p>Ask me to comment on case 3!</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
</div>
</div>
<div id="model-choice-and-causal-inference" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Model Choice and Causal Inference<a href="understanding_linear_regression.html#model-choice-and-causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What we saw so far is that there is a large number of models we could fit. But how do we decide which is the “right” one? A basic requirement is that the residuals should more or less fit. It is seldom sensible to use a model that does not fit to the data. Beyond that, however, there is a range of options which is sensible, depending on the purpose of the model.</p>
<p>In stats, we distinguish at least 2 basic purposes:</p>
<ul>
<li><strong>Prediction</strong>: If our purpose is to build a predictive model, we are searching for the model that makes the smallest possible error on a new data sample.</li>
<li><strong>(Causal) inference</strong>: When we are speaking about inference, that means we are interested in the estimated effects and we would like them to be identical to the “true” causal effects.</li>
</ul>
<p>There is a further subdivision with regards to prior knowledge:</p>
<ul>
<li>In an <strong>exploratory analysis</strong>, we have only a vague idea what we are looking for. We might just be scanning the data set for possible (causal) relationships.</li>
<li>In a <strong>confirmatory analysis</strong>, we have a clear target for the analysis, and ideally a plan for which model we want to fit, <em>prior</em> to seeing the data.</li>
</ul>
<p>Depending on the analysis goal, different methods are appropriate, and we will talk about those in this chapter. The most common goal for scientific papers is a confirmatory causal analysis (even though the actual practice does not always follow this).</p>
<p>Even within each of these objectives, there are a number of additional criteria that may influence which method and model one will choose for the analysis. For example,</p>
<ul>
<li><p>Either for predictions or for estimators, do I care more about a small <strong>error</strong>, or about <strong>bias</strong>? (Error = typical (mean) difference between estimator and truth; Bias = systematic difference between estimator and truth)</p></li>
<li><p>Do I want confidence intervals to be correct (coverage), and calibrated p-values?</p></li>
<li><p>Do we have <strong>experimental data</strong>, where all predictors are known, measured, and randomized / orthogonal, or do we have <strong>observational data</strong>, where we do not have controlled predictors, and collinearity / confounding is the norm.</p></li>
</ul>
<p>All of these play into the choice of model and model selection method. Some methods, for example, produce smaller errors on the estimators, but a larger bias. In this chapter, I will provide you with a rough overview about the methods. We will talk about them in more detail in the next days.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Discussion</span></strong><br/>
<p>Discuss with your partners: How do you typically choose which regression formula to fit?</p>
    </p>
  </details>
  <br/><hr/>
<div id="the-bias-variance-trade-off" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> The Bias-Variance Trade-off<a href="understanding_linear_regression.html#the-bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One fundamental idea about modelling choice is the <strong>bias-variance trade-off</strong>, which applies regardless of whether we are interested in causal effects (next section) or predictions. The idea is the following:</p>
<ul>
<li>The more variables / complexity we include in the model, the better it can (in principle) adjust to the true relationship, thus reducing model error from bias.</li>
<li>The more variables / complexity we include in the model, the larger our error (variance) on the fitted coefficients, thus increasing model error from variance. This means, the model adopts to the given data but no longer to the underlying relationship.</li>
</ul>
<p>If we sum both terms up, we see that at the total error of a model that is too simple will be dominated by bias (underfitting), and the total error of a model that is too complex will be dominated by variance (overfitting):</p>
<p><img src="images/BiasBarianceTradeOff.jpg" width="150%" height="150%" /></p>
<p>We will do some practical simulations on this on Wednesday, for the moment let’s just accept this idea as a fact.</p>
</div>
<div id="causal-inference" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Causal Inference<a href="understanding_linear_regression.html#causal-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Apart from the bias-variance trade-off, a crucial consideration is if we are just interested in predictions, or in causal effects. If we are after causal effects, the correct selection of variables is crucial, while it isn’t if we just want to predict. This is reviewed in the excellent paper by Lederer et al., which is available <a href="https://www.atsjournals.org/doi/full/10.1513/AnnalsATS.201808-564PS" target="_blank" rel="noopener">here</a>.</p>
<p>The basic idea is the following:</p>
<p>Let’s first define what we mean by “causality”: Assume we look at the effect of a target variable (something that could be manipulated = <strong>predictor</strong>) on another variable (the outcome = <strong>response</strong>) in the presence of other (non-target) variables. The goal of a causal analysis is to control for these other variables, in such a way that we estimate the same effect size we would obtain if only the target predictor was manipulated (as in a randomized controlled trial).</p>
<p>You probably have learned in your intro stats class that, to do so, we have to control for <strong>confounders.</strong> I am less sure, however, if everyone is clear about what a confounder is. In particular, confounding is more specific than having a variable that correlates with predictor and response. The direction is crucial to identify true confounders. For example, C) in the figure below shows a <strong>collider</strong>, i.e. a variable that is influenced by predictor and response. Although it correlates with predictor and response, correcting for it (or including it) in a multiple regression will create a collider bias on the causal link we are interested in (Corollary: Including all variables is not always a good thing).</p>
<p><img src="images/CausalStructures.jpg" width="150%" height="150%" /></p>
<p>The bottom line of this discussions (and the essence of Pearl 2000, 2009) is that to establish causality for a specific link, we have to close the so-called back-door paths for this link. So, the strategy for fitting a causal effect is:</p>
<ul>
<li>Start by writing down the hypothesis / structure that you want to estimate causally (for example, in A, B “Plant diversity” -&gt; Ecosystem productivity).</li>
</ul>
<p>Then, include / exclude other variables with the goal of:</p>
<ul>
<li>Controlling for confounders (back-doors, blue paths in the figure).</li>
<li>Not controlling for colliders, (something similar, called “M-Bias”,) and other similar relationships (red paths).</li>
<li>It depends on the question whether we should control for <strong>mediators</strong> (yellow paths).</li>
</ul>
<p>Note: These other variables (if included) are just there to correct our estimates (-&gt; called <strong>nuisance parameters</strong>), and we should later not discuss them, as they were not themselves checked for confounding (Table 2 fallacy).</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Case study 1</span></strong><br/>
<p>Take the example of the past exercise (airquality) and assume, the goal is to understand the causal effect of Temperature on Ozone (primary hypothesis). Draw a causal diagram to decide which variables to take into the regression (i.e. noting which are confounders, mediators or colliders), and fit the model.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Case study 2</span></strong><br/>
<p>Perform a causal, a predictive and an exploratory analysis of the Swiss fertility data set called “swiss”, available in the standard R data sets. Target for the causal analysis is to estimate the causal (separate direct and indirect effects) of education on fertility, i.e. <code class="sourceCode r"><span class="fu">lm</span>(Fertility <span class="sc">~</span> Education, <span class="at">data =</span> swiss)</code>.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="model-selection-methods" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Model Selection Methods<a href="understanding_linear_regression.html#model-selection-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regardless of whether we do a causal, exploratory or a predictive analysis, we sometimes may still want to get some aid in deciding on the model structure. Specifically:</p>
<ul>
<li>For a predictive analysis, even if we know the true causal structure, it may be better to fit a simpler model to reduce the bias-variance trade-off.</li>
<li>For a causal analysis, we may not be sure about certain relationships, and we may want to test if a particular hypothesis is better supported by the data than another, or we may be data-limited as well, which means we have to reduce complexity.</li>
</ul>
<p>In these situations, model selection methods may help. The key for using them is to understand that neither of them can do magic. If you have a limited data set and a massive number of predictors, they will not magically produce the correct model. However, they can be useful in certain situations. Let’s introduce them first. I discuss possible problems in the next chapter.</p>
<p><strong><em>Likelihood-ratio tests</em></strong></p>
<p>A likelihood-ratio test (LRT) is a hypothesis test that can be used to compare 2 <strong>nested</strong> models. Nested means that the simpler of the 2 models is included in the more complex model.</p>
<p>The more complex model will always fit the data better, i.e. have a higher likelihood. This is the reason why you shouldn’t use fit or residual patterns for model selection. The likelihood-ratio test tests whether this improvement in likelihood is significantly larger than one would expect if the simpler model is the correct model.</p>
<p>Likelihood-ratio tests are used to get the p-values in an R ANOVA, and thus you can also use the <code class="sourceCode r">anova</code> function to perform an likelihood-ratio test between 2 models (Note: For simple models, this will run an F-test, which is technically not exactly a likelihood-ratio test, but the principle is the same):</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="understanding_linear_regression.html#cb236-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 1</span></span>
<span id="cb236-2"><a href="understanding_linear_regression.html#cb236-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind , <span class="at">data =</span> airquality)</span>
<span id="cb236-3"><a href="understanding_linear_regression.html#cb236-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-4"><a href="understanding_linear_regression.html#cb236-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 2</span></span>
<span id="cb236-5"><a href="understanding_linear_regression.html#cb236-5" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb236-6"><a href="understanding_linear_regression.html#cb236-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-7"><a href="understanding_linear_regression.html#cb236-7" aria-hidden="true" tabindex="-1"></a><span class="co"># LRT</span></span>
<span id="cb236-8"><a href="understanding_linear_regression.html#cb236-8" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m1, m2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Ozone ~ Wind
## Model 2: Ozone ~ Wind + Temp
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    114 79859                                  
## 2    113 53973  1     25886 54.196 3.149e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><strong><em>AIC model selection </em></strong></p>
<p>Another method for model selection, and probably the most widely used, also because it does not require that models are nested, is the AIC = <strong>Akaike Information Criterion</strong>.</p>
<p>The AIC is defined as <span class="math inline">\(2 \ln(\text{likelihood}) + 2k\)</span>, where <span class="math inline">\(k\)</span> = number of parameters.</p>
<p>Essentially, this means AIC = Fit - Penalty for complexity.</p>
<p><strong>Lower AIC is better!</strong></p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="understanding_linear_regression.html#cb238-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp, <span class="at">data =</span> airquality)</span>
<span id="cb238-2"><a href="understanding_linear_regression.html#cb238-2" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> Temp <span class="sc">+</span> Wind, <span class="at">data =</span> airquality)</span>
<span id="cb238-3"><a href="understanding_linear_regression.html#cb238-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb238-4"><a href="understanding_linear_regression.html#cb238-4" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(m1)</span></code></pre></div>
<pre><code>## [1] 1067.706</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="understanding_linear_regression.html#cb240-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(m2)</span></code></pre></div>
<pre><code>## [1] 1049.741</code></pre>
<p><strong>Note 1:</strong> It can be shown that AIC is asymptotically identical to leave-one-out cross-validation, so what AIC is optimizing is essentially the predictive error of the model on new data.</p>
<p><strong>Note 2:</strong> There are other information criteria, such as BIC, DIC, WAIC etc., as well as sample-size corrected versions of either of them (e.g. AICc). The difference between the methods is beyond the scope of this course. For the most common one (BIC), just the note that this penalizes more strongly for large data sets, and thus corrects a tendency of AIC to overfit for large data sets.</p>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Compare results of AIC with likelihood-ratio tests. Discuss: When to use one or the other?</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
<p><strong><em>Shrinkage estimation </em></strong></p>
<p>A third option option for model selection are shrinkage estimators. These include the LASSO and ridge.</p>
<p>The basic idea behind these estimators is not to reduce the number of parameters, but to reduce the flexibility of the model by introducing a penalty on the regression coefficients that code a preference for smaller or zero coefficient values. Effectively, this can either amount to model selection (because some coefficients are shrunk directly to zero), or it can mean that we can fit very large models while still being able to do good predictions, or avoid overfitting.</p>
<p>To put a ridge penalty on the standard <code class="sourceCode r">lm</code>, we can use</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="understanding_linear_regression.html#cb242-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm.ridge</span>(Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp <span class="sc">+</span> Solar.R, <span class="at">data =</span> airquality, <span class="at">lambda =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                      Wind         Temp      Solar.R 
## -62.73376169  -3.30622990   1.62842247   0.05961015</code></pre>
<p>We can see how the regression estimates vary for different penalties via</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="understanding_linear_regression.html#cb244-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( <span class="fu">lm.ridge</span>( Ozone <span class="sc">~</span> Wind <span class="sc">+</span> Temp <span class="sc">+</span> Solar.R, <span class="at">data =</span> airquality,</span>
<span id="cb244-2"><a href="understanding_linear_regression.html#cb244-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">200</span>, <span class="fl">0.1</span>) ) )</span></code></pre></div>
<p><img src="_main_files/figure-html/chunk_chapter3_chunk77-1.png" width="672" /></p>
</div>
<div id="pHacking" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> P-hacking<a href="understanding_linear_regression.html#pHacking" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most dubious model selection strategy, actually considered scientific <strong>misconduct</strong>, is <strong>p-hacking</strong>. The purpose of this exercises is to show you how <strong>not</strong> to do model selection, i.e, that by playing around with the variables, you can make any outcome significant. That is why your hypothesis needs to be fixed <strong>before</strong> looking at the data, ideally through pre-registration, based on an experimental plan or a causal analysis. Here is the example:</p>
<p>Measurements of plant performance. Target was to find out if Gen1 has an effect on Performance. Various other variables are measured</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="understanding_linear_regression.html#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb245-2"><a href="understanding_linear_regression.html#cb245-2" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">300</span>), <span class="at">ncol =</span> <span class="dv">10</span>))</span>
<span id="cb245-3"><a href="understanding_linear_regression.html#cb245-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(dat) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Performance&quot;</span>, <span class="st">&quot;Gen1&quot;</span>, <span class="st">&quot;Gen2&quot;</span>, <span class="st">&quot;soilC&quot;</span>, <span class="st">&quot;soilP&quot;</span>, <span class="st">&quot;Temp&quot;</span>,</span>
<span id="cb245-4"><a href="understanding_linear_regression.html#cb245-4" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Humidity&quot;</span>, <span class="st">&quot;xPos&quot;</span>, <span class="st">&quot;yPos&quot;</span>, <span class="st">&quot;Water&quot;</span>)</span>
<span id="cb245-5"><a href="understanding_linear_regression.html#cb245-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dat)</span></code></pre></div>
<pre><code>##   Performance            Gen1               Gen2             soilC        
##  Min.   :-2.21470   Min.   :-1.37706   Min.   :-1.8050   Min.   :-1.2766  
##  1st Qu.:-0.43496   1st Qu.:-0.38752   1st Qu.:-0.5373   1st Qu.:-0.5656  
##  Median : 0.25658   Median :-0.05656   Median : 0.1138   Median :-0.1924  
##  Mean   : 0.08246   Mean   : 0.13277   Mean   : 0.1103   Mean   : 0.1133  
##  3rd Qu.: 0.70870   3rd Qu.: 0.66515   3rd Qu.: 0.5643   3rd Qu.: 0.7126  
##  Max.   : 1.59528   Max.   : 1.98040   Max.   : 2.4016   Max.   : 1.7673  
##      soilP                Temp             Humidity             xPos        
##  Min.   :-1.914359   Min.   :-1.48746   Min.   :-2.28524   Min.   :-2.8889  
##  1st Qu.:-0.733529   1st Qu.:-0.33002   1st Qu.:-0.75750   1st Qu.:-0.8995  
##  Median :-0.312623   Median : 0.04362   Median : 0.10326   Median :-0.1313  
##  Mean   :-0.330028   Mean   : 0.23700   Mean   : 0.06683   Mean   :-0.2380  
##  3rd Qu.: 0.003638   3rd Qu.: 0.97163   3rd Qu.: 0.63563   3rd Qu.: 0.3813  
##  Max.   : 2.087166   Max.   : 2.30798   Max.   : 2.49766   Max.   : 1.8031  
##       yPos              Water        
##  Min.   :-2.40310   Min.   :-2.2891  
##  1st Qu.:-0.41395   1st Qu.:-0.5373  
##  Median : 0.03328   Median : 0.2001  
##  Mean   : 0.02441   Mean   : 0.1368  
##  3rd Qu.: 0.70437   3rd Qu.: 0.8813  
##  Max.   : 1.71963   Max.   : 2.6492</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="understanding_linear_regression.html#cb247-1" aria-hidden="true" tabindex="-1"></a><span class="co"># As you see, no effect of Gen1.</span></span>
<span id="cb247-2"><a href="understanding_linear_regression.html#cb247-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Performance <span class="sc">~</span> ., <span class="at">data =</span> dat))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Performance ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.1014 -0.2262  0.1023  0.5836  1.0351 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.01744    0.19941   0.087    0.931
## Gen1        -0.02324    0.29154  -0.080    0.937
## Gen2        -0.02607    0.23874  -0.109    0.914
## soilC        0.04102    0.25354   0.162    0.873
## soilP       -0.07209    0.24970  -0.289    0.776
## Temp        -0.23499    0.19354  -1.214    0.239
## Humidity    -0.04075    0.21180  -0.192    0.849
## xPos        -0.33340    0.20491  -1.627    0.119
## yPos         0.15390    0.21238   0.725    0.477
## Water        0.13047    0.24852   0.525    0.605
## 
## Residual standard error: 0.9503 on 20 degrees of freedom
## Multiple R-squared:  0.2707, Adjusted R-squared:  -0.05751 
## F-statistic: 0.8248 on 9 and 20 DF,  p-value: 0.6012</code></pre>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Task for you: P-hack the analysis, i.e. make an effect appear, by trying around (systematically, e.g. with selecting with data, model selection, or by hand to find a model combination that has an effect). The group who finds the model with the highest significance for Gen1 wins!</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Example</span></strong>
    </summary>
    <p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="understanding_linear_regression.html#cb249-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Performance <span class="sc">~</span> Gen1 <span class="sc">*</span> Humidity, <span class="at">data =</span> dat[<span class="dv">20</span><span class="sc">:</span><span class="dv">30</span>,]))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Performance ~ Gen1 * Humidity, data = dat[20:30, 
##     ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.71665 -0.39627 -0.05915  0.28044  0.91257 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)    -0.5248     0.2277  -2.304  0.05465 . 
## Gen1            0.8657     0.2276   3.804  0.00668 **
## Humidity        0.6738     0.2544   2.649  0.03298 * 
## Gen1:Humidity  -0.5480     0.1756  -3.122  0.01680 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6102 on 7 degrees of freedom
## Multiple R-squared:  0.7004, Adjusted R-squared:  0.572 
## F-statistic: 5.454 on 3 and 7 DF,  p-value: 0.03</code></pre>
    </p>
  </details>
  <br/><hr/>
<p>Here some inspiration:</p>
<ol style="list-style-type: decimal">
<li>Hack Your Way To Scientific Glory: <a href="https://projects.fivethirtyeight.com/p-hacking/" target="_blank" rel="noopener">https://projects.fivethirtyeight.com/p-hacking/</a></li>
<li>False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant: <a href="https://journals.sagepub.com/doi/full/10.1177/0956797611417632" target="_blank" rel="noopener">https://journals.sagepub.com/doi/full/10.1177/0956797611417632</a></li>
<li>Sixty seconds on … P-hacking: <a href="https://sci-hub.tw/https://www.bmj.com/content/362/bmj.k4039" target="_blank" rel="noopener">https://sci-hub.tw/https://www.bmj.com/content/362/bmj.k4039</a></li>
</ol>
<p>John Oliver about p-hacking:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FLNeWgs2n_Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
  encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<div id="problems-of-stepwise-model-selection" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Problems of Stepwise Model Selection<a href="understanding_linear_regression.html#problems-of-stepwise-model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LRT or AIC model selections are often used stepwise or global, i.e. we run either a chain of model selections (AIC or LRT), adding or removing complexity, or we run immediately all possible models and compare their AIC. Options in R for automatic model selection using AIC are the</p>
<ul>
<li><code class="sourceCode r">StepAIC <span class="cf">function</span></code></li>
<li><code>MuMIn</code>.{R} package</li>
</ul>
<p>Here is an example for either of those:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="understanding_linear_regression.html#cb251-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb251-2"><a href="understanding_linear_regression.html#cb251-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MuMIn)</span>
<span id="cb251-3"><a href="understanding_linear_regression.html#cb251-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb251-4"><a href="understanding_linear_regression.html#cb251-4" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> . , <span class="at">data =</span> airquality)</span>
<span id="cb251-5"><a href="understanding_linear_regression.html#cb251-5" aria-hidden="true" tabindex="-1"></a><span class="fu">stepAIC</span>(fit)</span></code></pre></div>
<pre><code>## Start:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + TempMix + mTemp10 + mTemp01 + fMonth + sTemp + 
##     sWind + sSolar.R
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + TempMix + mTemp10 + mTemp01 + fMonth + sTemp + 
##     sWind
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + TempMix + mTemp10 + mTemp01 + fMonth + sTemp
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + TempMix + mTemp10 + mTemp01 + fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + TempMix + mTemp10 + fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + TempMix + fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     TempMult + fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + TempAdd + 
##     fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + cTemp + fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Month + Day + fMonth
## 
## 
## Step:  AIC=681.55
## Ozone ~ Solar.R + Wind + Temp + Day + fMonth
## 
##           Df Sum of Sq   RSS    AIC
## - Day      1     429.5 44231 680.63
## &lt;none&gt;                 43801 681.55
## - fMonth   4    3636.8 47438 682.40
## - Solar.R  1    2101.6 45903 684.75
## - Wind     1    9552.6 53354 701.44
## - Temp     1   13410.1 57212 709.19
## 
## Step:  AIC=680.63
## Ozone ~ Solar.R + Wind + Temp + fMonth
## 
##           Df Sum of Sq   RSS    AIC
## &lt;none&gt;                 44231 680.63
## - fMonth   4    3771.8 48003 681.71
## - Solar.R  1    2090.7 46322 683.76
## - Wind     1    9524.7 53756 700.28
## - Temp     1   13005.6 57237 707.24</code></pre>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Solar.R + Wind + Temp + fMonth, data = airquality)
## 
## Coefficients:
## (Intercept)      Solar.R         Wind         Temp      fMonth6      fMonth7  
##   -74.23481      0.05222     -3.10872      1.87511    -14.75895     -8.74861  
##     fMonth8      fMonth9  
##    -4.19654    -15.96728</code></pre>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="understanding_linear_regression.html#cb254-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Default na.action for regressions in R is that NA lines are removed.</span></span>
<span id="cb254-2"><a href="understanding_linear_regression.html#cb254-2" aria-hidden="true" tabindex="-1"></a><span class="co"># MuMIn requires that there are no NA in the data in the first place.</span></span>
<span id="cb254-3"><a href="understanding_linear_regression.html#cb254-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We have to change the default and remove the NA in the data.</span></span>
<span id="cb254-4"><a href="understanding_linear_regression.html#cb254-4" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">na.action =</span> <span class="st">&quot;na.fail&quot;</span>)</span>
<span id="cb254-5"><a href="understanding_linear_regression.html#cb254-5" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> airquality[<span class="fu">complete.cases</span>(airquality),]</span>
<span id="cb254-6"><a href="understanding_linear_regression.html#cb254-6" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">lm</span>(Ozone <span class="sc">~</span> . , <span class="at">data =</span> dat)</span>
<span id="cb254-7"><a href="understanding_linear_regression.html#cb254-7" aria-hidden="true" tabindex="-1"></a>out <span class="ot">=</span> <span class="fu">dredge</span>(fit)</span></code></pre></div>
<pre><code>## Fixed term is &quot;(Intercept)&quot;</code></pre>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="understanding_linear_regression.html#cb256-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set back to default NA action.</span></span>
<span id="cb256-2"><a href="understanding_linear_regression.html#cb256-2" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">na.action =</span> <span class="st">&quot;na.omit&quot;</span>)</span>
<span id="cb256-3"><a href="understanding_linear_regression.html#cb256-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb256-4"><a href="understanding_linear_regression.html#cb256-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot only first 6 and last 6 elements of the (realy) long list:</span></span>
<span id="cb256-5"><a href="understanding_linear_regression.html#cb256-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(out)</span></code></pre></div>
<pre><code>## Global model call: lm(formula = Ozone ~ ., data = dat)
## ---
## Model selection table 
##      (Int)   cTm    Mnt   mT0    mT1  Slr.R    sWn df   logLik  AICc delta
## 586  54.63 1.871 -2.992              0.0496 -11.68  6 -492.356 997.5     0
## 601 -91.08       -2.992 18.71        0.0496 -11.68  6 -492.356 997.5     0
## 602  54.63 1.871 -2.992              0.0496 -11.68  6 -492.356 997.5     0
## 617 -91.08       -2.992       0.1871 0.0496 -11.68  6 -492.356 997.5     0
## 618  54.63 1.871 -2.992              0.0496 -11.68  6 -492.356 997.5     0
## 633 -91.08       -2.992 18.71        0.0496 -11.68  6 -492.356 997.5     0
##     weight
## 586  0.167
## 601  0.167
## 602  0.167
## 617  0.167
## 618  0.167
## 633  0.167
## Models ranked by AICc(x)</code></pre>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="understanding_linear_regression.html#cb258-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(out)</span></code></pre></div>
<pre><code>## Global model call: lm(formula = Ozone ~ ., data = dat)
## ---
## Model selection table 
##     (Int)      Day   Mnt  Slr.R sSl.R df   logLik   AICc  delta weight
## 131 41.33  0.05724              11.48  4 -538.843 1086.1  88.54  0.496
## 195 17.63  0.05724       0.1275        4 -538.843 1086.1  88.54  0.496
## 9   18.81          3.227               3 -544.892 1096.0  98.49  0.003
## 1   42.10                              2 -546.037 1096.2  98.66  0.003
## 11  19.06 -0.01492 3.226               4 -544.891 1098.2 100.64  0.001
## 3   42.42 -0.01983                     3 -546.035 1098.3 100.77  0.001
## Models ranked by AICc(x)</code></pre>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Discuss with your group: What are the problems with model selection? Concentrate on two points in particular:</p>
<ul>
<li>Causal structure.</li>
<li>Validity of p-values / multiple testing. For the latter, see example below.</li>
</ul>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="understanding_linear_regression.html#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb260-2"><a href="understanding_linear_regression.html#cb260-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb260-3"><a href="understanding_linear_regression.html#cb260-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-4"><a href="understanding_linear_regression.html#cb260-4" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">20000</span>), <span class="at">ncol =</span> <span class="dv">100</span>))</span>
<span id="cb260-5"><a href="understanding_linear_regression.html#cb260-5" aria-hidden="true" tabindex="-1"></a>dat<span class="sc">$</span>y <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">200</span>)</span>
<span id="cb260-6"><a href="understanding_linear_regression.html#cb260-6" aria-hidden="true" tabindex="-1"></a>fullModel <span class="ot">=</span> <span class="fu">lm</span>(y <span class="sc">~</span> . , <span class="at">data =</span> dat)</span>
<span id="cb260-7"><a href="understanding_linear_regression.html#cb260-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-8"><a href="understanding_linear_regression.html#cb260-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of predictors + intercept:</span></span>
<span id="cb260-9"><a href="understanding_linear_regression.html#cb260-9" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(fullModel<span class="sc">$</span>coefficients)</span></code></pre></div>
<pre><code>## [1] 101</code></pre>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="understanding_linear_regression.html#cb262-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of significant predictors:</span></span>
<span id="cb262-2"><a href="understanding_linear_regression.html#cb262-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">summary</span>(fullModel)[[<span class="dv">4</span>]][,<span class="dv">4</span>][<span class="fu">summary</span>(fullModel)[[<span class="dv">4</span>]][,<span class="dv">4</span>] <span class="sc">&lt;=</span> <span class="fl">0.05</span>])</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>2 predictors out of 100are significant (on average, we expect 5 of 100 to be significant).</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="understanding_linear_regression.html#cb264-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(selection)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ X1 + X2 + X3 + X5 + X7 + X13 + X20 + X23 + X30 + 
##     X37 + X42 + X45 + X46 + X47 + X48 + X64 + X65 + X66 + X71 + 
##     X75 + X80 + X81 + X87 + X88 + X89 + X90 + X94 + X100, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.04660 -0.50885  0.05722  0.49612  1.53704 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   1.0314     0.5045   2.044  0.04244 * 
## X1            0.4728     0.2185   2.164  0.03187 * 
## X2           -0.3809     0.2012  -1.893  0.06008 . 
## X3            0.3954     0.1973   2.004  0.04668 * 
## X5           -0.2742     0.1861  -1.473  0.14251   
## X7           -0.4442     0.1945  -2.284  0.02359 * 
## X13           0.4396     0.1980   2.220  0.02775 * 
## X20           0.3984     0.1918   2.078  0.03924 * 
## X23          -0.4137     0.2081  -1.988  0.04836 * 
## X30          -0.3750     0.1991  -1.884  0.06125 . 
## X37           0.4006     0.1989   2.015  0.04550 * 
## X42          -0.3934     0.2021  -1.946  0.05325 . 
## X45          -0.3197     0.2063  -1.550  0.12296   
## X46           0.3673     0.1992   1.844  0.06690 . 
## X47          -0.4240     0.2029  -2.090  0.03811 * 
## X48           0.5130     0.1937   2.649  0.00884 **
## X64          -0.3676     0.2094  -1.755  0.08102 . 
## X65          -0.2887     0.1975  -1.462  0.14561   
## X66           0.2769     0.2107   1.315  0.19039   
## X71          -0.5301     0.2003  -2.646  0.00891 **
## X75           0.5020     0.1969   2.550  0.01165 * 
## X80           0.3722     0.2058   1.809  0.07224 . 
## X81          -0.3731     0.2176  -1.715  0.08820 . 
## X87          -0.2684     0.1958  -1.371  0.17225   
## X88          -0.4524     0.2069  -2.187  0.03011 * 
## X89          -0.4123     0.2060  -2.002  0.04691 * 
## X90          -0.3528     0.2067  -1.707  0.08971 . 
## X94           0.3813     0.2049   1.861  0.06440 . 
## X100         -0.4058     0.2024  -2.005  0.04653 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.76 on 171 degrees of freedom
## Multiple R-squared:  0.3177, Adjusted R-squared:  0.2059 
## F-statistic: 2.843 on 28 and 171 DF,  p-value: 1.799e-05</code></pre>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="understanding_linear_regression.html#cb266-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of predictors + intercept:</span></span>
<span id="cb266-2"><a href="understanding_linear_regression.html#cb266-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(selection<span class="sc">$</span>coefficients)</span></code></pre></div>
<pre><code>## [1] 29</code></pre>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="understanding_linear_regression.html#cb268-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of significant predictors:</span></span>
<span id="cb268-2"><a href="understanding_linear_regression.html#cb268-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">summary</span>(selection)[[<span class="dv">4</span>]][,<span class="dv">4</span>][<span class="fu">summary</span>(selection)[[<span class="dv">4</span>]][,<span class="dv">4</span>] <span class="sc">&lt;=</span> <span class="fl">0.05</span>])</span></code></pre></div>
<pre><code>## [1] 15</code></pre>
<p>Voila, 15 out of 28 (before 100) predictors significant.
Looks like we could have good fun to discuss / publish these results!</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
<p>Conclusion: Stepwise selection + regression table is <strong>hidden multiple testing</strong> and has inflated Type I error rates! This is well-known in the stats literature. You <em>CAN</em> do hypothesis tests after model selection, but those require corrections and are not particularly popular, because they are even less significant than the full regression.</p>
<p>That being said, those methods work excellent to generate <em>predictive</em> models!</p>
</div>
</div>
<div id="case-studies" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Case studies<a href="understanding_linear_regression.html#case-studies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exercise-global-plant-trait-analysis-3" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Exercise: Global Plant Trait Analysis #3<a href="understanding_linear_regression.html#exercise-global-plant-trait-analysis-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Revisit exercises <a href="understanding_linear_regression.html#plantTrait1">3.1.5</a> / <a href="understanding_linear_regression.html#plantTrait2">3.2.5</a>, and discuss / analyze:</p>
<ul>
<li>Which would be the appropriate model, if we want to get a predictive model for plant height, based on the variables in the data set?</li>
<li>Which would be the appropriate model, if we want to fit the <strong>causal</strong> effect of temp on height? Consider all variables in the data set!</li>
</ul>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>
</div>
<div id="case-study-life-satisfaction" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Case study: Life satisfaction<a href="understanding_linear_regression.html#case-study-life-satisfaction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following data set contains information about life satisfaction (lebensz_org) in Germany, based on the socio-economic panel.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="understanding_linear_regression.html#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EcoData)</span>
<span id="cb270-2"><a href="understanding_linear_regression.html#cb270-2" aria-hidden="true" tabindex="-1"></a>?soep</span></code></pre></div>
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
<p>Perform an exploratory causal analysis of the data to find out what determines life satisfaction and if the effect of those factors has changed over time.</p>
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
    </p>
  </details>
  <br/><hr/>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="reminder.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="heteroskedasticity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
