--- 
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    highlight: kate
documentclass: book
editor_options: 
  chunk_output_type: console
---


# Summary and advanced topics {#advanced_topics}

```{=html}
<!-- Put this here (right after the first markdown headline) and only here for each document! -->
<script src="./scripts/multipleChoice.js"></script>
```


## Reminder: Modelling Strategy

```{r chunk_chapter7_chunk0, echo=FALSE, out.width="150%", out.height="150%"}
knitr::include_graphics(c("images/reminder.jpg"))
```

***Things to note:***

1. For an lm, the link function is the identity function.
2. Fixed effects $\operatorname{f}(x)$ can be either a polynomial $\left( a \cdot x = b \right)$ = linear regression, a nonlinear function = nonlinear regression, or a smooth spline = generalized additive model (GAM).
3. Random effects assume normal distribution for groups.
4. Random effects can also act on fixed effects (random slope).
5. For an lm with correlation structure, C is integrated in Dist. For all other GLMMs, there is another distribution, plus the additional multivariate normal on the linear predictor.


***Strategy for analysis:***

1. Define formula via scientific questions + confounders.
2. Define type of GLM (lm, logistic, Poisson).
3. Blocks in data -> Random effects, start with random intercept.

Fit this base model, then do residual checks for

* Wrong functional form -> Change fitted function.
* Wrong distribution-> Transformation or GLM adjustment.
* (Over)dispersion -> Variable dispersion GLM.
* Heteroskedasticity -> Model dispersion.
* Zero-inflation -> Add ZIP term.
* Correlation -> Add correlation structure.

And adjust the model accordingly.

Packages:

* `baseR`.{R}: `lm`.{R}, `glm`.{R}.
* `lme4`.{R}: mixed models, `lmer`.{R}, `glmer`.{R}.
* `mgcv`.{R}: GAM.
* `nlme`.{R}: Variance and correlations structure modelling for linear (mixed) models, using `gls`.{R} + `lme`.{R}.
* `glmmTMB`.{R}: Generalized linear mixed models with variance / correlation modelling and zip term.


## Thoughts About the Analysis Pipeline

In statistics, we rarely use a simple analysis. We often use an entire pipeline, consisting, for example, of the protocol that I sketched in chapter \@ref(protocol). What we should constantly ask ourselves: Is our pipeline good? By "good", we typically mean: If 1000 analyses are run in that way:

* What is the typical error of the estimate?
* What is the Type I error (false positives)?
* Are the confidence intervals correctly calculated?
* ...

The way to check this is to run simulations. For example, the following function creates data that follows the assumptions of a linear regression with slope 0.5, then fits a linear regression, and returns the estimate

```{r chunk_chapter7_chunk1, echo=TRUE, eval=TRUE}
getEstimate = function(n = 100){
  x = runif(n)
  y = 0.5 * x + rnorm(n)
  fit = lm(y ~ x)
  x = summary(fit)
  return(x$coefficients[2, 1])  # Get fitted x weight (should be ~0.5).
}
```

The replicate function allows us to execute this 1000 times:

```{r chunk_chapter7_chunk2, echo=TRUE, eval=TRUE}
set.seed(543210)

out = replicate(1000, getEstimate())
```

Plotting the result, we can check whether the linear regression is an unbiased estimator for the slope. 

```{r chunk_chapter7_chunk3, echo=TRUE, eval=TRUE}
hist(out, breaks = 50)
abline(v = 0.5, col = "red")
```

"Unbiased" means that, while each single estimate will have some error, the mean of many estimates will spread around the true value.



***Explicitly calculating these values***



**Bias**

```{r chunk_chapter7_chunk4, echo=TRUE, eval=TRUE}
mean(out) - 0.5 # Should be ~0.
```



**Variance / standard deviation of the estimator**

```{r chunk_chapter7_chunk5, echo=TRUE, eval=TRUE}
sd(out)
```

To check p-values, we could run:

```{r chunk_chapter7_chunk6, echo=TRUE, eval=TRUE}
set.seed(12345)

getEstimate = function(n = 100){  # Mind: Function has changed!
  x = runif(n)
  y = rnorm(n)  # No dependence of x! Identical: y = 0 * x + rnorm(100).
  fit = lm(y ~ x)
  x = summary(fit)
  return(x$coefficients[2, 4])  # P-value for H0: Weight of x = 0.
}

out = replicate(2000, getEstimate())

hist(out) # Expected: Uniformly distributed p-values. -> Check.

mean(out < 0.05) # Expected: ~0.05. But this is NO p-value... Check H0/H1!
# Explanation of syntax: Logical vectors are interpreted as vectors of 0s and 1s.
```

To check the properties of other, possibly more complicated pipelines, statisticians will typically use the same technique. I recommend doing this! For example, you could modify the function above to have a non-normal error. How much difference does that make? Simulating often beats recommendations in the books!


## Nonparametric estimators


### Non-parametric p-values: Null Models

Parametric hypothesis tests usually make a fixed assumption about H0. A non-parametric method to get around this that is used for complicated situations are **randomization null models**. The idea of these is to shuffle around the data, and thus generate a null distribution 

```{r chunk_chapter7_chunk19, echo=TRUE, eval=TRUE}
set.seed(1337)

# Permutation t-test.
# A hand-coded randomization test for comparing two groups with arbitrary distribution.

groupA = rnorm(50)
groupB = rlnorm(50)

dat = data.frame(value = c(groupA, groupB), group = factor(rep(c("A", "B"), each = 50)))
plot(value ~ group, data = dat)

# We can't do a t-test, because groups are not normal. So, let's create a non-parametric p-value

# test statistic: difference of the means
reference = mean(groupA) - mean(groupB)

# now, we generate the null expecation of the test statistic by re-shuffling the data
nSim = 5000
nullDistribution = rep(NA, nSim)

for(i in 1:nSim){
  sel = dat$value[sample.int(100, size = 100)]
  nullDistribution[i] = mean(sel[1:50]) - mean(sel[51:100])
}

hist(nullDistribution, xlim = c(-2,2))
abline(v = reference, col = "red")
ecdf(nullDistribution)(reference) # 1-sided p-value
```

Null models are used in many R packages where analytical p-values are not available, e.g., in:

* `library(vegan)`.{R}
* `library(bipartide)`.{R}

### Non-parametric CI - the bootstrap

***Standard (non-parametric) bootstrap***

The **bootstrap** is a method to generate approximate confidence intervals based on resampling the data. Imagine you have some kind of weird data distribution:

```{r chunk_chapter7_chunk7, echo=TRUE, eval=TRUE}
set.seed(123)

data = ifelse(rbinom(100, 1, 0.5) == 1, rexp(100, 4) , rnorm(100, -2))
hist(data)
```

We want to calculate the mean and it's uncertainty. The mean is simple, but what is the uncertainty of the mean? The standard error can't be used, because this is not a normal distribution. If we don't know the distribution, we can't use a parametric method to calculate the confidence interval.

The solution is the bootstrap. The idea is the following: We re-sample from the data to generate an estimation of the uncertainty of the mean. Let's first do this by hand:

```{r chunk_chapter7_chunk8, echo=TRUE, eval=TRUE}
set.seed(123)

performBootstrap = function(){
  resampledData = sample(data, size = length(data), replace = T) 
  return(mean(resampledData))
}

bootstrappedMean = replicate(500, performBootstrap())
hist(bootstrappedMean, breaks = 50)
abline(v = mean(data), col = "red")
```

Roughly, this distribution is the confidence interval for the mean for this particular distribution. 

In detail, there are a few tricks to correct confidence intervals for the bootstrap, which are implemented in the `boot`.{R} package. Here is how you would do a boostrap with the boot package. The trick here is to implement the function `f()`.{R}, which must take the data as well as a selection of data points "k" (for example `c(1,3,4,5,8,9)`{.R}, or `1:20`{.R}, etc.) as input, and calculate the desired statistics. 

```{r chunk_chapter7_chunk9, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(boot)

f = function(d, k){ mean(d[k]) }
out = boot(data, f, 500)
plot(out)
boot.ci(out)
```

```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
```

Calculate a bootstrapped confidence interval for the mean of this exponential distribution. Compare it to the naive standard error:

```{r chunk_chapter7_task_0, message=FALSE, warning=FALSE}
set.seed(1234)
data = rexp(500)
```

```{=html}
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
```

```{r chunk_chapter7_task_1, message=FALSE, warning=FALSE}

```

```{=html}
    </p>
  </details>
  <br/><hr/>
```


***Jacknife***

An alternative to the bootstrap is the **jacknife**.

From Wikipedia:

> In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a data set and calculating the estimate and then finding the average of these calculations. Given a sample of size N, the jackknife estimate is found by aggregating the estimates of each N-1-sized sub-sample.

> The jackknife technique was developed by Maurice Quenouille (1949, 1956). John Tukey (1958) expanded on the technique and proposed the name "jackknife" since, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.

> The jackknife is a linear approximation of the bootstrap.

```{r chunk_chapter7_chunk10, echo=TRUE, eval=TRUE}
library(bootstrap)

theta = function(x){ mean(x) }
results = jackknife(data, theta)

results$jack.se
results$jack.bias
```

***Parametric bootstrap***

We call it a **parametric bootstrap** if we don't re-sample the data to generate new data, but simulate from the fitted model. Simple example with a linear model:

```{r chunk_chapter7_chunk11, echo=TRUE, eval=TRUE}
set.seed(123)

x = runif(100, -2, 2)
y = rnorm(100, 1 + 2*x, 1)
dat = data.frame(x = x, y = y)

m = lm(y ~ x)

summary(m)
```

We are interested in getting the confidence intervals for the coefficients of the model:

```{r chunk_chapter7_chunk12, echo=TRUE, eval=TRUE}
resampledParameters = function(){
  newData = dat
  newData$y = unlist(simulate(m))
  mNew = lm(y ~ x, newData)
  return(coef(mNew)[1])
}
bootstrappedIntercept = replicate(500, resampledParameters())

hist(bootstrappedIntercept, breaks = 50)
abline(v = coef(m)[1], col = "red")
```

The same with the `boot`.{R} package. We need a statistics:

```{r chunk_chapter7_chunk14, echo=TRUE, eval=TRUE}
foo = function(out){
  m = lm(y ~ x, out)
  return(coef(m))
}
```

and a function to create new data

```{r chunk_chapter7_chunk15, echo=TRUE, eval=TRUE}
rgen = function(dat, mle){
  out = dat
  out$y = unlist(simulate(mle))
  return(out)
}

b2 = boot(dat, foo, R = 1000, sim = "parametric", ran.gen = rgen, mle = m)
boot.ci(b2, type = "perc", index = 1)
```


***Application: Simulated likelihood ratio test***

The parametric bootstrap can be used to generate simulated likelihood ratio tests for mixed models. This allows us to test for the significance of variance components without specifying degrees of freedom. 

To demonstrate this, let's simulated some Poisson data under a model with a random intercept, and fit with am appropriate mixed model (M1) and a standard GLM (M0):

```{r}
set.seed(123)
dat <- DHARMa::createData(sampleSize = 200, randomEffectVariance = 1)

m1 = glmer(observedResponse ~ Environment1 + (1|group), data = dat, family = "poisson")
deviance(m1)

m0 = glm(observedResponse ~ Environment1 , data = dat, family = "poisson")
deviance(m0)
```

The deviance of RE model is larger (meaning better fit), which is to be expected, but is the difference significant? Standard ANOVA as well as AIC have the problem, that df are not exact. We can circumvent this by using a simulated LRT. The idea is the following:

1. H0: simple model, without RE
2. Test statistic: M1/M0 or log(M1/M0) = log(M1) - log(M0)
3. Distribution of test statistic: we use the parametric bootstrap to new data, and fit M0 to this data to generate a distribution under H0

```{r}
resampledParameters = function(){
  newData = dat
  newData$observedResponse = unlist(simulate(m0))
  mNew = glm(observedResponse ~ Environment1, data = newData, family = "poisson")
  return(mNew$deviance)
}

nullDevianceDistribution = replicate(500, resampledParameters())
hist(deviance(m1) - nullDevianceDistribution, xlim = c(-100,50), breaks = 50, main = "log(L(M1) / L(M0))")
abline(v = 0, col = "red")
mean(deviance(m1) > nullDevianceDistribution)
```

This idea is implemented in a number of R packages, including pbkrtest and RLRsim, but neither of these fully generalizes to all models that you want to compare, so I recommend to use the hand-coded version. 

### Non-parametric R2 - cross-validation

Cross-validation is the non-parametric alternative to AIC. Note that AIC is asymptotically equal to leave-one-out cross-validation. 

For most advanced models, you will have to program the cross-validation by hand, but here an example for `glm`.{R}, using the `cv.glm`{.R} function:

```{r chunk_chapter7_chunk18, echo=TRUE, eval=TRUE}
library(boot)

# Leave-one-out and 6-fold cross-validation prediction error for the mammals data set.
data(mammals, package="MASS")
mammals.glm = glm(log(brain) ~ log(body), data = mammals)
cv.err = cv.glm(mammals, mammals.glm, K = 5)$delta


# As this is a linear model we could calculate the leave-one-out 
# cross-validation estimate without any extra model-fitting.
muhat = fitted(mammals.glm)
mammals.diag = glm.diag(mammals.glm)
(cv.err = mean((mammals.glm$y - muhat)^2/(1 - mammals.diag$h)^2))

# Leave-one-out and 11-fold cross-validation prediction error for 
# the nodal data set.  Since the response is a binary variable an
# appropriate cost function is
cost = function(r, pi = 0){ mean(abs(r - pi) > 0.5) }

nodal.glm = glm(r ~ stage+xray+acid, binomial, data = nodal)
(cv.err = cv.glm(nodal, nodal.glm, cost, K = nrow(nodal))$delta)
(cv.11.err = cv.glm(nodal, nodal.glm, cost, K = 11)$delta)
```

Note that cross-validation requires independence of data points. For non-independent data, it is possible to block the cross-validation, see Roberts, David R., et al. "Cross‐validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure." *Ecography* 40.8 (2017): 913-929., methods implemented in package `blockCV`{.R}, see <a href="https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html</a>.


## Structural Equation Models (SEMs)

Structural equation models (SEMs) are models that are designed to estimate entire causal diagrams. For GLMs responses, you will currently have to estimate the DAG (directed acyclic graph) piece-wise, e.g. with <a href="https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html</a>.


```{r}
library(ggdag)
library(ggplot2)
theme_set(theme_dag())

dag <- dagify(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,
  firesev ~ elev + age + cover,
  cover ~ age + elev + abiotic ,
  exposure = "age",
  outcome = "rich"
  )

ggdag(dag)

ggdag_paths(dag)

#ggdag_adjustment_set(dag)
#ggdag_dseparated(dag, controlling_for = c("cover", "hetero"))
```


```{r chunk_chapter7_chunk20, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(piecewiseSEM)

mod = psem(
  lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley),
  lm(firesev ~ elev + age + cover, data = keeley), 
  lm(cover ~ age + elev + hetero + abiotic, data = keeley)
)

summary(mod)
plot(mod)
```

For linear SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is `lavaan`.{R}:

```{r chunk_chapter7_chunk21, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(lavaan)

mod = "
  rich ~ distance + elev + abiotic + age + hetero + firesev + cover
  firesev ~ elev + age + cover
  cover ~ age + elev + abiotic 
"

fit = sem(mod, data = keeley)

summary(fit)
```

Plot options ... not so nice as before.

```{r chunk_chapter7_chunk22, echo=TRUE, eval=TRUE}
library(lavaanPlot)
lavaanPlot(model = fit)
```

## Intro Bayes

Intro Bayes will be done via Slides. Here a simple scrip that exemplifies the basic difference between frequentist and Bayesian parameter estimates, for a coin flip example:

```{r}
# extended commented version of this example at
# https://github.com/florianhartig/LearningBayes/blob/master/CommentedCode/01-Principles/InferenceMethods.md

# Assume we flipped a coin 10 times, and want to find out if
# it is biased - what can we learn about the probability to
# obtain heads with this coin?

trials = 10
success = 7

# For all three statistical methods, we use the same statistical model
# which is the binomial model. The probability density is available in R
# throught he function dbinom - dbinom(6,10,0.9) gives you the probability
# density of obtaining 6/10 heads when the true probability of heads is 0.9

dbinom(8,10, 0.5)

# We will now use this model to calculate the three classical inferential
# outputps of statistics - tests, MLE and Bayes

########### NHST #####################

# assume the coin is random (0.5), p-value is p >= observed
barplot(dbinom(0:10,10, 0.5), names.arg = 0:10, col = c(rep("grey", 7), rep("red", 4)))

# have to do 6 because R's diosyncrasies with lower.tail
pbinom(6,10,0.5, lower.tail = F)

binom.test(7,10,alternative = "greater")

############ MLE ######################

likelihood = function(x) dbinom(7,10, x)
parameterValues = seq(0,1,length.out = 100)

# assume data is fixed, true probability unknown
plot(parameterValues, likelihood(parameterValues), type = "l")
# calculate MLE 
abline(v = parameterValues[which.max(likelihood(parameterValues))], col = "red")

############# Bayes ##################

# posterior

prior = function(x) dnorm(x, 0, 0.1)

par(mfrow = c(2,2))

plot(parameterValues, prior(parameterValues), type = "l", main = "Prior")

plot(parameterValues, likelihood(parameterValues), type = "l", main = "Likelihood")
plot(parameterValues, prior(parameterValues) * likelihood(parameterValues), type = "l", main = "Posterior")
```

Note in the example above that the outcome of the estimating is a point estimate for the MLE, but a distribution for the posterior. 

To fit Bayesian models in practice, for full flexibility, most people use **Stan**. Stan is a completely new modelling specification language, therefore we won't do this here.

However, there is the `brms`{.R} package. `brms`{.R} allows you to specify regression models in the same syntax as `lme4`{.R} / `glmmTMB`{.R}, but translates then to Stan code and fits them. Here a comparison:

*Non-Bayesian (GLMM) with* `lme4`{.R}:

```{r chunk_chapter6_chunk16, echo=TRUE, eval=FALSE, purl=FALSE}
library(lme4)
m1 = glmer(SiblingNegotiation ~ FoodTreatment + (1|Nest),
         data = Owls , family = poisson)
```

*Bayesian with* `brms`{.R}:

```{r chunk_chapter6_chunk17, echo=TRUE, eval=FALSE, purl=FALSE}
library(brms)

mod1 = brm(SiblingNegotiation ~ FoodTreatment + (1|Nest),
         data = Owls , family = poisson)
summary(mod1)
```

For how to set priors, and modify sampler settings, see help!

```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
```

Take any of our simpler models, and run them with `brms`{.R}!

```{=html}
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
```

```{r chunk_chapter6_task_1, message=FALSE, warning=FALSE}
library(brms)

```

```{=html}
    </p>
  </details>
  <br/><hr/>
```


***Bayesian model comparison:***

In Bayesian stats, there is no p-value. So, how do we know if something has an effect? There are two options:

* Just look at the effect size and its uncertainties.
* Compare the simpler with the more complex model, and calculate which has a higher posterior probability

The latter is called **posterior weights**, and they are based on the so-called **Bayes factor**. For simple tests, e.g. t-test or lm, the Bayes factor is implemented in the BayesFactor package in R. Look at the examples <a href="https://cran.r-project.org/web/packages/BayesFactor/vignettes/manual.html" target="_blank" rel="noopener">here</a>.



