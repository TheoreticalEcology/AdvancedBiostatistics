--- 
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    highlight: kate
documentclass: book
editor_options:
  chunk_output_type: console
---


# Correlation structures {#correlation}

```{=html}
<!-- Put this here (right after the first markdown headline) and only here for each document! -->
<script src="./scripts/multipleChoice.js"></script>
```

This chapter explains how to model correlation structures in the residuals.


## General Idea

Except for the random effects, we have so far assumed that observations are independent. However, there are a number of other common correlation structures that we may want to consider. Here a visualization from <a href="https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881" target="_blank" rel="noopener">Roberts et al., 2016</a> (reproduced as OA, copyright: the authors).

```{r chunk_chapter6_chunk0, echo=FALSE, out.width="150%", out.height="150%"}
knitr::include_graphics(c("images/correlation.png"))
```

The figure shows random effects, and a number of other correlation structures. In random effects, residuals are structured in groups. All of the other three correlation structures discussed here are different. They are distance-based correlations between data points. Distance is expressed, e.g., by:

* Spatial distance.
* Temporal distance.
* Phylogenetic distance.

For either of these structures, there can be two phenomena that lead to correlations:

1. There can be a **trend** in the given space (e.g. time, space), which we have to remove first.
2. After accounting for the trend, there can be a so-called **autocorrelation** between data points.

The idea of the so-called **conditional autoregressive** (CAR) structures is, that we make parametric assumptions for how the correlation between data points falls off with distance. Then, we fit the model with this structure.

Similar as for the variance modelling, we can add this structures

* either in `nlme::gls`{.R}, see <a href="https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html" target="_blank" rel="noopener">https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html</a>,
* or in `glmmTMB`{.R}, see <a href="https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html</a>.

The following pages provide examples and further comments on how to do this.


## Temporal Correlation Structures

In principle, spatial and temporal correlation are quite similar, there are 2 options we can have:

1. There is a spatial trend in time / space, which creates a correlation in space / time.
2. There truly is a spatial correlation, after accounting for the trend.

Unfortunately, the distinction between a larger trend and a correlation is quite fluid. Nevertheless, one should always first check for and remove the trend, typically by including time/space as a predictor, potentially in a flexible way (GAMs come in handy). After this is done, we can fit a model with a temporally/spatially correlated error.

As our first example, I look at the hurricane study from yesterday, which is, after all, temporal data. This data set is located in `DHARMa`{.R}.

```{r chunk_chapter6_chunk1, echo=TRUE, eval=TRUE}
library(glmmTMB)
library(DHARMa)

originalModelGAM = glmmTMB(alldeaths ~ scale(MasFem) *
                          (scale(Minpressure_Updated_2014) + scale(NDAM)),
                           data = hurricanes, family = nbinom2)

# Residual checks with DHARMa.
res = simulateResiduals(originalModelGAM)
plot(res)

# No significant deviation in the general plot, but try this, which was highlighted by
# https://www.theguardian.com/science/grrlscientist/2014/jun/04/hurricane-gender-name-bias-sexism-statistics
plotResiduals(res, hurricanes$NDAM)

# We also find temporal autocorrelation.
res2 = recalculateResiduals(res, group = hurricanes$Year)
testTemporalAutocorrelation(res2, time = unique(hurricanes$Year))
```

A second example from Pinheiro and Bates, pp. 255-258. The data originates from Vonesh and Carter (1992), who describe data measured on high-flux hemodialyzers to assess their in vivo ultrafiltration characteristics. The ultrafiltration rates (in mL/hr) of 20 high-flux dialyzers were measured at seven different transmembrane pressures (in dmHg). The in vitro evaluation of the dialyzers used bovine blood at flow rates of either 200~dl/min or 300~dl/min. The data, are also analyzed in Littell, Milliken, Stroup and Wolfinger (1996).

See `?Dialyzer`{.R} for explanation of the variables (data comes with the package `nlme`.{R}).

The data highlights the flexibility of gls for structured `( 1| subject)`{.R} temporal data. Unfortunately, `nlme`.{R} does not interface with `DHARMa`.{R}.

```{r chunk_chapter6_chunk2, echo=TRUE, eval=TRUE}
library(nlme)

fm1Dial.gls = gls(rate ~(pressure + I(pressure^2) + I(pressure^3) + I(pressure^4))*QB,
                  data = Dialyzer)
plot(fm1Dial.gls)
fm2Dial.gls = update(fm1Dial.gls, weights = varPower(form = ~ pressure))
plot(fm2Dial.gls)
fm3Dial.gls = update(fm2Dial.gls, corr = corAR1(0.771, form = ~ 1 | Subject))
summary(fm3Dial.gls)
```


## Spatial Correlation Structures

We will use a data set with the thickness of coal seams, that we try to predict with a spatial (soil) predictor. Read in data

```{r chunk_chapter6_chunk4, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(EcoData)
library(DHARMa)
library(gstat)

plot(thick ~ soil, data = thickness)
fit = lm(thick ~ soil, data = thickness)
summary(fit)

# Quantile residuals are not actually needed in this case but
# DHARMa includes a test for spatial autocorrelation which
# will save us coding time
res = simulateResiduals(fit)
testSpatialAutocorrelation(res, x = thickness$north, y = thickness$east)

# Looking also at the directional variogram
tann.dir.vgm = variogram(residuals(fit) ~ 1,
                         loc =~ east + north, data = thickness,
                         alpha = c(0, 45, 90, 135))
plot(tann.dir.vgm)
```

Remove trend via a GAM:

```{r chunk_chapter6_chunk5, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(mgcv)
library(modEvA)

fit1 = gam(thick ~ soil + te(east, north) , data = thickness)
summary(fit1)
plot(fit1, pages = 0, lwd = 2)

res = simulateResiduals(fit1)
testSpatialAutocorrelation(res, x = thickness$north, y = thickness$east)
```

Almost the same, but simpler:

```{r chunk_chapter6_chunk6, echo=TRUE, eval=TRUE}
fit = lm(thick ~ soil + north + I(north^2), data = thickness)
```

Alternatively, fit an autoregressive model. Of course, both options can be combined.

```{r chunk_chapter6_chunk7, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
fit2 = gls(thick ~ soil , correlation = corExp(form =~ east + north) , data = thickness)
summary(fit2)

fit1 = gls(thick ~ soil + north + I(north^2), data = thickness)

anova(fit1, fit2)
```

### Exercise

```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
```

Use the dataset EcoData::plantcounts. Our scientific question is if richness ~ agrarea. Help on the dataset, as well as a few initial plots, is in the help of ?plantcounts.

This is count data, so start with a Poisson or Neg Binom GLM. The quadrats are not all equally sized, so you should include an offest to account for area. Then, check for spatial autocorrelation.

If you find autocorrelation that cannot be removed with a gam, the problem is that the gls function that we have used so far only extends lm, and not glm models. In this case, you can either read up in https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html how to specify a spatial covariance in glmmTMB, or just log transform your counts + 1, and fit a gls. 


```{=html}
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
```

```{r message=FALSE, warning=FALSE}
library(EcoData)
```

```{=html}
    </p>
  </details>
  <br/><hr/>
```


## Phylogenetic Structures (PGLS)

This is mostly taken from <a href="https://lukejharmon.github.io/ilhabela/instruction/2015/07/03/PGLS/" target="_blank" rel="noopener">https://lukejharmon.github.io/ilhabela/instruction/2015/07/03/PGLS/</a>. The two datasets associated with this example are in the `EcoData`{.R} package.

Perform analysis:

```{r chunk_chapter6_chunk8, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(EcoData)
library(ape)
library(geiger)
library(nlme)
library(phytools)
library(DHARMa)
```

To plot the phylogenetic tree, use

```{r chunk_chapter6_chunk9, eval=FALSE, fig.width=8, fig.height=18}
plot(anolisTree)
```

Regress species traits

```{r}
# Check whether names are matching in both files.
name.check(anolisTree, anolisData)

# Plot traits.
plot(anolisData[, c("awesomeness", "hostility")])

plot(hostility ~ awesomeness, data = anolisData)
fit = lm(hostility ~ awesomeness, data = anolisData)
summary(fit)
abline(fit)
```


Check for phylogenetic signal in residuals.

```{r}
# Calculate weight matrix for phylogenetic distance.
w = 1/cophenetic(anolisTree)
diag(w) = 0

Moran.I(residuals(fit), w)
```

Conclusion: signal in the residuals, a normal lm will not work.

You can also check with DHARMa, using this works also for GLMMs

```{r}
res = simulateResiduals(fit)
testSpatialAutocorrelation(res, distMat = cophenetic(anolisTree))
```


An old-school method to deal with the problem are the so-called **Phylogenetically Independent Contrasts** (PICs) (Felsenstein, J. (1985) "Phylogenies and the comparative method". American Naturalist, 125, 1–15.). The idea here is to transform your data in a way that an lm is still appropriate. For completeness, I show the method here.

```{r chunk_chapter6_chunk12, echo=TRUE, eval=TRUE}
# Extract columns.
host = anolisData[, "hostility"]
awe = anolisData[, "awesomeness"]

# Give them names.
names(host) = names(awe) = rownames(anolisData)

# Calculate PICs.
hPic = pic(host, anolisTree)
aPic = pic(awe, anolisTree)

# Make a model.
picModel = lm(hPic ~ aPic - 1)

summary(picModel) # Yes, significant.

# plot results.
plot(hPic ~ aPic)
abline(a = 0, b = coef(picModel))
```

Now, new school, with a PGLS

```{r chunk_chapter6_chunk13, echo=TRUE, eval=TRUE}
pglsModel = gls(hostility ~ awesomeness,
                 correlation = corBrownian(phy = anolisTree, form =~ species),
                 data = anolisData, method = "ML")
summary(pglsModel)
coef(pglsModel)
plot(hostility ~ awesomeness, data = anolisData)
abline(pglsModel, col = "red")
```

OK, same result, but PGLS is WAY more flexible than PICs.
For example, we can include a discrete predictor:

```{r chunk_chapter6_chunk14, echo=TRUE, eval=TRUE}
pglsModel2 = gls(hostility ~ ecomorph,
                    correlation = corBrownian(phy = anolisTree, form =~ species),
                    data = anolisData, method = "ML")
summary(pglsModel2)
anova(pglsModel2)

# We can even include multiple predictors:

pglsModel3 = gls(hostility ~ ecomorph * awesomeness,
                correlation = corBrownian(phy = anolisTree, form =~ species),
                data = anolisData, method = "ML")
summary(pglsModel3)
anova(pglsModel3)
```

We can also assume that the error structure follows an **Ornstein-Uhlenbeck** model rather than **Brownian motion**. When trying this, however, I noted that the model does not converge due to a scaling problem. We can do a quick fix by making the branch lengths longer. This will not affect the analysis other than rescaling a nuisance parameter.

```{r chunk_chapter6_chunk15, echo=TRUE, eval=TRUE}
tempTree = anolisTree
tempTree$edge.length = tempTree$edge.length * 100
pglsModelLambda = gls(hostility ~ awesomeness,
                      correlation = corPagel(1, phy = tempTree, fixed = FALSE,
                                             form =~ species),
                      data = anolisData, method = "ML")
summary(pglsModelLambda)

pglsModelOU = gls(hostility ~ awesomeness,
                   correlation = corMartins(1, phy = tempTree, form =~ species),
                   data = anolisData)
summary(pglsModelOU)
```

Other example: <a href="http://schmitzlab.info/pgls.htmla" target="_blank" rel="noopener">http://schmitzlab.info/pgls.html</a>.

For fitting PGLS with various models, you should also consider the `caper`{.R} package.


### Exercise

Download the following two datasets

http://www.phytools.org/Cordoba2017/data/BarbetTree.nex
http://www.phytools.org/Cordoba2017/data/Barbetdata.csv

These data are from a study by Corboda et al., 2017, which examined the influence of environmental factors on the evolution of song in an group of Asian bird species called “barbets.”

Check if there is a relationship between altitude at which a species is found and the length of the note in its song, which uses the variables Lnote~Lnalt

Help: you need to clean the data a bit, there are some species in the tree for which we have no observations
```{r, eval = F}
dat<-read.csv("Barbetdata.csv",header=TRUE,row.names=1)
tree<-read.nexus("BarbetTree.nex")
obj<-name.check(tree,dat)
obj
reducedTree<-drop.tip(tree, obj$tree_not_data)
name.check(reducedTree,dat)
# now you should be able to work with this data as in our example
```



One useful step that we haven't covered so far is to ensure that all the species in the data are in the tree & vice versa.

Remeber that if there are any spelling differences, even small, then the tree & dataset will not coincide.

To establish that we have the same species in our data as in the tree, or vice versa, we can use a function in the geiger package called name.check.

obj<-name.check(arbol,datos)
obj
## $tree_not_data
## [1] "M_asiatica_chersonesus"    "M_australis_australis"    
## [3] "M_haemacephala_celestinoi" "M_haemacephala_delica"    
## [5] "M_haemacephala_intermedia" "M_haemacephala_rosea"     
## [7] "M_lineata_lineata"         "M_oorti_faber"            
## [9] "M_oorti_sini"             
## 
## $data_not_tree
## character(0)
We should see that there are 9 species in the tree for which we do not have data. We have to prune these species from the tree to proceed with the analysis. We can do this using the function drop.tip as we learned:


Now the data & the tree should coincide in both number of species and names. We can verify this using name.check:



## Case studies

### Snouter

```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:25px;">Task</span></strong><br/>
```

Fit one of the responses in the snouter datset against the predictors rain + djungle (see ?snouter). Check for spatial autocorrelation and proceed to fitting a spatial model if needed. See the data set's help for details on the variables.

```{=html}
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:25px;">Solution</span></strong>
    </summary>
    <p>
```

```{r chunk_chapter6_task_0, message=FALSE, warning=FALSE}
library(EcoData)
str(snouter)
```

```{=html}
    </p>
  </details>
  <br/><hr/>
```


### Covariance structures in glmmTMB

gls only allows normally distributed responses. For GLMMs, you can use glmmTMB, which has (experimental) support for spatial, temporal or phylogenetic covariance structures on the REs. If you want to specific residual autocorrelation, you can create and observation-level RE and specify the covariance structure there. Take one of the examples that we had before (e.g. plantcount) and try to fit a spatial covariance with glmmTMB, using the tutorial here https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html

Alternative packages for spatial models are MASS::glmmPQL, BRMS, or INLA. 





